#!/usr/bin/env python3
"""
å¼·åŒ–å­¦ç¿’ç‰ˆCQCNNé§’æ¨å®šã‚·ã‚¹ãƒ†ãƒ 
è‡ªå·±å¯¾æˆ¦ã«ã‚ˆã‚‹å­¦ç¿’ã¨æ•™å¸«ã‚ã‚Šå­¦ç¿’ç‰ˆã¨ã®æ¯”è¼ƒå®Ÿé¨“
"""

import sys
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from typing import Dict, Tuple, List, Optional
import random
from collections import deque
import time
import json
import matplotlib.pyplot as plt
from dataclasses import dataclass

# ãƒ‘ã‚¹è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, "src", "qugeister_competitive")
sys.path.insert(0, src_path)
sys.path.insert(0, current_dir)

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from separated_cqcnn_qmap import CQCNNPieceEstimator, QValueMapGenerator, IntegratedCQCNNSystem


# ================================================================================
# Part 1: å¼·åŒ–å­¦ç¿’ç”¨ã®å ±é…¬ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================


class RLRewardSystem:
    """å¼·åŒ–å­¦ç¿’ç”¨ã®å ±é…¬è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.weights = {
            # åŸºæœ¬å ±é…¬
            "win_game": 10.0,
            "lose_game": -10.0,
            "draw_game": 0.0,
            # é§’æ¨å®šé–¢é€£
            "correct_good_estimation": 2.0,  # å–„ç‰ã‚’æ­£ã—ãæ¨å®š
            "correct_bad_estimation": 1.5,  # æ‚ªç‰ã‚’æ­£ã—ãæ¨å®š
            "wrong_estimation": -1.0,  # é–“é•ã£ãŸæ¨å®š
            # è¡Œå‹•çµæœ
            "capture_good": 5.0,  # å–„ç‰æ•ç²
            "capture_bad": -3.0,  # æ‚ªç‰æ•ç²
            "good_escape": 8.0,  # å–„ç‰è„±å‡º
            "lost_good": -4.0,  # å–„ç‰å–ªå¤±
            # æˆ¦ç•¥çš„è¦ç´ 
            "forward_progress": 0.2,  # å‰é€²
            "control_center": 0.1,  # ä¸­å¤®åˆ¶å¾¡
            "piece_protection": 0.3,  # é§’ã®ä¿è­·
        }

        self.episode_rewards = []
        self.estimation_accuracy_history = []

    def calculate_step_reward(self, old_state: Dict, action: Tuple, new_state: Dict, estimation: Dict) -> float:
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®å ±é…¬ã‚’è¨ˆç®—"""
        reward = 0.0

        # ç§»å‹•ã®åŸºæœ¬è©•ä¾¡
        from_pos, to_pos = action

        # å‰é€²å ±é…¬
        if old_state["current_player"] == "A":
            if to_pos[1] > from_pos[1]:
                reward += self.weights["forward_progress"]
        else:
            if to_pos[1] < from_pos[1]:
                reward += self.weights["forward_progress"]

        # é§’å–ã‚Šã®è©•ä¾¡
        if to_pos in new_state.get("captured_pieces", {}):
            captured_type = new_state["captured_pieces"][to_pos]
            if captured_type == "good":
                reward += self.weights["capture_good"]
            else:
                reward += self.weights["capture_bad"]

        # ä¸­å¤®åˆ¶å¾¡
        center_dist = abs(to_pos[0] - 2.5) + abs(to_pos[1] - 2.5)
        reward += self.weights["control_center"] * (5 - center_dist) / 5

        return reward

    def calculate_episode_reward(self, winner: str, player: str, final_estimations: List[Dict]) -> float:
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã®å ±é…¬ã‚’è¨ˆç®—"""
        reward = 0.0

        # å‹æ•—å ±é…¬
        if winner == player:
            reward += self.weights["win_game"]
        elif winner == "Draw":
            reward += self.weights["draw_game"]
        else:
            reward += self.weights["lose_game"]

        # æ¨å®šç²¾åº¦ãƒœãƒ¼ãƒŠã‚¹
        if final_estimations:
            correct_estimations = sum(1 for e in final_estimations if e.get("correct", False))
            accuracy = correct_estimations / len(final_estimations)
            reward += accuracy * 3.0
            self.estimation_accuracy_history.append(accuracy)

        self.episode_rewards.append(reward)
        return reward

    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            "avg_episode_reward": np.mean(self.episode_rewards[-100:]) if self.episode_rewards else 0,
            "avg_estimation_accuracy": np.mean(self.estimation_accuracy_history[-100:])
            if self.estimation_accuracy_history
            else 0,
            "total_episodes": len(self.episode_rewards),
        }


# ================================================================================
# Part 2: çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡
# ================================================================================


@dataclass
class Experience:
    """çµŒé¨“ãƒ‡ãƒ¼ã‚¿"""

    state: np.ndarray
    enemy_positions: List[Tuple[int, int]]
    true_enemy_types: Dict[Tuple[int, int], str]
    estimations: Dict
    reward: float
    next_state: Optional[np.ndarray]
    done: bool


class ExperienceReplayBuffer:
    """çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡"""

    def __init__(self, capacity: int = 10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, experience: Experience):
        """çµŒé¨“ã‚’è¿½åŠ """
        self.buffer.append(experience)

    def sample(self, batch_size: int) -> List[Experience]:
        """ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        return random.sample(self.buffer, min(batch_size, len(self.buffer)))

    def __len__(self):
        return len(self.buffer)


# ================================================================================
# Part 3: å¼·åŒ–å­¦ç¿’ç‰ˆCQCNNãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
# ================================================================================


class RLCQCNNTrainer:
    """å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹CQCNNå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(
        self,
        estimator: CQCNNPieceEstimator,
        learning_rate: float = 0.001,
        gamma: float = 0.95,
        epsilon_start: float = 1.0,
        epsilon_end: float = 0.01,
        epsilon_decay: float = 0.995,
    ):
        # ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.estimator = estimator
        self.target_estimator = CQCNNPieceEstimator(estimator.n_qubits, estimator.n_layers)
        self.target_estimator.load_state_dict(estimator.state_dict())

        self.optimizer = optim.Adam(estimator.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        # å­¦ç¿’ç”¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.replay_buffer = ExperienceReplayBuffer()
        self.reward_system = RLRewardSystem()

        # å­¦ç¿’å±¥æ­´
        self.training_history = {
            "episodes": [],
            "rewards": [],
            "losses": [],
            "win_rates": [],
            "estimation_accuracy": [],
        }

        self.episodes_trained = 0
        self.update_target_every = 100

    def select_action(self, estimations: Dict, legal_moves: List, q_map: np.ndarray) -> Tuple:
        """Îµ-greedyè¡Œå‹•é¸æŠ"""
        if random.random() < self.epsilon:
            return random.choice(legal_moves)

        # Qå€¤ã«åŸºã¥ãé¸æŠ
        best_move = None
        best_q = -float("inf")

        for move in legal_moves:
            from_pos, to_pos = move
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]

            # æ–¹å‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            if dy == -1 and dx == 0:
                dir_idx = 0
            elif dx == 1 and dy == 0:
                dir_idx = 1
            elif dy == 1 and dx == 0:
                dir_idx = 2
            elif dx == -1 and dy == 0:
                dir_idx = 3
            else:
                continue

            q_value = q_map[from_pos[1], from_pos[0], dir_idx]

            if q_value > best_q:
                best_q = q_value
                best_move = move

        return best_move if best_move else random.choice(legal_moves)

    def train_step(self, batch_size: int = 32) -> float:
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’"""
        if len(self.replay_buffer) < batch_size:
            return 0.0

        # ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = self.replay_buffer.sample(batch_size)

        total_loss = 0.0
        loss_count = 0

        for experience in batch:
            # ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            board_tensor = self._prepare_board_tensor(
                experience.state,
                "A",  # ä»®ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼
            )

            # æ•µé§’ä½ç½®ã”ã¨ã«å­¦ç¿’
            for pos in experience.enemy_positions:
                if pos not in experience.true_enemy_types:
                    continue

                # çœŸã®ã‚¿ã‚¤ãƒ—
                true_type = experience.true_enemy_types[pos]

                # ç¾åœ¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§æ¨å®šï¼ˆå‹¾é…è¿½è·¡ã‚ã‚Šï¼‰
                # CQCNNPieceEstimatorã®forwardå‡¦ç†ã‚’ç›´æ¥å®Ÿè¡Œ
                features = self.estimator.feature_conv(board_tensor)
                features_flat = features.view(features.size(0), -1)

                # é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®å…¥åŠ›ã‚’æº–å‚™
                # ç°¡æ˜“çš„ãªé‡å­å›è·¯ã®ãŸã‚ã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã‚’é¿ã‘ã‚‹
                n_qubits = self.estimator.n_qubits
                board_flat = board_tensor.view(board_tensor.size(0), -1)
                quantum_input = board_flat[:, :n_qubits].clone()  # clone()ã§æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ

                # é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã‚’é¿ã‘ã‚‹ï¼‰
                batch_size_q = quantum_input.shape[0]
                q_state = torch.zeros(batch_size_q, n_qubits)

                for layer in range(self.estimator.n_layers):
                    new_q_state = torch.zeros_like(q_state)
                    for q in range(n_qubits):
                        rotation = self.estimator.quantum_params[layer, q]
                        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã‚’é¿ã‘ã‚‹
                        new_q_state[:, q] = torch.sin(
                            rotation[0] * quantum_input[:, q % quantum_input.shape[1]]
                        ) * torch.cos(rotation[1] * quantum_input[:, (q + 1) % quantum_input.shape[1]]) + torch.tanh(
                            rotation[2] * q_state[:, q]
                        )
                    q_state = new_q_state  # æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«ã«ç½®ãæ›ãˆ

                quantum_out = q_state

                # çµåˆ
                combined = torch.cat([features_flat, quantum_out], dim=1)

                # é§’ã‚¿ã‚¤ãƒ—äºˆæ¸¬
                type_logits = self.estimator.piece_type_head(combined)
                type_probs = torch.softmax(type_logits, dim=1)

                # ç›®æ¨™å€¤ã®è¨ˆç®—
                if true_type == "good":
                    target = torch.tensor([[1.0, 0.0]], requires_grad=False)
                else:
                    target = torch.tensor([[0.0, 1.0]], requires_grad=False)

                # å ±é…¬ã‚’åæ˜ 
                reward_factor = 1.0 + experience.reward * 0.1
                target = target * reward_factor
                target = torch.clamp(target, 0.0, 1.0)

                # æå¤±è¨ˆç®—
                loss = self.criterion(type_probs, target)

                # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
                self.optimizer.zero_grad()
                loss.backward()

                # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                torch.nn.utils.clip_grad_norm_(self.estimator.parameters(), 1.0)

                self.optimizer.step()

                total_loss += loss.item()
                loss_count += 1

        return total_loss / max(loss_count, 1)

    def update_target_network(self):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°"""
        self.target_estimator.load_state_dict(self.estimator.state_dict())

    def train_episode(self, game_func, verbose: bool = False) -> Dict:
        """1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å­¦ç¿’ï¼ˆè‡ªå·±å¯¾æˆ¦ï¼‰"""
        # ã‚²ãƒ¼ãƒ å®Ÿè¡Œ
        game_result = game_func(self, verbose=verbose)

        # çµŒé¨“ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        for exp in game_result.get("experiences", []):
            self.replay_buffer.push(exp)

        # å­¦ç¿’å®Ÿè¡Œ
        losses = []
        for _ in range(10):  # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«10å›å­¦ç¿’
            loss = self.train_step()
            if loss > 0:
                losses.append(loss)

        # Îµã®æ¸›è¡°
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°
        self.episodes_trained += 1
        if self.episodes_trained % self.update_target_every == 0:
            self.update_target_network()

        # å±¥æ­´è¨˜éŒ²
        avg_loss = np.mean(losses) if losses else 0
        self.training_history["episodes"].append(self.episodes_trained)
        self.training_history["losses"].append(avg_loss)
        self.training_history["rewards"].append(game_result.get("total_reward", 0))

        return {
            "episode": self.episodes_trained,
            "reward": game_result.get("total_reward", 0),
            "loss": avg_loss,
            "epsilon": self.epsilon,
        }

    def _prepare_board_tensor(self, board: np.ndarray, player: str) -> torch.Tensor:
        """ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
        tensor = torch.zeros(1, 3, 6, 6)
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val

        tensor[0, 0] = torch.from_numpy((board == player_val).astype(np.float32))
        tensor[0, 1] = torch.from_numpy((board == enemy_val).astype(np.float32))
        tensor[0, 2] = torch.from_numpy((board == 0).astype(np.float32))

        return tensor

    def save_checkpoint(self, filepath: str):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜"""
        torch.save(
            {
                "estimator_state": self.estimator.state_dict(),
                "target_state": self.target_estimator.state_dict(),
                "optimizer_state": self.optimizer.state_dict(),
                "epsilon": self.epsilon,
                "episodes": self.episodes_trained,
                "training_history": self.training_history,
            },
            filepath,
        )
        print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {filepath}")

    def load_checkpoint(self, filepath: str):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        # PyTorch 2.6å¯¾å¿œ: weights_only=Falseã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        checkpoint = torch.load(filepath, weights_only=False)
        self.estimator.load_state_dict(checkpoint["estimator_state"])
        self.target_estimator.load_state_dict(checkpoint["target_state"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state"])
        self.epsilon = checkpoint["epsilon"]
        self.episodes_trained = checkpoint["episodes"]
        self.training_history = checkpoint["training_history"]
        print(f"ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {filepath}")


# ================================================================================
# Part 4: è‡ªå·±å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================


class SelfPlaySystem:
    """è‡ªå·±å¯¾æˆ¦ã«ã‚ˆã‚‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, rl_trainer: RLCQCNNTrainer):
        self.trainer = rl_trainer
        self.qmap_generator = QValueMapGenerator()
        self.game_history = []

    def play_episode(self, verbose: bool = False) -> Dict:
        """1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è‡ªå·±å¯¾æˆ¦"""
        # ãƒ€ãƒŸãƒ¼ã®ã‚²ãƒ¼ãƒ å®Ÿè¡Œï¼ˆå®Ÿéš›ã®ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã¨çµ±åˆãŒå¿…è¦ï¼‰
        experiences = []
        total_reward = 0.0

        # ä»®ã®ã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ—
        for turn in range(50):
            # ä»®ã®ãƒœãƒ¼ãƒ‰çŠ¶æ…‹
            board = np.random.randint(-1, 2, (6, 6))
            enemy_positions = [(i, j) for i in range(6) for j in range(6) if board[j, i] == -1][:4]

            if not enemy_positions:
                break

            # æ¨å®šå®Ÿè¡Œ
            estimations = {}
            true_types = {}

            for pos in enemy_positions:
                # ä»®ã®çœŸã®ã‚¿ã‚¤ãƒ—
                true_types[pos] = random.choice(["good", "bad"])

                # æ¨å®šï¼ˆå­¦ç¿’ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
                board_tensor = self.trainer._prepare_board_tensor(board, "A")
                with torch.no_grad():
                    # ç›´æ¥ç‰¹å¾´æŠ½å‡ºã—ã¦æ¨å®š
                    features = self.trainer.estimator.feature_conv(board_tensor)
                    features_flat = features.view(features.size(0), -1)

                    # quantum_circuit_simulationã«æ­£ã—ã„å½¢çŠ¶ã®å…¥åŠ›ã‚’æ¸¡ã™
                    # n_qubitsæ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
                    n_qubits = self.trainer.estimator.n_qubits
                    if features_flat.shape[1] > n_qubits:
                        # Linearå±¤ã§æ¬¡å…ƒå‰Šæ¸›
                        linear = nn.Linear(features_flat.shape[1], n_qubits)
                        quantum_input = torch.tanh(linear(features_flat))
                    else:
                        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹
                        padding = n_qubits - features_flat.shape[1]
                        quantum_input = F.pad(features_flat, (0, padding), "constant", 0)

                    quantum_out = self.trainer.estimator.quantum_circuit_simulation(quantum_input)
                    combined = torch.cat([features_flat, quantum_out], dim=1)
                    type_logits = self.trainer.estimator.piece_type_head(combined)
                    type_probs = torch.softmax(type_logits, dim=1)

                    estimations[pos] = {
                        "good_prob": type_probs[0, 0].item(),
                        "bad_prob": type_probs[0, 1].item(),
                        "confidence": max(type_probs[0].tolist()),
                    }

            # å ±é…¬è¨ˆç®—
            step_reward = 0.0
            for pos, est in estimations.items():
                true_type = true_types[pos]
                if true_type == "good" and est["good_prob"] > 0.5:
                    step_reward += 1.0
                elif true_type == "bad" and est["bad_prob"] > 0.5:
                    step_reward += 1.0
                else:
                    step_reward -= 0.5

            # çµŒé¨“ã‚’è¨˜éŒ²
            exp = Experience(
                state=board,
                enemy_positions=enemy_positions,
                true_enemy_types=true_types,
                estimations=estimations,
                reward=step_reward,
                next_state=None,
                done=False,
            )
            experiences.append(exp)
            total_reward += step_reward

            if verbose and turn % 10 == 0:
                print(f"  Turn {turn}: Reward = {step_reward:.2f}")

        return {"experiences": experiences, "total_reward": total_reward, "turns": len(experiences)}


# ================================================================================
# Part 5: æ¯”è¼ƒå®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================


class ComparisonExperiment:
    """æ•™å¸«ã‚ã‚Šå­¦ç¿’ vs å¼·åŒ–å­¦ç¿’ã®æ¯”è¼ƒå®Ÿé¨“"""

    def __init__(self):
        # æ•™å¸«ã‚ã‚Šå­¦ç¿’ç‰ˆ
        self.sl_system = IntegratedCQCNNSystem(n_qubits=8, n_layers=3)

        # å¼·åŒ–å­¦ç¿’ç‰ˆ
        self.rl_estimator = CQCNNPieceEstimator(n_qubits=8, n_layers=3)
        self.rl_trainer = RLCQCNNTrainer(self.rl_estimator)
        self.self_play = SelfPlaySystem(self.rl_trainer)

        # å®Ÿé¨“çµæœ
        self.results = {"sl_performance": [], "rl_performance": [], "training_episodes": []}

    def train_rl_model(self, episodes: int = 100, verbose: bool = True):
        """å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        print(f"\nğŸ¤– å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹ ({episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)")
        print("=" * 60)

        for episode in range(episodes):
            # è‡ªå·±å¯¾æˆ¦ã«ã‚ˆã‚‹å­¦ç¿’
            result = self.self_play.play_episode(verbose=False)

            # å­¦ç¿’å®Ÿè¡Œ
            train_result = self.rl_trainer.train_episode(lambda trainer, verbose: result, verbose=False)

            if verbose and (episode + 1) % 10 == 0:
                print(
                    f"Episode {episode + 1}/{episodes}: "
                    f"Reward = {train_result['reward']:.2f}, "
                    f"Loss = {train_result['loss']:.4f}, "
                    f"Îµ = {train_result['epsilon']:.3f}"
                )

        print("âœ… å¼·åŒ–å­¦ç¿’å®Œäº†")

    def evaluate_models(self, test_cases: int = 100) -> Dict:
        """ä¸¡ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
        print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ({test_cases}ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹)")
        print("-" * 40)

        sl_correct = 0
        rl_correct = 0

        for _ in range(test_cases):
            # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ
            board = np.random.randint(-1, 2, (6, 6))
            enemy_positions = [(i, j) for i in range(6) for j in range(6) if board[j, i] == -1][:4]

            if not enemy_positions:
                continue

            # çœŸã®ã‚¿ã‚¤ãƒ—
            true_types = {pos: random.choice(["good", "bad"]) for pos in enemy_positions}

            # ãƒœãƒ¼ãƒ‰ãƒ†ãƒ³ã‚½ãƒ«æº–å‚™ï¼ˆå…±é€šãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
            board_tensor_sl = self._prepare_board_tensor(board, "A")
            board_tensor_rl = self._prepare_board_tensor(board, "A")

            # è©•ä¾¡
            for pos in enemy_positions:
                true_type = true_types[pos]

                with torch.no_grad():
                    # SLç‰ˆã®æ¨å®š
                    features_sl = self.sl_system.estimator.feature_conv(board_tensor_sl)
                    features_flat_sl = features_sl.view(features_sl.size(0), -1)

                    # é‡å­å…¥åŠ›ã®æº–å‚™ï¼ˆSLç‰ˆï¼‰
                    n_qubits_sl = self.sl_system.estimator.n_qubits
                    board_flat_sl = board_tensor_sl.view(board_tensor_sl.size(0), -1)
                    quantum_input_sl = board_flat_sl[:, :n_qubits_sl].clone()

                    # é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSLç‰ˆï¼‰
                    batch_size_sl = quantum_input_sl.shape[0]
                    q_state_sl = torch.zeros(batch_size_sl, n_qubits_sl)

                    for layer in range(self.sl_system.estimator.n_layers):
                        new_q_state_sl = torch.zeros_like(q_state_sl)
                        for q in range(n_qubits_sl):
                            rotation_sl = self.sl_system.estimator.quantum_params[layer, q]
                            new_q_state_sl[:, q] = torch.sin(
                                rotation_sl[0] * quantum_input_sl[:, q % quantum_input_sl.shape[1]]
                            ) * torch.cos(
                                rotation_sl[1] * quantum_input_sl[:, (q + 1) % quantum_input_sl.shape[1]]
                            ) + torch.tanh(rotation_sl[2] * q_state_sl[:, q])
                        q_state_sl = new_q_state_sl

                    quantum_out_sl = q_state_sl
                    combined_sl = torch.cat([features_flat_sl, quantum_out_sl], dim=1)
                    type_logits_sl = self.sl_system.estimator.piece_type_head(combined_sl)
                    type_probs_sl = torch.softmax(type_logits_sl, dim=1)

                    if true_type == "good" and type_probs_sl[0, 0].item() > 0.5:
                        sl_correct += 1
                    elif true_type == "bad" and type_probs_sl[0, 1].item() > 0.5:
                        sl_correct += 1

                    # RLç‰ˆã®æ¨å®š
                    features_rl = self.rl_estimator.feature_conv(board_tensor_rl)
                    features_flat_rl = features_rl.view(features_rl.size(0), -1)

                    # é‡å­å…¥åŠ›ã®æº–å‚™ï¼ˆRLç‰ˆï¼‰
                    n_qubits_rl = self.rl_estimator.n_qubits
                    board_flat_rl = board_tensor_rl.view(board_tensor_rl.size(0), -1)
                    quantum_input_rl = board_flat_rl[:, :n_qubits_rl].clone()

                    # é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆRLç‰ˆï¼‰
                    batch_size_rl = quantum_input_rl.shape[0]
                    q_state_rl = torch.zeros(batch_size_rl, n_qubits_rl)

                    for layer in range(self.rl_estimator.n_layers):
                        new_q_state_rl = torch.zeros_like(q_state_rl)
                        for q in range(n_qubits_rl):
                            rotation_rl = self.rl_estimator.quantum_params[layer, q]
                            new_q_state_rl[:, q] = torch.sin(
                                rotation_rl[0] * quantum_input_rl[:, q % quantum_input_rl.shape[1]]
                            ) * torch.cos(
                                rotation_rl[1] * quantum_input_rl[:, (q + 1) % quantum_input_rl.shape[1]]
                            ) + torch.tanh(rotation_rl[2] * q_state_rl[:, q])
                        q_state_rl = new_q_state_rl

                    quantum_out_rl = q_state_rl
                    combined_rl = torch.cat([features_flat_rl, quantum_out_rl], dim=1)
                    type_logits_rl = self.rl_estimator.piece_type_head(combined_rl)
                    type_probs_rl = torch.softmax(type_logits_rl, dim=1)

                    if true_type == "good" and type_probs_rl[0, 0].item() > 0.5:
                        rl_correct += 1
                    elif true_type == "bad" and type_probs_rl[0, 1].item() > 0.5:
                        rl_correct += 1

        total_predictions = len([1 for _ in range(test_cases) for _ in range(4)])

        sl_accuracy = sl_correct / max(total_predictions, 1)
        rl_accuracy = rl_correct / max(total_predictions, 1)

        print(f"æ•™å¸«ã‚ã‚Šå­¦ç¿’: {sl_accuracy:.1%} ({sl_correct}/{total_predictions})")
        print(f"å¼·åŒ–å­¦ç¿’:     {rl_accuracy:.1%} ({rl_correct}/{total_predictions})")

        return {
            "sl_accuracy": sl_accuracy,
            "rl_accuracy": rl_accuracy,
            "sl_correct": sl_correct,
            "rl_correct": rl_correct,
            "total": total_predictions,
        }

    def _prepare_board_tensor(self, board: np.ndarray, player: str) -> torch.Tensor:
        """ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ï¼ˆå…±é€šãƒ¡ã‚½ãƒƒãƒ‰ï¼‰"""
        tensor = torch.zeros(1, 3, 6, 6)
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val

        tensor[0, 0] = torch.from_numpy((board == player_val).astype(np.float32))
        tensor[0, 1] = torch.from_numpy((board == enemy_val).astype(np.float32))
        tensor[0, 2] = torch.from_numpy((board == 0).astype(np.float32))

        return tensor

    def plot_comparison(self):
        """å­¦ç¿’æ›²ç·šã‚’æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ"""
        if not self.rl_trainer.training_history["episodes"]:
            print("âš ï¸ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        # å ±é…¬æ¨ç§»
        ax1.plot(
            self.rl_trainer.training_history["episodes"], self.rl_trainer.training_history["rewards"], "b-", alpha=0.7
        )
        ax1.set_xlabel("Episode")
        ax1.set_ylabel("Total Reward")
        ax1.set_title("å¼·åŒ–å­¦ç¿’: å ±é…¬æ¨ç§»")
        ax1.grid(True)

        # æå¤±æ¨ç§»
        ax2.plot(
            self.rl_trainer.training_history["episodes"], self.rl_trainer.training_history["losses"], "r-", alpha=0.7
        )
        ax2.set_xlabel("Episode")
        ax2.set_ylabel("Loss")
        ax2.set_title("å¼·åŒ–å­¦ç¿’: æå¤±æ¨ç§»")
        ax2.grid(True)

        plt.tight_layout()
        plt.savefig("rl_vs_sl_comparison.png")
        print("ğŸ“Š ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: rl_vs_sl_comparison.png")


# ================================================================================
# Part 6: ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ================================================================================


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ å¼·åŒ–å­¦ç¿’ç‰ˆCQCNN vs æ•™å¸«ã‚ã‚Šå­¦ç¿’ç‰ˆCQCNN")
    print("=" * 70)

    # å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    experiment = ComparisonExperiment()

    print("\nã€å®Ÿé¨“ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€‘")
    print("1. å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’")
    print("2. ä¸¡ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ»æ¯”è¼ƒ")
    print("3. å®Œå…¨ãªæ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ")

    choice = input("\né¸æŠ (1-3): ").strip()

    if choice == "1":
        episodes = int(input("å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° (10-1000): ") or "100")
        experiment.train_rl_model(episodes=episodes)
        experiment.rl_trainer.save_checkpoint("rl_cqcnn_checkpoint.pth")

    elif choice == "2":
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
        if os.path.exists("rl_cqcnn_checkpoint.pth"):
            experiment.rl_trainer.load_checkpoint("rl_cqcnn_checkpoint.pth")

        experiment.evaluate_models(test_cases=100)

    elif choice == "3":
        print("\nğŸ“ å®Œå…¨ãªæ¯”è¼ƒå®Ÿé¨“")
        print("-" * 40)

        # å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        experiment.train_rl_model(episodes=200)

        # è©•ä¾¡
        print("\n" + "=" * 70)
        results = experiment.evaluate_models(test_cases=200)

        # ã‚°ãƒ©ãƒ•ä½œæˆ
        experiment.plot_comparison()

        # çµæœã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 70)
        print("ğŸ¯ å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 70)

        print("\næœ€çµ‚æ€§èƒ½æ¯”è¼ƒ:")
        print(f"  æ•™å¸«ã‚ã‚Šå­¦ç¿’: {results['sl_accuracy']:.1%}")
        print(f"  å¼·åŒ–å­¦ç¿’:     {results['rl_accuracy']:.1%}")

        if results["rl_accuracy"] > results["sl_accuracy"]:
            improvement = (results["rl_accuracy"] - results["sl_accuracy"]) * 100
            print(f"\nâœ¨ å¼·åŒ–å­¦ç¿’ãŒ {improvement:.1f}% å„ªã‚Œã¦ã„ã¾ã™ï¼")
        elif results["sl_accuracy"] > results["rl_accuracy"]:
            improvement = (results["sl_accuracy"] - results["rl_accuracy"]) * 100
            print(f"\nğŸ“š æ•™å¸«ã‚ã‚Šå­¦ç¿’ãŒ {improvement:.1f}% å„ªã‚Œã¦ã„ã¾ã™")
        else:
            print("\nğŸ¤ ä¸¡æ‰‹æ³•ã¯ã»ã¼åŒç­‰ã®æ€§èƒ½ã§ã™")

        # çµæœã‚’ä¿å­˜
        with open("comparison_results.json", "w") as f:
            json.dump(
                {
                    "results": results,
                    "rl_history": experiment.rl_trainer.training_history,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                },
                f,
                indent=2,
            )

        print("\nğŸ’¾ çµæœã‚’ä¿å­˜: comparison_results.json")
        print("ğŸ“Š ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: rl_vs_sl_comparison.png")

    else:
        print("ç„¡åŠ¹ãªé¸æŠã§ã™")

    print("\nâœ… å®Ÿé¨“å®Œäº†ï¼")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ å®Ÿè¡Œä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
