#!/usr/bin/env python3
"""
CQCNNå¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼æ‹¡å¼µç‰ˆ
é‡å­å›è·¯ã¨CNNã‚’çµ„ã¿åˆã‚ã›ãŸé§’æ¨å®šAIã«ã‚ˆã‚‹å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªç”±ã«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ„ã¿æ›¿ãˆã¦ã€ç‹¬è‡ªã®AIã‚’æ§‹ç¯‰å¯èƒ½
- åˆæœŸé…ç½®æˆ¦ç•¥
- æ•µé§’æ¨å®šå™¨ï¼ˆé‡å­/å¤å…¸ï¼‰
- è¡Œå‹•é¸æŠæˆ¦ç•¥
- Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import random
import json
import os
from datetime import datetime
from enum import Enum

# ================================================================================
# Part 1: åŸºæœ¬ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
# ================================================================================

class ModuleType(Enum):
    """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¿ã‚¤ãƒ—"""
    PLACEMENT = "placement"
    ESTIMATOR = "estimator"
    QMAP = "qmap"
    SELECTOR = "selector"


# ================================================================================
# Part 2: åˆæœŸé…ç½®æˆ¦ç•¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ================================================================================

class PlacementStrategy(ABC):
    """åˆæœŸé…ç½®æˆ¦ç•¥ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’å–å¾—"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """æˆ¦ç•¥åã‚’å–å¾—"""
        pass


class StandardPlacement(PlacementStrategy):
    """æ¨™æº–é…ç½®æˆ¦ç•¥"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}
        
        if player_id == "A":
            # å‰åˆ—ã«æ‚ªç‰ã€å¾Œåˆ—ã«å–„ç‰
            positions = [(0, j) for j in range(4)] + [(1, j) for j in range(4)]
            piece_types = ["bad"] * 4 + ["good"] * 4
        else:
            # å‰åˆ—ã«æ‚ªç‰ã€å¾Œåˆ—ã«å–„ç‰
            positions = [(4, j) for j in range(4)] + [(5, j) for j in range(4)]
            piece_types = ["bad"] * 4 + ["good"] * 4
        
        for pos, piece_type in zip(positions, piece_types):
            placement[pos] = piece_type
        
        return placement
    
    def get_name(self) -> str:
        return "æ¨™æº–é…ç½®"


class DefensivePlacement(PlacementStrategy):
    """å®ˆå‚™çš„é…ç½®æˆ¦ç•¥"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}
        
        if player_id == "A":
            # å–„ç‰ã‚’å¾Œæ–¹ã«ã€æ‚ªç‰ã‚’å‰æ–¹ã«é…ç½®
            positions = [(0, j) for j in range(4)] + [(1, j) for j in range(4)]
            piece_types = ["bad", "bad", "bad", "good"] + ["good", "good", "good", "bad"]
        else:
            positions = [(4, j) for j in range(4)] + [(5, j) for j in range(4)]
            piece_types = ["bad", "bad", "bad", "good"] + ["good", "good", "good", "bad"]
        
        for pos, piece_type in zip(positions, piece_types):
            placement[pos] = piece_type
        
        return placement
    
    def get_name(self) -> str:
        return "å®ˆå‚™çš„é…ç½®"


class AggressivePlacement(PlacementStrategy):
    """æ”»æ’ƒçš„é…ç½®æˆ¦ç•¥"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}
        
        if player_id == "A":
            # å–„ç‰ã‚’å‰æ–¹ã«é…ç½®ã—ã¦æ—©æœŸè„±å‡ºã‚’ç‹™ã†
            positions = [(0, j) for j in range(4)] + [(1, j) for j in range(4)]
            piece_types = ["good", "good", "bad", "bad"] + ["bad", "bad", "good", "good"]
        else:
            positions = [(4, j) for j in range(4)] + [(5, j) for j in range(4)]
            piece_types = ["good", "good", "bad", "bad"] + ["bad", "bad", "good", "good"]
        
        for pos, piece_type in zip(positions, piece_types):
            placement[pos] = piece_type
        
        return placement
    
    def get_name(self) -> str:
        return "æ”»æ’ƒçš„é…ç½®"


class RandomPlacement(PlacementStrategy):
    """ãƒ©ãƒ³ãƒ€ãƒ é…ç½®æˆ¦ç•¥"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}
        
        if player_id == "A":
            positions = [(i, j) for i in range(2) for j in range(4)]
        else:
            positions = [(i+4, j) for i in range(2) for j in range(4)]
        
        piece_types = ["good"] * 4 + ["bad"] * 4
        random.shuffle(piece_types)
        
        for pos, piece_type in zip(positions, piece_types):
            placement[pos] = piece_type
        
        return placement
    
    def get_name(self) -> str:
        return "ãƒ©ãƒ³ãƒ€ãƒ é…ç½®"


# ================================================================================
# Part 3: æ•µé§’æ¨å®šå™¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ================================================================================

class EstimatorModule(ABC):
    """æ•µé§’æ¨å®šå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def estimate(self, board: np.ndarray, 
                enemy_positions: List[Tuple[int, int]], 
                player: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        """æ•µé§’ã‚’æ¨å®š"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """æ¨å®šå™¨åã‚’å–å¾—"""
        pass
    
    def train(self, data: List[Dict]) -> None:
        """å­¦ç¿’ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
        pass


class QuantumCircuitLayer(nn.Module):
    """é‡å­å›è·¯å±¤"""
    
    def __init__(self, n_qubits: int = 6, n_layers: int = 3):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        
        self.rotation_params = nn.Parameter(
            torch.randn(n_layers, n_qubits, 3) * 0.1
        )
        self.entanglement_params = nn.Parameter(
            torch.randn(n_layers, n_qubits - 1) * 0.1
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.shape[0]
        
        # åˆæœŸçŠ¶æ…‹
        state_real = torch.ones(batch_size, self.n_qubits)
        state_imag = torch.zeros(batch_size, self.n_qubits)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        for i in range(min(self.n_qubits, x.shape[1])):
            angle = x[:, i] * np.pi
            new_state_real = state_real.clone()
            new_state_imag = state_imag.clone()
            new_state_real[:, i] = torch.cos(angle/2)
            new_state_imag[:, i] = torch.sin(angle/2)
            state_real = new_state_real
            state_imag = new_state_imag
        
        # å¤‰åˆ†å›è·¯
        for layer in range(self.n_layers):
            new_state_real = state_real.clone()
            new_state_imag = state_imag.clone()
            
            # å›è»¢ã‚²ãƒ¼ãƒˆ
            for qubit in range(self.n_qubits):
                rx = self.rotation_params[layer, qubit, 0]
                q_real = state_real[:, qubit]
                q_imag = state_imag[:, qubit]
                
                cos_rx = torch.cos(rx/2)
                sin_rx = torch.sin(rx/2)
                new_q_real = cos_rx * q_real + sin_rx * q_imag
                new_q_imag = cos_rx * q_imag - sin_rx * q_real
                
                new_state_real[:, qubit] = new_q_real
                new_state_imag[:, qubit] = new_q_imag
            
            state_real = new_state_real
            state_imag = new_state_imag
            
            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
            entangled_real = state_real.clone()
            for i in range(self.n_qubits - 1):
                strength = torch.sigmoid(self.entanglement_params[layer, i])
                avg = (state_real[:, i] + state_real[:, i+1]) / 2
                entangled_real[:, i] = (1 - strength) * state_real[:, i] + strength * avg
                entangled_real[:, i+1] = (1 - strength) * state_real[:, i+1] + strength * avg
            state_real = entangled_real
        
        measurements = torch.sqrt(state_real**2 + state_imag**2)
        return measurements


class CQCNNEstimator(EstimatorModule):
    """CQCNNï¼ˆé‡å­å›è·¯ï¼‰æ¨å®šå™¨"""
    
    def __init__(self, n_qubits: int = 6, n_layers: int = 3):
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.model = self._build_model()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
    
    def _build_model(self):
        """CQCNNãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰"""
        class CQCNNModel(nn.Module):
            def __init__(self, n_qubits, n_layers):
                super().__init__()
                
                # CNNç‰¹å¾´æŠ½å‡º
                self.cnn = nn.Sequential(
                    nn.Conv2d(5, 16, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(16),
                    nn.Conv2d(16, 32, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Conv2d(32, 64, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.Flatten()
                )
                
                # æ¬¡å…ƒå‰Šæ¸›
                self.reduction = nn.Sequential(
                    nn.Linear(64 * 3 * 3, 64),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(64, n_qubits),
                    nn.Tanh()
                )
                
                # é‡å­å›è·¯
                self.quantum = QuantumCircuitLayer(n_qubits, n_layers)
                
                # å‡ºåŠ›å±¤
                self.output = nn.Sequential(
                    nn.Linear(n_qubits, 32),
                    nn.ReLU(),
                    nn.Linear(32, 2)
                )
            
            def forward(self, x):
                x = self.cnn(x)
                x = self.reduction(x)
                x = self.quantum(x)
                x = self.output(x)
                return x
        
        return CQCNNModel(self.n_qubits, self.n_layers)
    
    def estimate(self, board: np.ndarray, 
                enemy_positions: List[Tuple[int, int]], 
                player: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        self.model.eval()
        results = {}
        
        # ãƒœãƒ¼ãƒ‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        board_tensor = self._prepare_board_tensor(board, player)
        
        with torch.no_grad():
            output = self.model(board_tensor)
            probs = F.softmax(output, dim=1)
            
            for i, pos in enumerate(enemy_positions):
                position_factor = (pos[0] / 5.0 + pos[1] / 5.0) / 2.0
                adjusted_probs = probs[0] * (0.8 + 0.2 * position_factor)
                adjusted_probs = adjusted_probs / adjusted_probs.sum()
                
                results[pos] = {
                    'good_prob': adjusted_probs[0].item(),
                    'bad_prob': adjusted_probs[1].item(),
                    'confidence': max(adjusted_probs.tolist())
                }
        
        return results
    
    def _prepare_board_tensor(self, board: np.ndarray, player: str) -> torch.Tensor:
        tensor = torch.zeros(1, 5, 6, 6)
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        tensor[0, 0] = torch.from_numpy((board == player_val).astype(np.float32))
        tensor[0, 1] = torch.from_numpy((board == enemy_val).astype(np.float32))
        tensor[0, 2] = torch.from_numpy((board == 0).astype(np.float32))
        
        if player == "A":
            tensor[0, 3, 0, 5] = 1.0
            tensor[0, 3, 5, 5] = 1.0
            tensor[0, 4, 0, 0] = 1.0
            tensor[0, 4, 5, 0] = 1.0
        else:
            tensor[0, 3, 0, 0] = 1.0
            tensor[0, 3, 5, 0] = 1.0
            tensor[0, 4, 0, 5] = 1.0
            tensor[0, 4, 5, 5] = 1.0
        
        return tensor
    
    def get_name(self) -> str:
        return f"CQCNN(q={self.n_qubits},l={self.n_layers})"


class SimpleCNNEstimator(EstimatorModule):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªCNNæ¨å®šå™¨ï¼ˆé‡å­å›è·¯ãªã—ï¼‰"""
    
    def __init__(self):
        self.model = self._build_model()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
    
    def _build_model(self):
        return nn.Sequential(
            nn.Conv2d(5, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32 * 6 * 6, 64),
            nn.ReLU(),
            nn.Linear(64, 2)
        )
    
    def estimate(self, board: np.ndarray, 
                enemy_positions: List[Tuple[int, int]], 
                player: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        self.model.eval()
        results = {}
        
        board_tensor = self._prepare_board_tensor(board, player)
        
        with torch.no_grad():
            output = self.model(board_tensor)
            probs = F.softmax(output, dim=1)
            
            for pos in enemy_positions:
                results[pos] = {
                    'good_prob': probs[0, 0].item(),
                    'bad_prob': probs[0, 1].item(),
                    'confidence': max(probs[0].tolist())
                }
        
        return results
    
    def _prepare_board_tensor(self, board: np.ndarray, player: str) -> torch.Tensor:
        # CQCNNEstimatorã¨åŒã˜å‡¦ç†
        tensor = torch.zeros(1, 5, 6, 6)
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        tensor[0, 0] = torch.from_numpy((board == player_val).astype(np.float32))
        tensor[0, 1] = torch.from_numpy((board == enemy_val).astype(np.float32))
        tensor[0, 2] = torch.from_numpy((board == 0).astype(np.float32))
        
        if player == "A":
            tensor[0, 3, 0, 5] = 1.0
            tensor[0, 3, 5, 5] = 1.0
            tensor[0, 4, 0, 0] = 1.0
            tensor[0, 4, 5, 0] = 1.0
        else:
            tensor[0, 3, 0, 0] = 1.0
            tensor[0, 3, 5, 0] = 1.0
            tensor[0, 4, 0, 5] = 1.0
            tensor[0, 4, 5, 5] = 1.0
        
        return tensor
    
    def get_name(self) -> str:
        return "SimpleCNN"


class RandomEstimator(EstimatorModule):
    """ãƒ©ãƒ³ãƒ€ãƒ æ¨å®šå™¨"""
    
    def estimate(self, board: np.ndarray, 
                enemy_positions: List[Tuple[int, int]], 
                player: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        results = {}
        
        for pos in enemy_positions:
            good_prob = random.random()
            bad_prob = 1 - good_prob
            
            results[pos] = {
                'good_prob': good_prob,
                'bad_prob': bad_prob,
                'confidence': max(good_prob, bad_prob)
            }
        
        return results
    
    def get_name(self) -> str:
        return "ãƒ©ãƒ³ãƒ€ãƒ "


# ================================================================================
# Part 4: Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ================================================================================

class QMapGenerator(ABC):
    """Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def generate(self, board: np.ndarray, 
                estimations: Dict, 
                my_pieces: Dict,
                player: str) -> np.ndarray:
        """Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """ç”Ÿæˆå™¨åã‚’å–å¾—"""
        pass


class SimpleQMapGenerator(QMapGenerator):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªQå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def generate(self, board: np.ndarray, 
                estimations: Dict, 
                my_pieces: Dict,
                player: str) -> np.ndarray:
        q_map = np.zeros((6, 6, 4))  # 4æ–¹å‘
        
        for pos, piece_type in my_pieces.items():
            base_value = 1.0 if piece_type == "good" else 0.5
            
            # å„æ–¹å‘ã®Qå€¤ã‚’è¨­å®š
            for i, (dx, dy) in enumerate([(0, 1), (0, -1), (1, 0), (-1, 0)]):
                new_pos = (pos[0] + dx, pos[1] + dy)
                
                if 0 <= new_pos[0] < 6 and 0 <= new_pos[1] < 6:
                    # åŸºæœ¬Qå€¤
                    q_value = base_value
                    
                    # æ•µé§’ãŒã„ã‚‹å ´åˆ
                    if new_pos in estimations:
                        est = estimations[new_pos]
                        if piece_type == "bad":
                            # æ‚ªç‰ã¯æ•µã‚’å–ã‚‹
                            q_value += est['good_prob'] * 2.0 + est['bad_prob'] * 1.0
                        else:
                            # å–„ç‰ã¯æ•µã‚’é¿ã‘ã‚‹
                            q_value -= est['bad_prob'] * 0.5
                    
                    q_map[pos[0], pos[1], i] = q_value
        
        return q_map
    
    def get_name(self) -> str:
        return "ã‚·ãƒ³ãƒ—ãƒ«Qå€¤"


class StrategicQMapGenerator(QMapGenerator):
    """æˆ¦ç•¥çš„Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def generate(self, board: np.ndarray, 
                estimations: Dict, 
                my_pieces: Dict,
                player: str) -> np.ndarray:
        q_map = np.zeros((6, 6, 4))
        
        # è„±å‡ºå£ã®ä½ç½®
        if player == "A":
            escape_positions = [(0, 5), (5, 5)]
        else:
            escape_positions = [(0, 0), (5, 0)]
        
        for pos, piece_type in my_pieces.items():
            for i, (dx, dy) in enumerate([(0, 1), (0, -1), (1, 0), (-1, 0)]):
                new_pos = (pos[0] + dx, pos[1] + dy)
                
                if not (0 <= new_pos[0] < 6 and 0 <= new_pos[1] < 6):
                    continue
                
                q_value = 0.0
                
                if piece_type == "good":
                    # å–„ç‰: è„±å‡ºã‚’å„ªå…ˆ
                    min_dist_before = min(abs(pos[0] - ep[0]) + abs(pos[1] - ep[1]) 
                                         for ep in escape_positions)
                    min_dist_after = min(abs(new_pos[0] - ep[0]) + abs(new_pos[1] - ep[1]) 
                                        for ep in escape_positions)
                    
                    if min_dist_after < min_dist_before:
                        q_value += 3.0
                    
                    if new_pos in escape_positions:
                        q_value += 10.0
                else:
                    # æ‚ªç‰: æ•µé§’æ’ƒç ´ã‚’å„ªå…ˆ
                    if new_pos in estimations:
                        est = estimations[new_pos]
                        q_value += est['good_prob'] * 3.0 + est['bad_prob'] * 1.5
                
                q_map[pos[0], pos[1], i] = q_value
        
        return q_map
    
    def get_name(self) -> str:
        return "æˆ¦ç•¥çš„Qå€¤"


# ================================================================================
# Part 5: è¡Œå‹•é¸æŠå™¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ================================================================================

class ActionSelector(ABC):
    """è¡Œå‹•é¸æŠå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def select(self, q_map: np.ndarray, 
              legal_moves: List[Tuple],
              temperature: float = 1.0) -> Tuple:
        """è¡Œå‹•ã‚’é¸æŠ"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """é¸æŠå™¨åã‚’å–å¾—"""
        pass


class GreedySelector(ActionSelector):
    """è²ªæ¬²é¸æŠå™¨"""
    
    def select(self, q_map: np.ndarray, 
              legal_moves: List[Tuple],
              temperature: float = 1.0) -> Tuple:
        if not legal_moves:
            return None
        
        best_move = None
        best_value = -float('inf')
        
        for from_pos, to_pos in legal_moves:
            # æ–¹å‘ã‚’è¨ˆç®—
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            dir_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3}
            if (dx, dy) in dir_map:
                dir_idx = dir_map[(dx, dy)]
                value = q_map[from_pos[0], from_pos[1], dir_idx]
                
                if value > best_value:
                    best_value = value
                    best_move = (from_pos, to_pos)
        
        return best_move or random.choice(legal_moves)
    
    def get_name(self) -> str:
        return "è²ªæ¬²é¸æŠ"


class EpsilonGreedySelector(ActionSelector):
    """Îµ-è²ªæ¬²é¸æŠå™¨"""
    
    def __init__(self, epsilon: float = 0.1):
        self.epsilon = epsilon
    
    def select(self, q_map: np.ndarray, 
              legal_moves: List[Tuple],
              temperature: float = 1.0) -> Tuple:
        if not legal_moves:
            return None
        
        if random.random() < self.epsilon:
            return random.choice(legal_moves)
        
        # è²ªæ¬²é¸æŠ
        greedy = GreedySelector()
        return greedy.select(q_map, legal_moves, temperature)
    
    def get_name(self) -> str:
        return f"Îµè²ªæ¬²(Îµ={self.epsilon})"


class SoftmaxSelector(ActionSelector):
    """ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é¸æŠå™¨"""
    
    def select(self, q_map: np.ndarray, 
              legal_moves: List[Tuple],
              temperature: float = 1.0) -> Tuple:
        if not legal_moves:
            return None
        
        q_values = []
        dir_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3}
        
        for from_pos, to_pos in legal_moves:
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            if (dx, dy) in dir_map:
                dir_idx = dir_map[(dx, dy)]
                value = q_map[from_pos[0], from_pos[1], dir_idx]
            else:
                value = 0.0
            
            q_values.append(value / temperature)
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç¢ºç‡
        q_tensor = torch.tensor(q_values, dtype=torch.float32)
        probs = F.softmax(q_tensor, dim=0).numpy()
        
        # ç¢ºç‡çš„ã«é¸æŠ
        idx = np.random.choice(len(legal_moves), p=probs)
        return legal_moves[idx]
    
    def get_name(self) -> str:
        return "ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹"


# ================================================================================
# Part 6: ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
# ================================================================================

@dataclass
class AgentConfig:
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š"""
    placement: PlacementStrategy
    estimator: EstimatorModule
    qmap_generator: QMapGenerator
    action_selector: ActionSelector


class ModularAgent:
    """ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼å‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, player_id: str, config: AgentConfig):
        self.player_id = player_id
        self.config = config
        self.name = self._generate_name()
        self.game_history = []
    
    def _generate_name(self) -> str:
        return f"Agent_{self.player_id}[{self.config.placement.get_name()[:3]}+" \
               f"{self.config.estimator.get_name()[:8]}+" \
               f"{self.config.qmap_generator.get_name()[:3]}+" \
               f"{self.config.action_selector.get_name()[:3]}]"
    
    def get_initial_placement(self) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’å–å¾—"""
        return self.config.placement.get_placement(self.player_id)
    
    def get_move(self, game_state, legal_moves: List[Tuple]) -> Tuple:
        """æ¬¡ã®æ‰‹ã‚’å–å¾—"""
        if not legal_moves:
            return None
        
        try:
            # æ•µé§’ä½ç½®ã‚’ç‰¹å®š
            enemy_positions = self._find_enemy_positions(game_state)
            
            # æ•µé§’ã‚’æ¨å®š
            estimations = {}
            if enemy_positions:
                estimations = self.config.estimator.estimate(
                    game_state.board,
                    enemy_positions,
                    self.player_id
                )
            
            # è‡ªåˆ†ã®é§’æƒ…å ±
            my_pieces = game_state.player_a_pieces if self.player_id == "A" else game_state.player_b_pieces
            
            # Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
            q_map = self.config.qmap_generator.generate(
                game_state.board,
                estimations,
                my_pieces,
                self.player_id
            )
            
            # è¡Œå‹•ã‚’é¸æŠ
            move = self.config.action_selector.select(q_map, legal_moves)
            
            # å±¥æ­´ã«è¨˜éŒ²
            self.game_history.append({
                'turn': game_state.turn,
                'move': move,
                'estimations': len(estimations),
                'q_max': np.max(q_map) if q_map is not None else 0
            })
            
            return move
            
        except Exception as e:
            print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ in {self.name}: {e}")
            return random.choice(legal_moves)
    
    def _find_enemy_positions(self, game_state) -> List[Tuple[int, int]]:
        """æ•µé§’ã®ä½ç½®ã‚’ç‰¹å®š"""
        enemy_val = -1 if self.player_id == "A" else 1
        positions = []
        
        for i in range(game_state.board.shape[0]):
            for j in range(game_state.board.shape[1]):
                if game_state.board[i, j] == enemy_val:
                    positions.append((i, j))
        
        return positions


# ================================================================================
# Part 7: ã‚²ãƒ¼ãƒ ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================

class GameState:
    """ã‚²ãƒ¼ãƒ çŠ¶æ…‹"""
    def __init__(self):
        self.board = np.zeros((6, 6), dtype=int)
        self.turn = 0
        self.player_a_pieces = {}
        self.player_b_pieces = {}
        self.winner = None
    
    def is_game_over(self):
        return self.winner is not None or self.turn >= 100


class GameEngine:
    """ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.state = GameState()
        self.move_history = []
    
    def get_legal_moves(self, player: str) -> List[Tuple]:
        """åˆæ³•æ‰‹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        legal_moves = []
        pieces = self.state.player_a_pieces if player == "A" else self.state.player_b_pieces
        
        for pos, piece_type in pieces.items():
            for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
                new_pos = (pos[0] + dx, pos[1] + dy)
                if self._is_valid_move(pos, new_pos, player):
                    legal_moves.append((pos, new_pos))
        
        return legal_moves
    
    def _is_valid_move(self, from_pos: Tuple, to_pos: Tuple, player: str) -> bool:
        if not (0 <= to_pos[0] < 6 and 0 <= to_pos[1] < 6):
            return False
        
        player_val = 1 if player == "A" else -1
        if self.state.board[to_pos] == player_val:
            return False
        
        return True
    
    def make_move(self, from_pos: Tuple, to_pos: Tuple, player: str):
        """æ‰‹ã‚’å®Ÿè¡Œ"""
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        # æ•µé§’ã‚’å–ã‚‹å ´åˆ
        if self.state.board[to_pos] == enemy_val:
            enemy_pieces = self.state.player_b_pieces if player == "A" else self.state.player_a_pieces
            if to_pos in enemy_pieces:
                del enemy_pieces[to_pos]
        
        # ç§»å‹•
        self.state.board[from_pos] = 0
        self.state.board[to_pos] = player_val
        
        pieces = self.state.player_a_pieces if player == "A" else self.state.player_b_pieces
        piece_type = pieces.pop(from_pos)
        pieces[to_pos] = piece_type
        
        # è„±å‡ºåˆ¤å®š
        escape_positions = [(0, 5), (5, 5)] if player == "A" else [(0, 0), (5, 0)]
        if to_pos in escape_positions and piece_type == "good":
            self.state.winner = player
        
        self.state.turn += 1


# ================================================================================
# Part 8: ç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================

class CompetitionRunner:
    """ç«¶æŠ€å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        self.modules = {
            'placement': [
                StandardPlacement(),
                DefensivePlacement(),
                AggressivePlacement(),
                RandomPlacement()
            ],
            'estimator': [
                CQCNNEstimator(n_qubits=4, n_layers=2),
                CQCNNEstimator(n_qubits=6, n_layers=3),
                SimpleCNNEstimator(),
                RandomEstimator()
            ],
            'qmap': [
                SimpleQMapGenerator(),
                StrategicQMapGenerator()
            ],
            'selector': [
                GreedySelector(),
                EpsilonGreedySelector(epsilon=0.1),
                EpsilonGreedySelector(epsilon=0.3),
                SoftmaxSelector()
            ]
        }
        
        self.match_results = []
    
    def show_modules(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¡¨ç¤º"""
        print("=" * 70)
        print("ğŸ® CQCNNç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ  - åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«")
        print("=" * 70)
        
        print("\nã€1. åˆæœŸé…ç½®æˆ¦ç•¥ã€‘")
        for i, module in enumerate(self.modules['placement']):
            print(f"  {i}: {module.get_name()}")
        
        print("\nã€2. æ•µé§’æ¨å®šå™¨ã€‘")
        for i, module in enumerate(self.modules['estimator']):
            print(f"  {i}: {module.get_name()}")
        
        print("\nã€3. Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ã€‘")
        for i, module in enumerate(self.modules['qmap']):
            print(f"  {i}: {module.get_name()}")
        
        print("\nã€4. è¡Œå‹•é¸æŠå™¨ã€‘")
        for i, module in enumerate(self.modules['selector']):
            print(f"  {i}: {module.get_name()}")
    
    def create_agent(self, player_id: str, 
                    placement_idx: int = 0,
                    estimator_idx: int = 0,
                    qmap_idx: int = 0,
                    selector_idx: int = 0) -> ModularAgent:
        """æŒ‡å®šã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ"""
        config = AgentConfig(
            placement=self.modules['placement'][placement_idx],
            estimator=self.modules['estimator'][estimator_idx],
            qmap_generator=self.modules['qmap'][qmap_idx],
            action_selector=self.modules['selector'][selector_idx]
        )
        
        return ModularAgent(player_id, config)
    
    def run_match(self, agent1: ModularAgent, agent2: ModularAgent, verbose: bool = False):
        """1è©¦åˆã‚’å®Ÿè¡Œ"""
        engine = GameEngine()
        
        # åˆæœŸé…ç½®
        placement1 = agent1.get_initial_placement()
        placement2 = agent2.get_initial_placement()
        
        for pos, piece_type in placement1.items():
            engine.state.board[pos] = 1
            engine.state.player_a_pieces[pos] = piece_type
        
        for pos, piece_type in placement2.items():
            engine.state.board[pos] = -1
            engine.state.player_b_pieces[pos] = piece_type
        
        if verbose:
            print(f"\nå¯¾æˆ¦: {agent1.name} vs {agent2.name}")
        
        # ã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ—
        current_player = "A"
        current_agent = agent1
        
        while not engine.state.is_game_over():
            legal_moves = engine.get_legal_moves(current_player)
            
            if not legal_moves:
                break
            
            move = current_agent.get_move(engine.state, legal_moves)
            
            if move:
                engine.make_move(move[0], move[1], current_player)
            
            # å‹è€…åˆ¤å®š
            if engine.state.winner:
                break
            
            # ã‚¿ãƒ¼ãƒ³äº¤ä»£
            current_player = "B" if current_player == "A" else "A"
            current_agent = agent2 if current_agent == agent1 else agent1
        
        result = {
            'winner': engine.state.winner or "Draw",
            'turns': engine.state.turn,
            'agent1': agent1.name,
            'agent2': agent2.name
        }
        
        self.match_results.append(result)
        
        if verbose:
            print(f"å‹è€…: {result['winner']}, ã‚¿ãƒ¼ãƒ³æ•°: {result['turns']}")
        
        return result
    
    def run_tournament(self, n_agents: int = 4):
        """ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
        print("\nğŸ† ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé–‹å§‹")
        print("=" * 70)
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç”Ÿæˆ
        agents = []
        for i in range(n_agents):
            agent = self.create_agent(
                "A" if i % 2 == 0 else "B",
                placement_idx=random.randint(0, len(self.modules['placement'])-1),
                estimator_idx=random.randint(0, len(self.modules['estimator'])-1),
                qmap_idx=random.randint(0, len(self.modules['qmap'])-1),
                selector_idx=random.randint(0, len(self.modules['selector'])-1)
            )
            agents.append(agent)
            print(f"Agent {i+1}: {agent.name}")
        
        # ç·å½“ãŸã‚Šæˆ¦
        results = {agent.name: {'wins': 0, 'losses': 0, 'draws': 0} for agent in agents}
        
        for i, agent1 in enumerate(agents):
            for j, agent2 in enumerate(agents):
                if i >= j:
                    continue
                
                # å…ˆæ‰‹å¾Œæ‰‹ã‚’å…¥ã‚Œæ›¿ãˆã¦2è©¦åˆ
                for swap in [False, True]:
                    if swap:
                        a1, a2 = agent2, agent1
                    else:
                        a1, a2 = agent1, agent2
                    
                    result = self.run_match(a1, a2)
                    
                    if result['winner'] == "A":
                        results[a1.name]['wins'] += 1
                        results[a2.name]['losses'] += 1
                    elif result['winner'] == "B":
                        results[a1.name]['losses'] += 1
                        results[a2.name]['wins'] += 1
                    else:
                        results[a1.name]['draws'] += 1
                        results[a2.name]['draws'] += 1
        
        # çµæœè¡¨ç¤º
        print("\nğŸ“Š ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆçµæœ")
        print("=" * 70)
        
        sorted_results = sorted(results.items(), 
                              key=lambda x: (x[1]['wins'], -x[1]['losses']), 
                              reverse=True)
        
        for rank, (name, stats) in enumerate(sorted_results, 1):
            total = stats['wins'] + stats['losses'] + stats['draws']
            win_rate = stats['wins'] / total if total > 0 else 0
            print(f"{rank}ä½: {name}")
            print(f"    å‹:{stats['wins']} è² :{stats['losses']} åˆ†:{stats['draws']} (å‹ç‡:{win_rate:.1%})")
        
        return sorted_results[0][0]  # å„ªå‹è€…ã‚’è¿”ã™


# ================================================================================
# Part 9: ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
# ================================================================================

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ """
    print("=" * 70)
    print("ğŸŒŸ CQCNNå¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼æ‹¡å¼µç‰ˆ")
    print("=" * 70)
    
    runner = CompetitionRunner()
    
    while True:
        print("\nğŸ“‹ ãƒ¡ãƒ‹ãƒ¥ãƒ¼:")
        print("1. åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¡¨ç¤º")
        print("2. ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾æˆ¦")
        print("3. ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆ")
        print("4. æœ€å¼·æ§‹æˆã‚’æ¢ç´¢")
        print("5. çµ‚äº†")
        
        choice = input("\né¸æŠ (1-5): ")
        
        if choice == "1":
            runner.show_modules()
        
        elif choice == "2":
            print("\nğŸ® ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾æˆ¦")
            runner.show_modules()
            
            print("\nã€Agent 1ã®æ§‹æˆã€‘")
            p1 = int(input("é…ç½®æˆ¦ç•¥ (0-3): "))
            e1 = int(input("æ¨å®šå™¨ (0-3): "))
            q1 = int(input("Qå€¤ç”Ÿæˆ (0-1): "))
            s1 = int(input("é¸æŠå™¨ (0-3): "))
            
            print("\nã€Agent 2ã®æ§‹æˆã€‘")
            p2 = int(input("é…ç½®æˆ¦ç•¥ (0-3): "))
            e2 = int(input("æ¨å®šå™¨ (0-3): "))
            q2 = int(input("Qå€¤ç”Ÿæˆ (0-1): "))
            s2 = int(input("é¸æŠå™¨ (0-3): "))
            
            agent1 = runner.create_agent("A", p1, e1, q1, s1)
            agent2 = runner.create_agent("B", p2, e2, q2, s2)
            
            print(f"\nå¯¾æˆ¦è¨­å®šå®Œäº†:")
            print(f"Agent 1: {agent1.name}")
            print(f"Agent 2: {agent2.name}")
            
            result = runner.run_match(agent1, agent2, verbose=True)
        
        elif choice == "3":
            n = int(input("å‚åŠ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•° (2-8): "))
            winner = runner.run_tournament(n)
            print(f"\nğŸ† å„ªå‹: {winner}")
        
        elif choice == "4":
            print("\nğŸ” æœ€å¼·æ§‹æˆã‚’æ¢ç´¢ä¸­...")
            best_configs = []
            
            for _ in range(10):  # 10å›ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆ
                winner = runner.run_tournament(6)
                best_configs.append(winner)
            
            from collections import Counter
            counter = Counter(best_configs)
            print("\nğŸ“Š æœ€å¼·æ§‹æˆãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
            for config, count in counter.most_common(3):
                print(f"  {config}: {count}å›å„ªå‹")
        
        elif choice == "5":
            print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")
            break
    
    print("\n" + "=" * 70)
    print("âœ¨ ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´:")
    print("  â€¢ ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆã§è‡ªç”±ãªçµ„ã¿åˆã‚ã›")
    print("  â€¢ é‡å­å›è·¯ã«ã‚ˆã‚‹é«˜åº¦ãªæ¨å®š")
    print("  â€¢ æˆ¦ç•¥çš„ãªè¡Œå‹•é¸æŠ")
    print("  â€¢ ç«¶æŠ€ã«ã‚ˆã‚‹æœ€é©åŒ–")
    print("=" * 70)


if __name__ == "__main__":
    main()
