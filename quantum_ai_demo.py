#!/usr/bin/env python3
"""
ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³é‡å­ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AI ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¿®æ­£ç‰ˆï¼‰
æ­£ã—ã„è„±å‡ºãƒ«ãƒ¼ãƒ«å¯¾å¿œ - BaseAIå®Œå…¨äº’æ›
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import torch
import random
import time
import json
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
sys.path.append('.')
sys.path.append('src')

try:
    import pennylane as qml
    QUANTUM_AVAILABLE = True
    print("âœ… PennyLaneåˆ©ç”¨å¯èƒ½ - é‡å­å›è·¯ã§å®Ÿè¡Œ")
except ImportError:
    QUANTUM_AVAILABLE = False
    print("âš ï¸  PennyLaneæœªå¯¾å¿œ - å¤å…¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ä»£æ›¿")

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from qugeister_competitive.game_engine import GeisterGame
from qugeister_competitive.ai_base import BaseAI, RandomAI, SimpleAI, AggressiveAI
from qugeister_competitive.tournament import TournamentManager

class QuantumAI(BaseAI):
    """é‡å­AIï¼ˆæ­£ã—ã„è„±å‡ºãƒ«ãƒ¼ãƒ«å¯¾å¿œç‰ˆï¼‰"""
    
    def __init__(self, player_id="A", n_qubits=8, n_layers=3):
        # BaseAIåˆæœŸåŒ–
        super().__init__("QuantumAI", player_id)
        
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.epsilon = 0.8          # åˆæœŸæ¢ç´¢ç‡
        self.epsilon_min = 0.01     # æœ€å°æ¢ç´¢ç‡
        self.epsilon_decay = 0.995  # æ¸›è¡°ç‡
        self.learning_rate = 0.01  # å­¦ç¿’ç‡
        
        # çµŒé¨“è¨˜æ†¶
        self.memory = []
        self.max_memory = 1000
        
        if QUANTUM_AVAILABLE:
            self._setup_quantum_circuit()
        else:
            self._setup_classical_network()
        
        print(f"ğŸ§  {self.name} åˆæœŸåŒ–å®Œäº† (Player {player_id})")
        print(f"   é‡å­ãƒ“ãƒƒãƒˆ: {n_qubits}, ãƒ¬ã‚¤ãƒ¤ãƒ¼: {n_layers}")
        print(f"   ãƒ¢ãƒ¼ãƒ‰: {'Quantum' if QUANTUM_AVAILABLE else 'Classical'}")
        print(f"   è„±å‡ºå£: {self._get_escape_positions()}")
    
    def _get_escape_positions(self):
        """ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è„±å‡ºå£å–å¾—"""
        if self.player_id == "A":
            return [(0, 5), (5, 5)]  # ç›¸æ‰‹é™£åœ°
        else:
            return [(0, 0), (5, 0)]  # ç›¸æ‰‹é™£åœ°
    
    def _setup_quantum_circuit(self):
        """é‡å­å›è·¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            self.dev = qml.device('lightning.qubit', wires=self.n_qubits)
            print("âš¡ Lightning.Qubit ãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨")
        except:
            self.dev = qml.device('default.qubit', wires=self.n_qubits)
            print("âš ï¸  default.qubit ãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨")
        
        self.params = torch.randn(self.n_layers, self.n_qubits, 3, requires_grad=True)
        
        @qml.qnode(self.dev, interface='torch', diff_method='adjoint')
        def quantum_circuit(inputs, params):
            # çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            for i in range(self.n_qubits):
                qml.RY(inputs[i], wires=i)
            
            # å¤‰åˆ†å›è·¯
            for layer in range(self.n_layers):
                for i in range(self.n_qubits):
                    qml.RX(params[layer, i, 0], wires=i)
                    qml.RY(params[layer, i, 1], wires=i)
                    qml.RZ(params[layer, i, 2], wires=i)
                
                # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
                for i in range(self.n_qubits - 1):
                    qml.CNOT(wires=[i, i + 1])
                if self.n_qubits > 2:
                    qml.CNOT(wires=[self.n_qubits - 1, 0])
            
            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]
        
        self.quantum_circuit = quantum_circuit
        self.optimizer = torch.optim.Adam([self.params], lr=self.learning_rate)
        print("âš›ï¸  é‡å­å›è·¯åˆæœŸåŒ–å®Œäº†")
    
    def _setup_classical_network(self):
        """å¤å…¸ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        import torch.nn as nn
        
        self.network = nn.Sequential(
            nn.Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.Linear(16, 4)
        )
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)
        print("ğŸ–¥ï¸  å¤å…¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–å®Œäº†")
    
    def encode_state(self, game_state):
        """ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆè„±å‡ºæƒ…å ±å«ã‚€ï¼‰"""
        board = game_state.board.copy()
        
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç‚¹æ­£è¦åŒ–
        if self.player_id == "B":
            board = -board
            board = np.flipud(board)
        
        # åŸºæœ¬ç›¤é¢çŠ¶æ…‹
        state_vector = board.flatten().astype(np.float32) / 2.0
        
        # è„±å‡ºæƒ…å ±ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        escape_features = self._encode_escape_features(game_state)
        
        # ç‰¹å¾´é‡çµåˆ
        enhanced_state = np.concatenate([state_vector, escape_features])
        
        if QUANTUM_AVAILABLE:
            # é‡å­ç”¨: æ¬¡å…ƒèª¿æ•´
            if len(enhanced_state) > self.n_qubits:
                # é‡è¦ãªéƒ¨åˆ†ã®ã¿æŠ½å‡ºï¼ˆè„±å‡ºæƒ…å ±ã‚’å„ªå…ˆä¿æŒï¼‰
                escape_size = len(escape_features)
                important_indices = list(range(len(state_vector)))  # ç›¤é¢æƒ…å ±
                important_indices.extend(range(len(state_vector), len(enhanced_state)))  # è„±å‡ºæƒ…å ±
                
                if len(important_indices) > self.n_qubits:
                    # ç›¤é¢ã‹ã‚‰é‡è¦éƒ¨åˆ†ã‚’é¸æŠ
                    board_indices = np.random.choice(len(state_vector), self.n_qubits - escape_size, replace=False)
                    escape_indices = list(range(len(state_vector), len(enhanced_state)))
                    important_indices = list(board_indices) + escape_indices
                
                enhanced_state = enhanced_state[important_indices[:self.n_qubits]]
            elif len(enhanced_state) < self.n_qubits:
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                padding = np.zeros(self.n_qubits - len(enhanced_state))
                enhanced_state = np.concatenate([enhanced_state, padding])
        
        return torch.tensor(enhanced_state, dtype=torch.float32)
    
    def _encode_escape_features(self, game_state):
        """è„±å‡ºé–¢é€£ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        my_pieces = game_state.player_a_pieces if self.player_id == "A" else game_state.player_b_pieces
        escape_positions = self._get_escape_positions()
        
        escape_features = []
        
        # è„±å‡ºå¯èƒ½æ€§æƒ…å ±
        min_escape_distance = float('inf')
        escape_ready_count = 0
        
        for pos, piece_type in my_pieces.items():
            if piece_type == "good":
                # å„è„±å‡ºå£ã¸ã®è·é›¢
                distances = []
                for escape_pos in escape_positions:
                    distance = abs(pos[0] - escape_pos[0]) + abs(pos[1] - escape_pos[1])
                    distances.append(distance)
                
                min_distance = min(distances)
                min_escape_distance = min(min_escape_distance, min_distance)
                
                # è„±å‡ºå£ã«åˆ°é”ã—ã¦ã„ã‚‹å–„ç‰æ•°
                if pos in escape_positions:
                    escape_ready_count += 1
        
        # æ­£è¦åŒ–ã—ãŸè„±å‡ºç‰¹å¾´
        escape_features.extend([
            min_escape_distance / 10.0 if min_escape_distance != float('inf') else 1.0,  # æœ€çŸ­è„±å‡ºè·é›¢
            escape_ready_count / 4.0,  # è„±å‡ºæº–å‚™å®Œäº†é§’æ•°
        ])
        
        # ç›¸æ‰‹ã®è„…å¨æƒ…å ±
        opponent_pieces = game_state.player_b_pieces if self.player_id == "A" else game_state.player_a_pieces
        opponent_escape_positions = [(0, 0), (5, 0)] if self.player_id == "A" else [(0, 5), (5, 5)]
        
        opponent_min_distance = float('inf')
        for pos, piece_type in opponent_pieces.items():
            for escape_pos in opponent_escape_positions:
                distance = abs(pos[0] - escape_pos[0]) + abs(pos[1] - escape_pos[1])
                opponent_min_distance = min(opponent_min_distance, distance)
        
        escape_features.append(
            opponent_min_distance / 10.0 if opponent_min_distance != float('inf') else 1.0
        )
        
        return np.array(escape_features, dtype=np.float32)
    
    def get_q_values(self, state_tensor):
        """Qå€¤è¨ˆç®—"""
        if QUANTUM_AVAILABLE:
            quantum_output = self.quantum_circuit(state_tensor, self.params)
            return torch.stack(quantum_output)
        else:
            return self.network(state_tensor)
    
    def get_move(self, game_state, legal_moves):
        """æ‰‹é¸æŠï¼ˆBaseAIäº’æ›ï¼‰"""
        if not legal_moves:
            return None
        
        # Îµ-greedyæ¢ç´¢
        if random.random() < self.epsilon:
            return random.choice(legal_moves)
        
        # Qå€¤ãƒ™ãƒ¼ã‚¹é¸æŠ
        try:
            state_tensor = self.encode_state(game_state)
            q_values = self.get_q_values(state_tensor)
            
            # æœ€è‰¯ã®åˆæ³•æ‰‹é¸æŠ
            best_move = self._select_best_move(q_values, legal_moves, game_state)
            return best_move
        except:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ©ãƒ³ãƒ€ãƒ 
            return random.choice(legal_moves)
    
    def _select_best_move(self, q_values, legal_moves, game_state):
        """æœ€è‰¯æ‰‹é¸æŠï¼ˆè„±å‡ºæˆ¦ç•¥å¼·åŒ–ï¼‰"""
        move_scores = []
        
        for move in legal_moves:
            from_pos, to_pos = move
            
            # æ–¹å‘åˆ¤å®š
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            if dy == -1:    direction = 0  # ä¸Š
            elif dx == 1:   direction = 1  # å³
            elif dy == 1:   direction = 2  # ä¸‹
            elif dx == -1:  direction = 3  # å·¦
            else:           direction = 0
            
            # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Bè¦–ç‚¹èª¿æ•´
            if self.player_id == "B":
                direction = (direction + 2) % 4
            
            # Qå€¤ã‚¹ã‚³ã‚¢ + æˆ¦è¡“ãƒœãƒ¼ãƒŠã‚¹
            score = q_values[direction % len(q_values)].item()
            score += self._tactical_bonus(move, game_state)
            
            move_scores.append((move, score))
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢é¸æŠ
        return max(move_scores, key=lambda x: x[1])[0]
    
    def _tactical_bonus(self, move, game_state):
        """æˆ¦è¡“ãƒœãƒ¼ãƒŠã‚¹"""
        from_pos, to_pos = move
        bonus = 0.0
        
        my_pieces = game_state.player_a_pieces if self.player_id == "A" else game_state.player_b_pieces
        piece_type = my_pieces.get(from_pos, "unknown")
        
        # è„±å‡ºãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€é‡è¦ï¼‰
        escape_positions = self._get_escape_positions()
        if to_pos in escape_positions and piece_type == "good":
            bonus += 5.0  # è„±å‡ºå£åˆ°é”ã®å¤§ãƒœãƒ¼ãƒŠã‚¹
        
        # è„±å‡ºå£ã¸ã®æ¥è¿‘ãƒœãƒ¼ãƒŠã‚¹
        if piece_type == "good":
            for escape_pos in escape_positions:
                old_distance = abs(from_pos[0] - escape_pos[0]) + abs(from_pos[1] - escape_pos[1])
                new_distance = abs(to_pos[0] - escape_pos[0]) + abs(to_pos[1] - escape_pos[1])
                if new_distance < old_distance:
                    bonus += (old_distance - new_distance) * 0.3
        
        # å‰é€²ãƒœãƒ¼ãƒŠã‚¹ï¼ˆç›¸æ‰‹é™£åœ°æ–¹å‘ï¼‰
        if self.player_id == "A" and to_pos[1] > from_pos[1]:
            bonus += 0.2
        elif self.player_id == "B" and to_pos[1] < from_pos[1]:
            bonus += 0.2
        
        # ä¸­å¤®ãƒœãƒ¼ãƒŠã‚¹
        center_dist = abs(to_pos[0] - 2.5)
        bonus += (2.5 - center_dist) * 0.05
        
        # é§’å–ã‚Šãƒœãƒ¼ãƒŠã‚¹
        opponent_pieces = (game_state.player_b_pieces if self.player_id == "A" 
                          else game_state.player_a_pieces)
        if to_pos in opponent_pieces:
            bonus += 1.0
        
        return bonus
    
    def train_step(self, experiences):
        """å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—"""
        if len(experiences) < 5:
            return 0.0
        
        try:
            # ãƒãƒƒãƒä½œæˆï¼ˆGameStateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’é©åˆ‡ã«å‡¦ç†ï¼‰
            states = []
            rewards = []
            
            for exp in experiences[-5:]:
                state_data, reward = exp
                # GameStateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                if hasattr(state_data, 'board'):
                    state_tensor = self.encode_state(state_data)
                else:
                    state_tensor = state_data
                
                states.append(state_tensor)
                rewards.append(reward)
            
            states = torch.stack(states)
            rewards = torch.tensor(rewards, dtype=torch.float32)
            
            # æå¤±è¨ˆç®—
            if QUANTUM_AVAILABLE:
                predictions = []
                for state in states:
                    pred = self.quantum_circuit(state, self.params)
                    predictions.append(torch.stack(pred).mean())
                predictions = torch.stack(predictions)
            else:
                predictions = self.network(states).mean(dim=1)
            
            loss = torch.nn.MSELoss()(predictions, rewards)
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            return loss.item()
        except Exception as e:
            print(f"âš ï¸  å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def remember(self, state, reward):
        """çµŒé¨“è¨˜æ†¶ï¼ˆGameStateå¯¾å¿œï¼‰"""
        # GameStateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’äº‹å‰ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ä¿å­˜
        if hasattr(state, 'board'):
            state_tensor = self.encode_state(state)
        else:
            state_tensor = state
        
        self.memory.append((state_tensor, reward))
        
        if len(self.memory) > self.max_memory:
            self.memory.pop(0)
    
    def record_result(self, won):
        """çµæœè¨˜éŒ²ï¼ˆBaseAIäº’æ›ï¼‰"""
        super().record_result(won)
        
        # Îµæ¸›è¡°
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

def calculate_enhanced_reward(old_state, move, game):
    """å¼·åŒ–å ±é…¬ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ­£ã—ã„è„±å‡ºãƒ«ãƒ¼ãƒ«å¯¾å¿œï¼‰"""
    from_pos, to_pos = move
    current_player = old_state.current_player
    reward = 0.0
    
    # åŸºæœ¬ç§»å‹•å ±é…¬
    reward += 0.1
    
    # è„±å‡ºé–¢é€£å ±é…¬ï¼ˆæœ€é‡è¦ï¼‰
    my_pieces = old_state.player_a_pieces if current_player == "A" else old_state.player_b_pieces
    piece_type = my_pieces.get(from_pos, "unknown")
    
    if piece_type == "good":
        # è„±å‡ºå£å®šç¾©
        escape_positions = [(0, 5), (5, 5)] if current_player == "A" else [(0, 0), (5, 0)]
        
        # è„±å‡ºå£åˆ°é”ãƒœãƒ¼ãƒŠã‚¹
        if to_pos in escape_positions:
            reward += 10.0  # è„±å‡ºå£åˆ°é”ã®å¤§ãƒœãƒ¼ãƒŠã‚¹
        
        # è„±å‡ºå£ã¸ã®æ¥è¿‘ãƒœãƒ¼ãƒŠã‚¹
        for escape_pos in escape_positions:
            old_distance = abs(from_pos[0] - escape_pos[0]) + abs(from_pos[1] - escape_pos[1])
            new_distance = abs(to_pos[0] - escape_pos[0]) + abs(to_pos[1] - escape_pos[1])
            if new_distance < old_distance:
                reward += (old_distance - new_distance) * 0.5
    
    # å‰é€²å ±é…¬
    if current_player == "A" and to_pos[1] > from_pos[1]:
        reward += 0.3
    elif current_player == "B" and to_pos[1] < from_pos[1]:
        reward += 0.3
    
    # ä¸­å¤®åˆ¶å¾¡å ±é…¬
    center_dist = abs(to_pos[0] - 2.5)
    reward += (2.5 - center_dist) * 0.1
    
    # é§’å–ã‚Šå ±é…¬
    opponent_pieces = old_state.player_b_pieces if current_player == "A" else old_state.player_a_pieces
    if to_pos in opponent_pieces:
        reward += 3.0
    
    # ã‚²ãƒ¼ãƒ çµ‚äº†å ±é…¬
    if game.game_over:
        if game.winner == current_player:
            reward += 20.0  # å‹åˆ©ãƒœãƒ¼ãƒŠã‚¹
        elif game.winner is None:
            reward -= 1.0   # å¼•ãåˆ†ã‘ãƒšãƒŠãƒ«ãƒ†ã‚£
        else:
            reward -= 5.0   # æ•—åŒ—ãƒšãƒŠãƒ«ãƒ†ã‚£
    
    return reward

def train_quantum_ai(quantum_ai, opponent, episodes=500):
    """é‡å­AIå­¦ç¿’ï¼ˆæ­£ã—ã„è„±å‡ºãƒ«ãƒ¼ãƒ«å¯¾å¿œï¼‰"""
    print(f"\nğŸ“ é‡å­AIå­¦ç¿’é–‹å§‹")
    print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {episodes}")
    print(f"   å¯¾æˆ¦ç›¸æ‰‹: {opponent.name}")
    print(f"   é‡å­AIè„±å‡ºå£: {quantum_ai._get_escape_positions()}")
    print("=" * 50)
    
    stats = {
        'episode_rewards': [],
        'win_rates': [],
        'losses': [],
        'escape_victories': 0
    }
    
    for episode in range(episodes):
        game = GeisterGame()
        episode_reward = 0
        experiences = []
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼é…ç½®
        if random.random() < 0.5:
            player_a, player_b = quantum_ai, opponent
            quantum_player_id = "A"
        else:
            player_a, player_b = opponent, quantum_ai
            quantum_player_id = "B"
        
        # ã‚²ãƒ¼ãƒ å®Ÿè¡Œ
        while not game.game_over:
            current_player = player_a if game.current_player == "A" else player_b
            
            game_state = game.get_game_state(game.current_player)
            legal_moves = game.get_legal_moves(game.current_player)
            
            if not legal_moves:
                break
            
            move = current_player.get_move(game_state, legal_moves)
            if not move:
                break
            
            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆé‡å­AIã®å ´åˆï¼‰
            if current_player == quantum_ai:
                old_state = game_state
            
            success = game.make_move(move[0], move[1])
            if not success:
                break
            
            # å ±é…¬è¨ˆç®—ï¼ˆé‡å­AIã®å ´åˆï¼‰
            if current_player == quantum_ai:
                reward = calculate_enhanced_reward(old_state, move, game)
                episode_reward += reward
                # GameStateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’äº‹å‰ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ä¿å­˜
                encoded_state = quantum_ai.encode_state(old_state)
                experiences.append((encoded_state, reward))
        
        # æœ€çµ‚å ±é…¬ï¼ˆã‚²ãƒ¼ãƒ çµ‚äº†æ™‚ï¼‰
        final_reward = 0
        if game.winner == quantum_player_id:
            final_reward = 25.0  # å‹åˆ©ãƒœãƒ¼ãƒŠã‚¹
            # è„±å‡ºå‹åˆ©ã®æ¤œå‡º
            quantum_pieces = game.player_a_pieces if quantum_player_id == "A" else game.player_b_pieces
            escape_positions = quantum_ai._get_escape_positions()
            
            # è„±å‡ºå‹åˆ©ã‹ãƒã‚§ãƒƒã‚¯
            escape_win = False
            for pos, piece_type in quantum_pieces.items():
                if piece_type == "good" and pos in escape_positions:
                    escape_win = True
                    break
            
            if escape_win:
                stats['escape_victories'] += 1
                final_reward += 10.0  # è„±å‡ºå‹åˆ©ãƒœãƒ¼ãƒŠã‚¹
                
        elif game.winner is None:
            final_reward = -1.0   # å¼•ãåˆ†ã‘ãƒšãƒŠãƒ«ãƒ†ã‚£
        else:
            final_reward = -8.0   # æ•—åŒ—ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        episode_reward += final_reward
        stats['episode_rewards'].append(episode_reward)
        
        # å­¦ç¿’å®Ÿè¡Œ
        if experiences:
            loss = quantum_ai.train_step(experiences)
            stats['losses'].append(loss)
            
            # æ—¢ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ã®çµŒé¨“ã‚’ä¿å­˜
            for encoded_state, reward in experiences:
                quantum_ai.memory.append((encoded_state, reward))
                
            # ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºç®¡ç†
            while len(quantum_ai.memory) > quantum_ai.max_memory:
                quantum_ai.memory.pop(0)
        
        # çµæœè¨˜éŒ²
        quantum_ai.record_result(game.winner == quantum_player_id)
        
        # é€²æ—è¡¨ç¤º
        if (episode + 1) % 100 == 0:
            win_rate = quantum_ai.win_rate
            avg_reward = np.mean(stats['episode_rewards'][-100:])
            escape_rate = stats['escape_victories'] / (episode + 1)
            stats['win_rates'].append(win_rate)
            
            print(f"Episode {episode+1:3d}/{episodes} | "
                  f"Win Rate: {win_rate:.3f} | "
                  f"Escape Rate: {escape_rate:.3f} | "
                  f"Avg Reward: {avg_reward:6.2f} | "
                  f"Îµ: {quantum_ai.epsilon:.3f}")
    
    print("\nğŸ‰ å­¦ç¿’å®Œäº†!")
    print(f"   æœ€çµ‚å‹ç‡: {quantum_ai.win_rate:.3f}")
    print(f"   è„±å‡ºå‹åˆ©ç‡: {stats['escape_victories'] / episodes:.3f}")
    print(f"   ã‚²ãƒ¼ãƒ æ•°: {quantum_ai.games_played}")
    
    return stats

def evaluate_performance(quantum_ai, opponents, games_per_opponent=50):
    """æ€§èƒ½è©•ä¾¡"""
    print(f"\nğŸ” é‡å­AIæ€§èƒ½è©•ä¾¡")
    print("=" * 50)
    
    results = {}
    original_epsilon = quantum_ai.epsilon
    quantum_ai.epsilon = 0.0  # è©•ä¾¡æ™‚ã¯æ¢ç´¢ãªã—
    
    for opponent in opponents:
        wins = 0
        escape_wins = 0
        
        for game_num in range(games_per_opponent):
            game = GeisterGame()
            
            # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼é…ç½®ã‚’äº¤äº’å¤‰æ›´
            if game_num % 2 == 0:
                player_a, player_b = quantum_ai, opponent
                quantum_player_id = "A"
            else:
                player_a, player_b = opponent, quantum_ai
                quantum_player_id = "B"
            
            while not game.game_over:
                current_player = player_a if game.current_player == "A" else player_b
                
                game_state = game.get_game_state(game.current_player)
                legal_moves = game.get_legal_moves(game.current_player)
                
                if not legal_moves:
                    break
                
                move = current_player.get_move(game_state, legal_moves)
                if not move:
                    break
                
                if not game.make_move(move[0], move[1]):
                    break
            
            if game.winner == quantum_player_id:
                wins += 1
                
                # è„±å‡ºå‹åˆ©ã‹ãƒã‚§ãƒƒã‚¯
                quantum_pieces = game.player_a_pieces if quantum_player_id == "A" else game.player_b_pieces
                escape_positions = quantum_ai._get_escape_positions()
                
                for pos, piece_type in quantum_pieces.items():
                    if piece_type == "good" and pos in escape_positions:
                        escape_wins += 1
                        break
        
        win_rate = wins / games_per_opponent
        escape_rate = escape_wins / games_per_opponent
        results[opponent.name] = {
            'win_rate': win_rate,
            'escape_rate': escape_rate
        }
        
        print(f"   vs {opponent.name:12s}: {win_rate:.1%} (è„±å‡º{escape_rate:.1%}) ({wins}/{games_per_opponent})")
    
    quantum_ai.epsilon = original_epsilon
    return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ é‡å­ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AI ãƒ‡ãƒ¢ - æ­£ã—ã„è„±å‡ºãƒ«ãƒ¼ãƒ«å¯¾å¿œç‰ˆ")
    print("=" * 60)
    
    # 1. é‡å­AIä½œæˆ
    print("âš›ï¸  é‡å­AIåˆæœŸåŒ–...")
    quantum_ai = QuantumAI(player_id="A", n_qubits=8, n_layers=4)
    
    # 2. å¯¾æˆ¦ç›¸æ‰‹ä½œæˆ
    random_opponent = RandomAI("B")
    simple_opponent = SimpleAI("B")
    aggressive_opponent = AggressiveAI("B")
    
    print("\nğŸ¯ å¯¾æˆ¦ç›¸æ‰‹æº–å‚™å®Œäº†:")
    for opponent in [random_opponent, simple_opponent, aggressive_opponent]:
        print(f"   - {opponent.name}")
    
    # 3. å­¦ç¿’å‰æ€§èƒ½ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“Š å­¦ç¿’å‰æ€§èƒ½æ¸¬å®š...")
    pre_results = evaluate_performance(quantum_ai, [random_opponent], games_per_opponent=30)
    
    # 4. å­¦ç¿’å®Ÿè¡Œ
    print("\nğŸ“ é‡å­AIå­¦ç¿’å®Ÿè¡Œ...")
    start_time = time.time()
    training_stats = train_quantum_ai(quantum_ai, random_opponent, episodes=1000)
    training_time = time.time() - start_time
    
    # 5. å­¦ç¿’å¾Œæ€§èƒ½ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“Š å­¦ç¿’å¾Œæ€§èƒ½æ¸¬å®š...")
    post_results = evaluate_performance(quantum_ai, [random_opponent, simple_opponent, aggressive_opponent])
    
    # 6. å­¦ç¿’åŠ¹æœåˆ†æ
    print("\nğŸ“ˆ å­¦ç¿’åŠ¹æœåˆ†æ:")
    print("-" * 30)
    pre_random = pre_results['RandomAI']['win_rate']
    post_random = post_results['RandomAI']['win_rate']
    post_escape = post_results['RandomAI']['escape_rate']
    improvement = post_random - pre_random
    
    print(f"vs Random AI:")
    print(f"   å­¦ç¿’å‰: {pre_random:.1%}")
    print(f"   å­¦ç¿’å¾Œ: {post_random:.1%} (è„±å‡º{post_escape:.1%})")
    print(f"   æ”¹å–„å¹…: {improvement:+.1%}")
    
    if post_random >= 0.6:
        print("   ğŸ‰ ç›®æ¨™é”æˆï¼60%ä»¥ä¸Šã®å‹ç‡ã‚’å®Ÿç¾ï¼")
    elif improvement > 0.1:
        print("   âœ… é¡•è‘—ãªå­¦ç¿’åŠ¹æœã‚’ç¢ºèªï¼")
    elif improvement > 0.05:
        print("   âœ… è‰¯å¥½ãªå­¦ç¿’åŠ¹æœã‚’ç¢ºèª")
    else:
        print("   ğŸ“ˆ ä¸€å®šã®å­¦ç¿’åŠ¹æœã‚’ç¢ºèª")
    
    # 7. å®Ÿé¨“ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ¯ å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼:")
    print("=" * 50)
    print(f"âš›ï¸  é‡å­å›è·¯: {quantum_ai.n_qubits}qubits, {quantum_ai.n_layers}layers")
    print(f"ğŸ“ å­¦ç¿’æ™‚é–“: {training_time:.1f}ç§’")
    print(f"ğŸ® å­¦ç¿’ã‚²ãƒ¼ãƒ : {quantum_ai.games_played}")
    print(f"ğŸ“ˆ æœ€çµ‚å‹ç‡: {quantum_ai.win_rate:.1%}")
    print(f"ğŸšª è„±å‡ºå‹åˆ©: {training_stats['escape_victories']}/{len(training_stats['episode_rewards'])}")
    
    if post_random >= 0.6:
        print("\nğŸ‰ é‡å­æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Šã‚’å®Ÿè¨¼ï¼")
        print("   æ­£ã—ã„è„±å‡ºæˆ¦ç•¥ã‚’å­¦ç¿’ã§ãã¾ã—ãŸï¼")
    elif post_random > 0.5:
        print("\nâœ… é‡å­AIãŒãƒ©ãƒ³ãƒ€ãƒ ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆï¼")
    else:
        print("\nğŸ“š æ›´ãªã‚‹å­¦ç¿’ã«ã‚ˆã‚‹æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã¾ã™")
    
    print("\nğŸš€ å®Ÿé¨“å®Œäº†ï¼")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸  å®Ÿé¨“ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()