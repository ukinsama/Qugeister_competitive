#!/usr/bin/env python3
"""
CQCNNç«¶æŠ€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - å®Œå…¨ç‰ˆãƒ©ãƒ³ãƒŠãƒ¼
ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ±åˆã—ãŸå®Ÿè¡Œå¯èƒ½ãªç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ 
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import random
import time
import json
from abc import ABC, abstractmethod


# ================================================================================
# åŸºæœ¬ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©
# ================================================================================

class InitialPlacementStrategy(ABC):
    """åˆæœŸé…ç½®æˆ¦ç•¥ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’è¿”ã™"""
        pass
    
    @abstractmethod
    def get_strategy_name(self) -> str:
        """æˆ¦ç•¥åã‚’è¿”ã™"""
        pass


class PieceEstimator(ABC):
    """æ•µé§’æ¨å®šå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def estimate(self, board: np.ndarray, enemy_positions: List[Tuple[int, int]], 
                player_id: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        """æ•µé§’ã®ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
        pass
    
    @abstractmethod
    def get_estimator_name(self) -> str:
        """æ¨å®šå™¨åã‚’è¿”ã™"""
        pass


class QMapGenerator(ABC):
    """Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def generate(self, board: np.ndarray, piece_estimations: Dict,
                my_pieces: Dict, player_id: str) -> np.ndarray:
        """Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
        pass
    
    @abstractmethod
    def get_generator_name(self) -> str:
        """ç”Ÿæˆå™¨åã‚’è¿”ã™"""
        pass


class ActionSelector(ABC):
    """è¡Œå‹•é¸æŠå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def select_action(self, q_map: np.ndarray, legal_moves: List) -> Tuple:
        """Qå€¤ãƒãƒƒãƒ—ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ"""
        pass
    
    @abstractmethod
    def get_selector_name(self) -> str:
        """é¸æŠå™¨åã‚’è¿”ã™"""
        pass


# ================================================================================
# åˆæœŸé…ç½®æˆ¦ç•¥ã®å®Ÿè£…
# ================================================================================

class StandardPlacement(InitialPlacementStrategy):
    """æ¨™æº–çš„ãªåˆæœŸé…ç½®"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        if player_id == "A":
            return {
                (0, 0): "P", (1, 0): "P", (2, 0): "P", (3, 0): "P", (4, 0): "P",
                (0, 1): "B", (1, 1): "N", (2, 1): "K", (3, 1): "R", (4, 1): "Q"
            }
        else:
            return {
                (0, 5): "P", (1, 5): "P", (2, 5): "P", (3, 5): "P", (4, 5): "P",
                (0, 4): "Q", (1, 4): "R", (2, 4): "K", (3, 4): "N", (4, 4): "B"
            }
    
    def get_strategy_name(self) -> str:
        return "æ¨™æº–é…ç½®"


class DefensivePlacement(InitialPlacementStrategy):
    """å®ˆå‚™çš„ãªåˆæœŸé…ç½®"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        if player_id == "A":
            return {
                (0, 0): "P", (1, 0): "B", (2, 0): "P", (3, 0): "B", (4, 0): "P",
                (0, 1): "R", (1, 1): "P", (2, 1): "K", (3, 1): "P", (4, 1): "R"
            }
        else:
            return {
                (0, 5): "P", (1, 5): "B", (2, 5): "P", (3, 5): "B", (4, 5): "P",
                (0, 4): "R", (1, 4): "P", (2, 4): "K", (3, 4): "P", (4, 4): "R"
            }
    
    def get_strategy_name(self) -> str:
        return "å®ˆå‚™çš„é…ç½®"


class RandomPlacement(InitialPlacementStrategy):
    """ãƒ©ãƒ³ãƒ€ãƒ ãªåˆæœŸé…ç½®"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        pieces = ["P"] * 5 + ["K", "Q", "R", "B", "N"]
        random.shuffle(pieces)
        
        if player_id == "A":
            positions = [(x, y) for y in range(2) for x in range(5)]
        else:
            positions = [(x, y) for y in range(4, 6) for x in range(5)]
        
        return dict(zip(positions, pieces))
    
    def get_strategy_name(self) -> str:
        return "ãƒ©ãƒ³ãƒ€ãƒ é…ç½®"


# ================================================================================
# é‡å­å›è·¯å±¤ï¼ˆCQCNNæ ¸å¿ƒéƒ¨åˆ†ï¼‰
# ================================================================================

class QuantumCircuitLayer(nn.Module):
    """é‡å­å›è·¯å±¤ - CQCNNã®æ ¸å¿ƒ"""
    
    def __init__(self, n_qubits: int, n_layers: int):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        
        # é‡å­å›è·¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.rotation_params = nn.Parameter(
            torch.randn(n_layers, n_qubits, 3) * 0.1
        )
        self.entanglement_params = nn.Parameter(
            torch.randn(n_layers, n_qubits - 1) * 0.1
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.shape[0]
        
        # å…¥åŠ›ã‚’é‡å­ãƒ“ãƒƒãƒˆæ•°ã«èª¿æ•´
        if x.shape[1] != self.n_qubits:
            fc = nn.Linear(x.shape[1], self.n_qubits).to(x.device)
            x = fc(x)
        
        # é‡å­çŠ¶æ…‹ã‚’åˆæœŸåŒ–ï¼ˆ|0>çŠ¶æ…‹ï¼‰
        quantum_state = torch.zeros(batch_size, self.n_qubits, 2)
        quantum_state[:, :, 0] = 1.0
        
        # å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        for i in range(self.n_qubits):
            angle = x[:, i].unsqueeze(1) * np.pi
            quantum_state[:, i, 0] = torch.cos(angle / 2).squeeze()
            quantum_state[:, i, 1] = torch.sin(angle / 2).squeeze()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–é‡å­å›è·¯
        for layer in range(self.n_layers):
            # å˜ä¸€é‡å­ãƒ“ãƒƒãƒˆå›è»¢
            for i in range(self.n_qubits):
                rx = self.rotation_params[layer, i, 0]
                ry = self.rotation_params[layer, i, 1]
                rz = self.rotation_params[layer, i, 2]
                
                # RZå›è»¢
                phase = torch.exp(1j * rz / 2)
                quantum_state[:, i, 0] *= phase
                quantum_state[:, i, 1] *= torch.conj(phase)
                
                # RYå›è»¢
                c = torch.cos(ry / 2)
                s = torch.sin(ry / 2)
                temp0 = c * quantum_state[:, i, 0] - s * quantum_state[:, i, 1]
                temp1 = s * quantum_state[:, i, 0] + c * quantum_state[:, i, 1]
                quantum_state[:, i, 0] = temp0
                quantum_state[:, i, 1] = temp1
                
                # RXå›è»¢
                c = torch.cos(rx / 2)
                s = torch.sin(rx / 2) * 1j
                temp0 = c * quantum_state[:, i, 0] - s * quantum_state[:, i, 1]
                temp1 = -s * quantum_state[:, i, 0] + c * quantum_state[:, i, 1]
                quantum_state[:, i, 0] = temp0
                quantum_state[:, i, 1] = temp1
            
            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆï¼ˆCNOTé¢¨ã®æ“ä½œï¼‰
            for i in range(self.n_qubits - 1):
                strength = torch.sigmoid(self.entanglement_params[layer, i])
                control = quantum_state[:, i, 1].abs()
                quantum_state[:, i+1, :] = (1 - strength) * quantum_state[:, i+1, :] + \
                                          strength * control.unsqueeze(1) * quantum_state[:, i+1, :].roll(1, dims=1)
        
        # æ¸¬å®šï¼ˆæœŸå¾…å€¤ï¼‰
        measurements = (quantum_state.abs() ** 2)[:, :, 1]
        
        return measurements


# ================================================================================
# æ•µé§’æ¨å®šå™¨ã®å®Ÿè£…
# ================================================================================

class SimpleCQCNNEstimator(PieceEstimator):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªCQCNNæ¨å®šå™¨"""
    
    def __init__(self, n_qubits: int = 4, n_layers: int = 2):
        self.quantum_layer = QuantumCircuitLayer(n_qubits, n_layers)
        self.conv = nn.Conv2d(1, 8, kernel_size=3, padding=1)
        self.fc = nn.Linear(8 * 25, 6)  # 6ç¨®é¡ã®é§’
    
    def estimate(self, board: np.ndarray, enemy_positions: List[Tuple[int, int]], 
                player_id: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        estimations = {}
        
        for pos in enemy_positions:
            # å±€æ‰€çš„ãªãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’æŠ½å‡º
            local_board = self._extract_local_board(board, pos)
            
            # CNNç‰¹å¾´æŠ½å‡º
            x = torch.tensor(local_board, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
            x = F.relu(self.conv(x))
            x = x.flatten(start_dim=1)
            
            # é‡å­å›è·¯å‡¦ç†
            quantum_features = self.quantum_layer(x[:, :4])
            
            # æœ€çµ‚äºˆæ¸¬
            combined = torch.cat([x[:, :8], quantum_features], dim=1)
            fc_layer = nn.Linear(combined.shape[1], 6)
            predictions = F.softmax(fc_layer(combined), dim=1)
            
            piece_types = ["P", "K", "Q", "R", "B", "N"]
            estimations[pos] = {
                piece: float(predictions[0, i])
                for i, piece in enumerate(piece_types)
            }
        
        return estimations
    
    def _extract_local_board(self, board: np.ndarray, pos: Tuple[int, int]) -> np.ndarray:
        """ä½ç½®å‘¨è¾ºã®ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’æŠ½å‡º"""
        x, y = pos
        local = np.zeros((5, 5))
        
        for dx in range(-2, 3):
            for dy in range(-2, 3):
                nx, ny = x + dx, y + dy
                if 0 <= nx < 5 and 0 <= ny < 6:
                    local[dx+2, dy+2] = board[ny, nx]
        
        return local
    
    def get_estimator_name(self) -> str:
        return "ã‚·ãƒ³ãƒ—ãƒ«CQCNNæ¨å®šå™¨"


class AdvancedCQCNNEstimator(PieceEstimator):
    """é«˜åº¦ãªCQCNNæ¨å®šå™¨"""
    
    def __init__(self, n_qubits: int = 6, n_layers: int = 3):
        self.quantum_layer = QuantumCircuitLayer(n_qubits, n_layers)
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.fc = nn.Linear(32 * 25 + n_qubits, 6)
    
    def estimate(self, board: np.ndarray, enemy_positions: List[Tuple[int, int]], 
                player_id: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        estimations = {}
        
        for pos in enemy_positions:
            local_board = self._extract_local_board(board, pos)
            
            # æ·±ã„CNNç‰¹å¾´æŠ½å‡º
            x = torch.tensor(local_board, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
            x = x.flatten(start_dim=1)
            
            # é‡å­å›è·¯å‡¦ç†
            quantum_features = self.quantum_layer(x[:, :6])
            
            # çµ„ã¿åˆã‚ã›ã¦äºˆæ¸¬
            combined = torch.cat([x[:, :32], quantum_features], dim=1)
            fc_layer = nn.Linear(combined.shape[1], 6)
            predictions = F.softmax(fc_layer(combined), dim=1)
            
            piece_types = ["P", "K", "Q", "R", "B", "N"]
            estimations[pos] = {
                piece: float(predictions[0, i])
                for i, piece in enumerate(piece_types)
            }
        
        return estimations
    
    def _extract_local_board(self, board: np.ndarray, pos: Tuple[int, int]) -> np.ndarray:
        x, y = pos
        local = np.zeros((5, 5))
        
        for dx in range(-2, 3):
            for dy in range(-2, 3):
                nx, ny = x + dx, y + dy
                if 0 <= nx < 5 and 0 <= ny < 6:
                    local[dx+2, dy+2] = board[ny, nx]
        
        return local
    
    def get_estimator_name(self) -> str:
        return "é«˜åº¦CQCNNæ¨å®šå™¨"


class RandomEstimator(PieceEstimator):
    """ãƒ©ãƒ³ãƒ€ãƒ æ¨å®šå™¨ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰"""
    
    def estimate(self, board: np.ndarray, enemy_positions: List[Tuple[int, int]], 
                player_id: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        estimations = {}
        piece_types = ["P", "K", "Q", "R", "B", "N"]
        
        for pos in enemy_positions:
            probs = np.random.dirichlet([1] * 6)
            estimations[pos] = {
                piece: float(probs[i])
                for i, piece in enumerate(piece_types)
            }
        
        return estimations
    
    def get_estimator_name(self) -> str:
        return "ãƒ©ãƒ³ãƒ€ãƒ æ¨å®šå™¨"


# ================================================================================
# Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ã®å®Ÿè£…
# ================================================================================

class SimpleQMapGenerator(QMapGenerator):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªQå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def generate(self, board: np.ndarray, piece_estimations: Dict,
                my_pieces: Dict, player_id: str) -> np.ndarray:
        q_map = np.zeros((6, 5, 8))  # 6x5ãƒœãƒ¼ãƒ‰ã€8æ–¹å‘
        
        # å„è‡ªåˆ†ã®é§’ã«ã¤ã„ã¦
        for pos, piece_type in my_pieces.items():
            x, y = pos
            
            # å„æ–¹å‘ã®Qå€¤ã‚’è¨ˆç®—
            for dir_idx, (dx, dy) in enumerate([(0,1), (1,1), (1,0), (1,-1), 
                                                 (0,-1), (-1,-1), (-1,0), (-1,1)]):
                nx, ny = x + dx, y + dy
                
                if 0 <= nx < 5 and 0 <= ny < 6:
                    # åŸºæœ¬Qå€¤
                    base_q = 0.5
                    
                    # æ•µé§’ãŒã„ã‚‹å ´åˆ
                    if (nx, ny) in piece_estimations:
                        enemy_probs = piece_estimations[(nx, ny)]
                        # é§’ã®ä¾¡å€¤ã«åŸºã¥ã„ã¦åŠ ç®—
                        piece_values = {"P": 1, "N": 3, "B": 3, "R": 5, "Q": 9, "K": 100}
                        for piece, prob in enemy_probs.items():
                            base_q += prob * piece_values.get(piece, 0) * 0.1
                    
                    # ç©ºããƒã‚¹ã®å ´åˆ
                    elif board[ny, nx] == 0:
                        base_q += 0.2
                    
                    q_map[y, x, dir_idx] = base_q
        
        return q_map
    
    def get_generator_name(self) -> str:
        return "ã‚·ãƒ³ãƒ—ãƒ«Qå€¤ç”Ÿæˆ"


class StrategicQMapGenerator(QMapGenerator):
    """æˆ¦ç•¥çš„Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def generate(self, board: np.ndarray, piece_estimations: Dict,
                my_pieces: Dict, player_id: str) -> np.ndarray:
        q_map = np.zeros((6, 5, 8))
        
        # ãƒœãƒ¼ãƒ‰ã®ä¸­å¿ƒæ€§ã‚’è©•ä¾¡
        center_x, center_y = 2, 3
        
        for pos, piece_type in my_pieces.items():
            x, y = pos
            
            for dir_idx, (dx, dy) in enumerate([(0,1), (1,1), (1,0), (1,-1), 
                                                 (0,-1), (-1,-1), (-1,0), (-1,1)]):
                nx, ny = x + dx, y + dy
                
                if 0 <= nx < 5 and 0 <= ny < 6:
                    # ä½ç½®ä¾¡å€¤
                    position_value = 1.0 - (abs(nx - center_x) + abs(ny - center_y)) * 0.1
                    
                    # é§’ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹è£œæ­£
                    piece_multiplier = {
                        "P": 0.8, "N": 1.2, "B": 1.1, 
                        "R": 1.3, "Q": 1.5, "K": 0.5
                    }.get(piece_type, 1.0)
                    
                    base_q = position_value * piece_multiplier
                    
                    # æ•µé§’è©•ä¾¡
                    if (nx, ny) in piece_estimations:
                        enemy_probs = piece_estimations[(nx, ny)]
                        threat_level = sum(prob * {"P": 1, "N": 2, "B": 2, 
                                                  "R": 3, "Q": 5, "K": 10}.get(p, 0)
                                         for p, prob in enemy_probs.items())
                        base_q += threat_level * 0.15
                    
                    q_map[y, x, dir_idx] = base_q
        
        return q_map
    
    def get_generator_name(self) -> str:
        return "æˆ¦ç•¥çš„Qå€¤ç”Ÿæˆ"


class NeuralQMapGenerator(QMapGenerator):
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®Qå€¤ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc = nn.Linear(64 * 30, 240)  # 6x5x8 = 240
    
    def generate(self, board: np.ndarray, piece_estimations: Dict,
                my_pieces: Dict, player_id: str) -> np.ndarray:
        # 3ãƒãƒ£ãƒ³ãƒãƒ«å…¥åŠ›ã‚’ä½œæˆ
        input_tensor = np.zeros((3, 6, 5))
        
        # ãƒãƒ£ãƒ³ãƒãƒ«1: ãƒœãƒ¼ãƒ‰çŠ¶æ…‹
        input_tensor[0] = board
        
        # ãƒãƒ£ãƒ³ãƒãƒ«2: è‡ªåˆ†ã®é§’
        for pos in my_pieces:
            input_tensor[1, pos[1], pos[0]] = 1
        
        # ãƒãƒ£ãƒ³ãƒãƒ«3: æ•µé§’æ¨å®š
        for pos in piece_estimations:
            input_tensor[2, pos[1], pos[0]] = max(piece_estimations[pos].values())
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡¦ç†
        x = torch.tensor(input_tensor, dtype=torch.float32).unsqueeze(0)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.flatten(start_dim=1)
        x = self.fc(x)
        
        # Qå€¤ãƒãƒƒãƒ—ã«å¤‰å½¢
        q_map = x.detach().numpy().reshape(6, 5, 8)
        
        return q_map
    
    def get_generator_name(self) -> str:
        return "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«Qå€¤ç”Ÿæˆ"


# ================================================================================
# è¡Œå‹•é¸æŠå™¨ã®å®Ÿè£…
# ================================================================================

class GreedySelector(ActionSelector):
    """è²ªæ¬²é¸æŠå™¨"""
    
    def select_action(self, q_map: np.ndarray, legal_moves: List) -> Tuple:
        if not legal_moves:
            return None
        
        best_q = -float('inf')
        best_move = legal_moves[0]
        
        for move in legal_moves:
            from_pos, to_pos, *_ = move
            
            # æ–¹å‘ã‚’è¨ˆç®—
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            # æ–¹å‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            directions = [(0,1), (1,1), (1,0), (1,-1), 
                         (0,-1), (-1,-1), (-1,0), (-1,1)]
            
            try:
                dir_idx = directions.index((dx, dy))
            except ValueError:
                continue
            
            q_value = q_map[from_pos[1], from_pos[0], dir_idx]
            
            if q_value > best_q:
                best_q = q_value
                best_move = move
        
        return best_move
    
    def get_selector_name(self) -> str:
        return "è²ªæ¬²é¸æŠ"


class EpsilonGreedySelector(ActionSelector):
    """Îµ-è²ªæ¬²é¸æŠå™¨"""
    
    def __init__(self, epsilon: float = 0.1):
        self.epsilon = epsilon
    
    def select_action(self, q_map: np.ndarray, legal_moves: List) -> Tuple:
        if not legal_moves:
            return None
        
        # Îµç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
        if random.random() < self.epsilon:
            return random.choice(legal_moves)
        
        # ãã‚Œä»¥å¤–ã¯è²ªæ¬²é¸æŠ
        greedy = GreedySelector()
        return greedy.select_action(q_map, legal_moves)
    
    def get_selector_name(self) -> str:
        return f"Îµ-è²ªæ¬²(Îµ={self.epsilon})"


class SoftmaxSelector(ActionSelector):
    """ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é¸æŠå™¨"""
    
    def __init__(self, temperature: float = 1.0):
        self.temperature = temperature
    
    def select_action(self, q_map: np.ndarray, legal_moves: List) -> Tuple:
        if not legal_moves:
            return None
        
        q_values = []
        directions = [(0,1), (1,1), (1,0), (1,-1), 
                     (0,-1), (-1,-1), (-1,0), (-1,1)]
        
        for move in legal_moves:
            from_pos, to_pos, *_ = move
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            try:
                dir_idx = directions.index((dx, dy))
            except ValueError:
                q_values.append(-100)
                continue
            
            q_values.append(q_map[from_pos[1], from_pos[0], dir_idx])
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç¢ºç‡ã‚’è¨ˆç®—
        q_tensor = torch.tensor(q_values, dtype=torch.float32) / self.temperature
        probs = F.softmax(q_tensor, dim=0).numpy()
        
        # ç¢ºç‡çš„ã«é¸æŠ
        choice_idx = np.random.choice(len(legal_moves), p=probs)
        return legal_moves[choice_idx]
    
    def get_selector_name(self) -> str:
        return f"ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹(T={self.temperature})"


# ================================================================================
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
# ================================================================================

@dataclass
class ModuleConfig:
    """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""
    placement_strategy: InitialPlacementStrategy
    piece_estimator: PieceEstimator
    qmap_generator: QMapGenerator
    action_selector: ActionSelector


# ================================================================================
# CQCNNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
# ================================================================================

class CQCNNAgent:
    """ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼å‹CQCNNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, player_id: str, config: ModuleConfig, name: str = None):
        self.player_id = player_id
        self.config = config
        self.name = name or self._generate_name()
        
        # çµ±è¨ˆæƒ…å ±
        self.games_played = 0
        self.wins = 0
        self.move_history = []
        self.last_estimations = {}
        self.last_q_map = None
    
    def _generate_name(self) -> str:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã‚’ç”Ÿæˆ"""
        return f"{self.config.placement_strategy.get_strategy_name()[:4]}+" \
               f"{self.config.piece_estimator.get_estimator_name()[:6]}+" \
               f"{self.config.qmap_generator.get_generator_name()[:4]}+" \
               f"{self.config.action_selector.get_selector_name()[:4]}"
    
    def get_initial_placement(self) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’å–å¾—"""
        return self.config.placement_strategy.get_placement(self.player_id)
    
    def get_move(self, board: np.ndarray, legal_moves: List, 
                enemy_positions: List[Tuple[int, int]], 
                my_pieces: Dict[Tuple[int, int], str]) -> Optional[Tuple]:
        """æ‰‹ã‚’é¸æŠ"""
        if not legal_moves:
            return None
        
        try:
            # 1. æ•µé§’ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š
            if enemy_positions:
                self.last_estimations = self.config.piece_estimator.estimate(
                    board, enemy_positions, self.player_id
                )
            else:
                self.last_estimations = {}
            
            # 2. Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
            self.last_q_map = self.config.qmap_generator.generate(
                board, self.last_estimations, my_pieces, self.player_id
            )
            
            # 3. è¡Œå‹•ã‚’é¸æŠ
            action = self.config.action_selector.select_action(
                self.last_q_map, legal_moves
            )
            
            # å±¥æ­´è¨˜éŒ²
            self.move_history.append({
                'action': action,
                'estimations': len(self.last_estimations),
                'q_max': np.max(self.last_q_map) if self.last_q_map is not None else 0
            })
            
            return action
            
        except Exception as e:
            print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ in {self.name}: {e}")
            return random.choice(legal_moves)
    
    def game_end(self, won: bool):
        """ã‚²ãƒ¼ãƒ çµ‚äº†å‡¦ç†"""
        self.games_played += 1
        if won:
            self.wins += 1
    
    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            'name': self.name,
            'games_played': self.games_played,
            'wins': self.wins,
            'win_rate': self.wins / max(self.games_played, 1),
            'total_moves': len(self.move_history)
        }


# ================================================================================
# ç«¶æŠ€ãƒ©ãƒ³ãƒŠãƒ¼
# ================================================================================

class CQCNNCompetitionRunner:
    """CQCNNç«¶æŠ€å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        self.modules = {
            'placement': [
                StandardPlacement(),
                DefensivePlacement(),
                RandomPlacement()
            ],
            'estimator': [
                SimpleCQCNNEstimator(n_qubits=4, n_layers=2),
                AdvancedCQCNNEstimator(n_qubits=6, n_layers=3),
                RandomEstimator()
            ],
            'qmap': [
                SimpleQMapGenerator(),
                StrategicQMapGenerator(),
                NeuralQMapGenerator()
            ],
            'selector': [
                GreedySelector(),
                EpsilonGreedySelector(epsilon=0.1),
                EpsilonGreedySelector(epsilon=0.3),
                SoftmaxSelector(temperature=1.0)
            ]
        }
        
        # å¯¾æˆ¦çµæœè¨˜éŒ²
        self.match_results = []
        self.agent_stats = {}
    
    def show_modules(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¡¨ç¤º"""
        print("=" * 70)
        print("ğŸ® CQCNNç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ  - åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«")
        print("=" * 70)
        
        print("\nã€1. åˆæœŸé…ç½®æˆ¦ç•¥ã€‘")
        for i, module in enumerate(self.modules['placement']):
            print(f"  {i}: {module.get_strategy_name()}")
        
        print("\nã€2. æ•µé§’æ¨å®šå™¨ã€‘")
        for i, module in enumerate(self.modules['estimator']):
            print(f"  {i}: {module.get_estimator_name()}")
        
        print("\nã€3. Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ã€‘")
        for i, module in enumerate(self.modules['qmap']):
            print(f"  {i}: {module.get_generator_name()}")
        
        print("\nã€4. è¡Œå‹•é¸æŠå™¨ã€‘")
        for i, module in enumerate(self.modules['selector']):
            print(f"  {i}: {module.get_selector_name()}")
    
    def create_agent(self, player_id: str, module_indices: Tuple[int, int, int, int],
                    name: str = None) -> CQCNNAgent:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ"""
        config = ModuleConfig(
            placement_strategy=self.modules['placement'][module_indices[0]],
            piece_estimator=self.modules['estimator'][module_indices[1]],
            qmap_generator=self.modules['qmap'][module_indices[2]],
            action_selector=self.modules['selector'][module_indices[3]]
        )
        
        agent = CQCNNAgent(player_id, config, name)
        
        # çµ±è¨ˆåˆæœŸåŒ–
        if agent.name not in self.agent_stats:
            self.agent_stats[agent.name] = {
                'games': 0,
                'wins': 0,
                'modules': module_indices
            }
        
        return agent
    
    def simulate_game(self, agent1: CQCNNAgent, agent2: CQCNNAgent, 
                     max_turns: int = 100) -> Dict:
        """ã‚²ãƒ¼ãƒ ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        # ç°¡æ˜“ã‚²ãƒ¼ãƒ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        board = np.zeros((6, 5))
        turn = 0
        
        # åˆæœŸé…ç½®
        placement1 = agent1.get_initial_placement()
        placement2 = agent2.get_initial_placement()
        
        # ãƒœãƒ¼ãƒ‰ã«é…ç½®
        for pos in placement1:
            board[pos[1], pos[0]] = 1
        for pos in placement2:
            board[pos[1], pos[0]] = -1
        
        # ã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        while turn < max_turns:
            turn += 1
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ1ã®æ‰‹ç•ª
            my_pieces1 = {pos: "P" for pos in placement1}  # ç°¡ç•¥åŒ–
            enemy_positions1 = list(placement2.keys())
            legal_moves1 = self._generate_legal_moves(board, my_pieces1)
            
            move1 = agent1.get_move(board, legal_moves1, enemy_positions1, my_pieces1)
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ2ã®æ‰‹ç•ª
            my_pieces2 = {pos: "P" for pos in placement2}  # ç°¡ç•¥åŒ–
            enemy_positions2 = list(placement1.keys())
            legal_moves2 = self._generate_legal_moves(board, my_pieces2)
            
            move2 = agent2.get_move(board, legal_moves2, enemy_positions2, my_pieces2)
            
            # å‹æ•—åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆ - ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
            if turn > 20:
                winner = "A" if random.random() > 0.5 else "B"
                break
        else:
            winner = "Draw"
        
        # çµ±è¨ˆæ›´æ–°
        agent1.game_end(winner == "A")
        agent2.game_end(winner == "B")
        
        if winner == "A":
            self.agent_stats[agent1.name]['wins'] += 1
        elif winner == "B":
            self.agent_stats[agent2.name]['wins'] += 1
        
        self.agent_stats[agent1.name]['games'] += 1
        self.agent_stats[agent2.name]['games'] += 1
        
        return {
            'winner': winner,
            'turns': turn,
            'agent1': agent1.name,
            'agent2': agent2.name
        }
    
    def _generate_legal_moves(self, board: np.ndarray, 
                             my_pieces: Dict) -> List[Tuple]:
        """åˆæ³•æ‰‹ã‚’ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        moves = []
        directions = [(0,1), (1,1), (1,0), (1,-1), 
                     (0,-1), (-1,-1), (-1,0), (-1,1)]
        
        for pos in my_pieces:
            for dx, dy in directions:
                new_pos = (pos[0] + dx, pos[1] + dy)
                if 0 <= new_pos[0] < 5 and 0 <= new_pos[1] < 6:
                    moves.append((pos, new_pos))
        
        return moves if moves else [(list(my_pieces.keys())[0], (2, 3))]
    
    def run_tournament(self, agents: List[CQCNNAgent], games_per_pair: int = 3):
        """ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
        print("\n" + "=" * 70)
        print("ğŸ† ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé–‹å§‹")
        print("=" * 70)
        
        total_games = 0
        
        for i, agent1 in enumerate(agents):
            for j, agent2 in enumerate(agents):
                if i >= j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹
                    continue
                
                print(f"\nã€{agent1.name} vs {agent2.name}ã€‘")
                
                for game_num in range(games_per_pair):
                    result = self.simulate_game(agent1, agent2)
                    total_games += 1
                    
                    winner_name = agent1.name if result['winner'] == "A" else agent2.name
                    if result['winner'] == "Draw":
                        winner_name = "å¼•ãåˆ†ã‘"
                    
                    print(f"  ã‚²ãƒ¼ãƒ  {game_num + 1}: {winner_name} ({result['turns']}ã‚¿ãƒ¼ãƒ³)")
        
        print(f"\nğŸ“Š ç·ã‚²ãƒ¼ãƒ æ•°: {total_games}")
    
    def show_results(self):
        """çµæœã‚’è¡¨ç¤º"""
        print("\n" + "=" * 70)
        print("ğŸ“Š æœ€çµ‚çµæœ")
        print("=" * 70)
        
        if not self.agent_stats:
            print("ã¾ã çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # å‹ç‡ã§ã‚½ãƒ¼ãƒˆ
        sorted_agents = sorted(
            self.agent_stats.items(),
            key=lambda x: x[1]['wins'] / max(x[1]['games'], 1),
            reverse=True
        )
        
        print("\nã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘")
        for rank, (name, stats) in enumerate(sorted_agents, 1):
            win_rate = stats['wins'] / max(stats['games'], 1) * 100
            modules = stats['modules']
            print(f"\n{rank}. {name}")
            print(f"   å‹ç‡: {win_rate:.1f}% ({stats['wins']}/{stats['games']})")
            print(f"   ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ:")
            print(f"     é…ç½®: {self.modules['placement'][modules[0]].get_strategy_name()}")
            print(f"     æ¨å®š: {self.modules['estimator'][modules[1]].get_estimator_name()}")
            print(f"     Qå€¤: {self.modules['qmap'][modules[2]].get_generator_name()}")
            print(f"     é¸æŠ: {self.modules['selector'][modules[3]].get_selector_name()}")


# ================================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ================================================================================

def quick_demo():
    """ã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("ğŸš€ CQCNNç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ  - ã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ¢")
    print("=" * 70)
    
    runner = CQCNNCompetitionRunner()
    
    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¡¨ç¤º
    runner.show_modules()
    
    print("\n" + "=" * 70)
    print("ğŸ“ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ")
    print("=" * 70)
    
    # 4ç¨®é¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ
    agents = [
        runner.create_agent("A", (0, 0, 0, 0), "æ¨™æº–å‹"),      # å…¨ã¦åŸºæœ¬
        runner.create_agent("B", (1, 1, 1, 1), "é«˜åº¦å‹"),      # å…¨ã¦é«˜åº¦
        runner.create_agent("A", (2, 0, 1, 2), "æ··åˆå‹"),      # æ··åˆ
        runner.create_agent("B", (0, 2, 2, 3), "å®Ÿé¨“å‹")       # å®Ÿé¨“çš„çµ„ã¿åˆã‚ã›
    ]
    
    for agent in agents:
        modules = runner.agent_stats[agent.name]['modules']
        print(f"\n{agent.name}:")
        print(f"  é…ç½®: {runner.modules['placement'][modules[0]].get_strategy_name()}")
        print(f"  æ¨å®š: {runner.modules['estimator'][modules[1]].get_estimator_name()}")
        print(f"  Qå€¤: {runner.modules['qmap'][modules[2]].get_generator_name()}")
        print(f"  é¸æŠ: {runner.modules['selector'][modules[3]].get_selector_name()}")
    
    # ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ
    runner.run_tournament(agents, games_per_pair=2)
    
    # çµæœè¡¨ç¤º
    runner.show_results()
    
    print("\nâœ… ãƒ‡ãƒ¢å®Œäº†ï¼")


def interactive_mode():
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
    runner = CQCNNCompetitionRunner()
    
    print("ğŸ® CQCNNç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ  - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    print("=" * 70)
    
    while True:
        print("\nã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€‘")
        print("1. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º")
        print("2. ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ")
        print("3. ã‚¯ã‚¤ãƒƒã‚¯å¯¾æˆ¦ï¼ˆãƒ—ãƒªã‚»ãƒƒãƒˆï¼‰")
        print("4. ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ")
        print("5. çµæœè¡¨ç¤º")
        print("0. çµ‚äº†")
        
        choice = input("\né¸æŠ (0-5): ").strip()
        
        if choice == "0":
            break
        
        elif choice == "1":
            runner.show_modules()
        
        elif choice == "2":
            print("\nğŸ“ ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ")
            print("å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            
            try:
                placement = int(input("åˆæœŸé…ç½® (0-2): "))
                estimator = int(input("æ¨å®šå™¨ (0-2): "))
                qmap = int(input("Qå€¤ç”Ÿæˆ (0-2): "))
                selector = int(input("è¡Œå‹•é¸æŠ (0-3): "))
                name = input("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå (çœç•¥å¯): ").strip() or None
                
                agent = runner.create_agent("A", (placement, estimator, qmap, selector), name)
                print(f"âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ: {agent.name}")
                
            except (ValueError, IndexError) as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        elif choice == "3":
            print("\nâš¡ ã‚¯ã‚¤ãƒƒã‚¯å¯¾æˆ¦")
            
            # ãƒ—ãƒªã‚»ãƒƒãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
            agent1 = runner.create_agent("A", (0, 0, 0, 0), "æ¨™æº–å‹")
            agent2 = runner.create_agent("B", (1, 1, 1, 1), "é«˜åº¦å‹")
            
            result = runner.simulate_game(agent1, agent2)
            
            winner_name = agent1.name if result['winner'] == "A" else agent2.name
            if result['winner'] == "Draw":
                winner_name = "å¼•ãåˆ†ã‘"
            
            print(f"\nçµæœ: {winner_name} ã®å‹åˆ©ï¼")
            print(f"ã‚¿ãƒ¼ãƒ³æ•°: {result['turns']}")
        
        elif choice == "4":
            print("\nğŸ† ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆè¨­å®š")
            
            # ãƒ—ãƒªã‚»ãƒƒãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç¾¤
            agents = [
                runner.create_agent("A", (0, 0, 0, 0), "æ¨™æº–Simple"),
                runner.create_agent("B", (1, 1, 1, 1), "å®ˆå‚™Advanced"),
                runner.create_agent("A", (2, 2, 2, 2), "ãƒ©ãƒ³ãƒ€ãƒ å‹"),
                runner.create_agent("B", (0, 1, 1, 0), "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰")
            ]
            
            games = int(input("å„ãƒšã‚¢ã®ã‚²ãƒ¼ãƒ æ•° (1-10): ") or "3")
            runner.run_tournament(agents, games)
        
        elif choice == "5":
            runner.show_results()
    
    print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("=" * 70)
    print("ğŸ® CQCNNç«¶æŠ€å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)
    print("\nå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("1. ã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ¢ï¼ˆè‡ªå‹•å®Ÿè¡Œï¼‰")
    print("2. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¯¾è©±å‹ï¼‰")
    print("3. ã‚«ã‚¹ã‚¿ãƒ å¯¾æˆ¦ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰")
    
    mode = input("\né¸æŠ (1-3): ").strip()
    
    if mode == "1":
        quick_demo()
    elif mode == "2":
        interactive_mode()
    elif mode == "3":
        print("\nğŸ“ ã‚«ã‚¹ã‚¿ãƒ å¯¾æˆ¦ãƒ¢ãƒ¼ãƒ‰")
        runner = CQCNNCompetitionRunner()
        runner.show_modules()
        
        print("\n2ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆã—ã¦å¯¾æˆ¦ã•ã›ã¾ã™")
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ1
        print("\nã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ1ã€‘")
        p1 = int(input("é…ç½® (0-2): "))
        e1 = int(input("æ¨å®š (0-2): "))
        q1 = int(input("Qå€¤ (0-2): "))
        s1 = int(input("é¸æŠ (0-3): "))
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ2  
        print("\nã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ2ã€‘")
        p2 = int(input("é…ç½® (0-2): "))
        e2 = int(input("æ¨å®š (0-2): "))
        q2 = int(input("Qå€¤ (0-2): "))
        s2 = int(input("é¸æŠ (0-3): "))
        
        agent1 = runner.create_agent("A", (p1, e1, q1, s1))
        agent2 = runner.create_agent("B", (p2, e2, q2, s2))
        
        games = int(input("\nã‚²ãƒ¼ãƒ æ•° (1-10): ") or "5")
        
        for i in range(games):
            print(f"\n--- ã‚²ãƒ¼ãƒ  {i+1}/{games} ---")
            result = runner.simulate_game(agent1, agent2)
            
            winner_name = agent1.name if result['winner'] == "A" else agent2.name
            if result['winner'] == "Draw":
                winner_name = "å¼•ãåˆ†ã‘"
            
            print(f"çµæœ: {winner_name} ({result['turns']}ã‚¿ãƒ¼ãƒ³)")
        
        runner.show_results()
    else:
        print("ç„¡åŠ¹ãªé¸æŠã§ã™")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()