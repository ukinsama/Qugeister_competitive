#!/usr/bin/env python3
"""
CQCNNå¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ  v2 - å®Œå…¨ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆç‰ˆ
5ã¤ã®ç‹¬ç«‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« + 7ãƒãƒ£ãƒ³ãƒãƒ«å…¥åŠ›å¯¾å¿œ

ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ:
1. PlacementStrategy - åˆæœŸé…ç½®æˆ¦ç•¥
2. PieceEstimator - æ•µé§’æ¨å®šå™¨
3. RewardFunction - å ±é…¬é–¢æ•°
4. QMapGenerator - Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨
5. ActionSelector - è¡Œå‹•é¸æŠå™¨
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import random
import json
import os
from datetime import datetime
from enum import Enum

# ================================================================================
# Part 1: åŸºæœ¬è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# ================================================================================

class GameConfig:
    """ã‚²ãƒ¼ãƒ è¨­å®š"""
    def __init__(self):
        self.board_size = (6, 6)
        self.max_turns = 100
        self.n_pieces = 8  # å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é§’æ•°
        self.n_good = 4   # å–„ç‰ã®æ•°
        self.n_bad = 4    # æ‚ªç‰ã®æ•°


class GameState:
    """ã‚²ãƒ¼ãƒ çŠ¶æ…‹"""
    def __init__(self):
        self.board = np.zeros((6, 6), dtype=int)
        self.player_a_pieces = {}  # {ä½ç½®: é§’ã‚¿ã‚¤ãƒ—}
        self.player_b_pieces = {}
        self.turn = 0
        self.winner = None
        
    def is_game_over(self):
        return self.winner is not None or self.turn >= 100


# ================================================================================
# Part 2: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«1 - åˆæœŸé…ç½®æˆ¦ç•¥
# ================================================================================

class PlacementStrategy(ABC):
    """åˆæœŸé…ç½®æˆ¦ç•¥ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’å–å¾—"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """æˆ¦ç•¥åã‚’å–å¾—"""
        pass


class StandardPlacement(PlacementStrategy):
    """æ¨™æº–é…ç½®æˆ¦ç•¥"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}
        
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Aã¯ä¸‹å´ï¼ˆè¡Œ0-1ï¼‰ã€Bã¯ä¸Šå´ï¼ˆè¡Œ4-5ï¼‰ã®ä¸­å¤®4åˆ—ã«é…ç½®
        if player_id == "A":
            positions = [(0, 1), (0, 2), (0, 3), (0, 4),
                        (1, 1), (1, 2), (1, 3), (1, 4)]
        else:
            positions = [(4, 1), (4, 2), (4, 3), (4, 4),
                        (5, 1), (5, 2), (5, 3), (5, 4)]
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«å–„ç‰ã¨æ‚ªç‰ã‚’é…ç½®
        piece_types = ['good'] * 4 + ['bad'] * 4
        random.shuffle(piece_types)
        
        for pos, piece_type in zip(positions, piece_types):
            placement[pos] = piece_type
        
        return placement
    
    def get_name(self) -> str:
        return "æ¨™æº–é…ç½®"


class AggressivePlacement(PlacementStrategy):
    """æ”»æ’ƒçš„é…ç½®æˆ¦ç•¥ï¼ˆå–„ç‰ã‚’å‰ç·šã«ï¼‰"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}
        
        if player_id == "A":
            # å‰åˆ—ã«å–„ç‰ã‚’å¤šãé…ç½®
            front_positions = [(1, 1), (1, 2), (1, 3), (1, 4)]
            back_positions = [(0, 1), (0, 2), (0, 3), (0, 4)]
        else:
            front_positions = [(4, 1), (4, 2), (4, 3), (4, 4)]
            back_positions = [(5, 1), (5, 2), (5, 3), (5, 4)]
        
        # å‰åˆ—ã«å–„ç‰3å€‹ã€æ‚ªç‰1å€‹
        front_pieces = ['good', 'good', 'good', 'bad']
        random.shuffle(front_pieces)
        
        # å¾Œåˆ—ã«å–„ç‰1å€‹ã€æ‚ªç‰3å€‹
        back_pieces = ['good', 'bad', 'bad', 'bad']
        random.shuffle(back_pieces)
        
        for pos, piece_type in zip(front_positions, front_pieces):
            placement[pos] = piece_type
        for pos, piece_type in zip(back_positions, back_pieces):
            placement[pos] = piece_type
        
        return placement
    
    def get_name(self) -> str:
        return "æ”»æ’ƒçš„é…ç½®"


class DefensivePlacement(PlacementStrategy):
    """é˜²å¾¡çš„é…ç½®æˆ¦ç•¥ï¼ˆå–„ç‰ã‚’å¾Œæ–¹ã«ï¼‰"""
    
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}
        
        if player_id == "A":
            front_positions = [(1, 1), (1, 2), (1, 3), (1, 4)]
            back_positions = [(0, 1), (0, 2), (0, 3), (0, 4)]
        else:
            front_positions = [(4, 1), (4, 2), (4, 3), (4, 4)]
            back_positions = [(5, 1), (5, 2), (5, 3), (5, 4)]
        
        # å‰åˆ—ã«æ‚ªç‰3å€‹ã€å–„ç‰1å€‹
        front_pieces = ['bad', 'bad', 'bad', 'good']
        random.shuffle(front_pieces)
        
        # å¾Œåˆ—ã«æ‚ªç‰1å€‹ã€å–„ç‰3å€‹
        back_pieces = ['bad', 'good', 'good', 'good']
        random.shuffle(back_pieces)
        
        for pos, piece_type in zip(front_positions, front_pieces):
            placement[pos] = piece_type
        for pos, piece_type in zip(back_positions, back_pieces):
            placement[pos] = piece_type
        
        return placement
    
    def get_name(self) -> str:
        return "é˜²å¾¡çš„é…ç½®"


# ================================================================================
# Part 3: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«2 - æ•µé§’æ¨å®šå™¨
# ================================================================================

class PieceEstimator(ABC):
    """æ•µé§’æ¨å®šå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def estimate(self, board: np.ndarray, 
                enemy_positions: List[Tuple[int, int]], 
                player: str,
                my_pieces: Dict[Tuple[int, int], str],
                turn: int) -> Dict[Tuple[int, int], Dict[str, float]]:
        """æ•µé§’ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """æ¨å®šå™¨åã‚’å–å¾—"""
        pass
    
    def prepare_tensor_7ch(self, board: np.ndarray, player: str, 
                           my_pieces: Dict, turn: int) -> torch.Tensor:
        """7ãƒãƒ£ãƒ³ãƒãƒ«å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚’æº–å‚™"""
        tensor = torch.zeros(1, 7, 6, 6)
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        # Ch0: è‡ªåˆ†ã®å–„ç‰
        for pos, piece_type in my_pieces.items():
            if piece_type == 'good' and board[pos] == player_val:
                tensor[0, 0, pos[0], pos[1]] = 1.0
        
        # Ch1: è‡ªåˆ†ã®æ‚ªç‰
        for pos, piece_type in my_pieces.items():
            if piece_type == 'bad' and board[pos] == player_val:
                tensor[0, 1, pos[0], pos[1]] = 1.0
        
        # Ch2: ç›¸æ‰‹ã®é§’ï¼ˆç¨®é¡ä¸æ˜ï¼‰
        tensor[0, 2] = torch.from_numpy((board == enemy_val).astype(np.float32))
        
        # Ch3: ç©ºããƒã‚¹
        tensor[0, 3] = torch.from_numpy((board == 0).astype(np.float32))
        
        # Ch4: è‡ªåˆ†ã®è„±å‡ºå£
        if player == "A":
            tensor[0, 4, 5, 0] = 1.0  # å·¦ä¸Š
            tensor[0, 4, 5, 5] = 1.0  # å³ä¸Š
        else:
            tensor[0, 4, 0, 0] = 1.0  # å·¦ä¸‹
            tensor[0, 4, 0, 5] = 1.0  # å³ä¸‹
        
        # Ch5: ç›¸æ‰‹ã®è„±å‡ºå£
        if player == "A":
            tensor[0, 5, 0, 0] = 1.0
            tensor[0, 5, 0, 5] = 1.0
        else:
            tensor[0, 5, 5, 0] = 1.0
            tensor[0, 5, 5, 5] = 1.0
        
        # Ch6: ã‚¿ãƒ¼ãƒ³é€²è¡Œåº¦
        tensor[0, 6, :, :] = turn / 100.0
        
        return tensor


class RandomEstimator(PieceEstimator):
    """ãƒ©ãƒ³ãƒ€ãƒ æ¨å®šå™¨"""
    
    def estimate(self, board: np.ndarray, 
                enemy_positions: List[Tuple[int, int]], 
                player: str,
                my_pieces: Dict[Tuple[int, int], str],
                turn: int) -> Dict[Tuple[int, int], Dict[str, float]]:
        
        results = {}
        for pos in enemy_positions:
            good_prob = random.random()
            results[pos] = {
                'good_prob': good_prob,
                'bad_prob': 1 - good_prob,
                'confidence': 0.5
            }
        return results
    
    def get_name(self) -> str:
        return "ãƒ©ãƒ³ãƒ€ãƒ æ¨å®š"


class SimpleCNNEstimator(PieceEstimator):
    """ã‚·ãƒ³ãƒ—ãƒ«CNNæ¨å®šå™¨"""
    
    def __init__(self):
        self.model = self._build_model()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
    
    def _build_model(self):
        """7ãƒãƒ£ãƒ³ãƒãƒ«å…¥åŠ›å¯¾å¿œã®CNNãƒ¢ãƒ‡ãƒ«"""
        return nn.Sequential(
            nn.Conv2d(7, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32 * 6 * 6, 64),
            nn.ReLU(),
            nn.Linear(64, 2)  # å–„ç‰/æ‚ªç‰ã®2ã‚¯ãƒ©ã‚¹
        )
    
    def estimate(self, board: np.ndarray, 
                enemy_positions: List[Tuple[int, int]], 
                player: str,
                my_pieces: Dict[Tuple[int, int], str],
                turn: int) -> Dict[Tuple[int, int], Dict[str, float]]:
        
        self.model.eval()
        results = {}
        
        # 7ãƒãƒ£ãƒ³ãƒãƒ«ãƒ†ãƒ³ã‚½ãƒ«ã‚’æº–å‚™
        tensor = self.prepare_tensor_7ch(board, player, my_pieces, turn)
        
        with torch.no_grad():
            output = self.model(tensor)
            probs = F.softmax(output, dim=1)
            
            # å…¨ã¦ã®æ•µé§’ã«åŒã˜æ¨å®šã‚’é©ç”¨ï¼ˆç°¡ç•¥åŒ–ï¼‰
            for pos in enemy_positions:
                results[pos] = {
                    'good_prob': probs[0, 0].item(),
                    'bad_prob': probs[0, 1].item(),
                    'confidence': max(probs[0].tolist())
                }
        
        return results
    
    def get_name(self) -> str:
        return "SimpleCNN"


class CQCNNEstimator(PieceEstimator):
    """CQCNNï¼ˆé‡å­å›è·¯ä»˜ãï¼‰æ¨å®šå™¨"""
    
    def __init__(self, n_qubits: int = 6, n_layers: int = 3):
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.model = self._build_model()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
    
    def _build_model(self):
        """é‡å­å›è·¯å±¤ã‚’å«ã‚€CQCNNãƒ¢ãƒ‡ãƒ«"""
        class QuantumLayer(nn.Module):
            def __init__(self, n_qubits, n_layers):
                super().__init__()
                self.n_qubits = n_qubits
                self.n_layers = n_layers
                self.rotation_params = nn.Parameter(
                    torch.randn(n_layers, n_qubits, 3) * 0.1
                )
                self.entangle_params = nn.Parameter(
                    torch.randn(n_layers, n_qubits - 1) * 0.1
                )
            
            def forward(self, x):
                batch_size = x.shape[0]
                
                # ç°¡ç•¥åŒ–ã•ã‚ŒãŸé‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                state_real = torch.ones(batch_size, self.n_qubits)
                state_imag = torch.zeros(batch_size, self.n_qubits)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                for i in range(min(self.n_qubits, x.shape[1])):
                    angle = x[:, i] * np.pi
                    state_real[:, i] = torch.cos(angle/2)
                    state_imag[:, i] = torch.sin(angle/2)
                
                # å¤‰åˆ†å›è·¯
                for layer in range(self.n_layers):
                    # å›è»¢ã‚²ãƒ¼ãƒˆ
                    for q in range(self.n_qubits):
                        rx = self.rotation_params[layer, q, 0]
                        state_real[:, q] = state_real[:, q] * torch.cos(rx/2)
                        state_imag[:, q] = state_imag[:, q] * torch.sin(rx/2)
                    
                    # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆï¼ˆç°¡ç•¥åŒ–ï¼‰
                    for i in range(self.n_qubits - 1):
                        strength = torch.sigmoid(self.entangle_params[layer, i])
                        avg = (state_real[:, i] + state_real[:, i+1]) / 2
                        state_real[:, i] = (1-strength) * state_real[:, i] + strength * avg
                        state_real[:, i+1] = (1-strength) * state_real[:, i+1] + strength * avg
                
                # æ¸¬å®š
                measurements = torch.sqrt(state_real**2 + state_imag**2)
                return measurements
        
        class CQCNNModel(nn.Module):
            def __init__(self, n_qubits, n_layers):
                super().__init__()
                # CNNç‰¹å¾´æŠ½å‡ºï¼ˆ7ãƒãƒ£ãƒ³ãƒãƒ«å…¥åŠ›ï¼‰
                self.cnn = nn.Sequential(
                    nn.Conv2d(7, 16, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(16),
                    nn.Conv2d(16, 32, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Conv2d(32, 64, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.Flatten()
                )
                
                # æ¬¡å…ƒå‰Šæ¸›
                self.reduction = nn.Sequential(
                    nn.Linear(64 * 3 * 3, 64),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(64, n_qubits),
                    nn.Tanh()
                )
                
                # é‡å­å›è·¯å±¤
                self.quantum = QuantumLayer(n_qubits, n_layers)
                
                # å‡ºåŠ›å±¤
                self.output = nn.Sequential(
                    nn.Linear(n_qubits, 32),
                    nn.ReLU(),
                    nn.Linear(32, 2)
                )
            
            def forward(self, x):
                x = self.cnn(x)
                x = self.reduction(x)
                x = self.quantum(x)
                x = self.output(x)
                return x
        
        return CQCNNModel(self.n_qubits, self.n_layers)
    
    def estimate(self, board: np.ndarray, 
                enemy_positions: List[Tuple[int, int]], 
                player: str,
                my_pieces: Dict[Tuple[int, int], str],
                turn: int) -> Dict[Tuple[int, int], Dict[str, float]]:
        
        self.model.eval()
        results = {}
        
        # 7ãƒãƒ£ãƒ³ãƒãƒ«ãƒ†ãƒ³ã‚½ãƒ«ã‚’æº–å‚™
        tensor = self.prepare_tensor_7ch(board, player, my_pieces, turn)
        
        with torch.no_grad():
            output = self.model(tensor)
            probs = F.softmax(output, dim=1)
            
            # ä½ç½®ã«ã‚ˆã‚‹èª¿æ•´ã‚’åŠ ãˆãŸæ¨å®š
            for pos in enemy_positions:
                # ä½ç½®ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
                position_factor = (pos[0] / 5.0 + pos[1] / 5.0) / 2.0
                adjusted_probs = probs[0] * (0.7 + 0.3 * position_factor)
                adjusted_probs = adjusted_probs / adjusted_probs.sum()
                
                results[pos] = {
                    'good_prob': adjusted_probs[0].item(),
                    'bad_prob': adjusted_probs[1].item(),
                    'confidence': max(adjusted_probs.tolist())
                }
        
        return results
    
    def get_name(self) -> str:
        return f"CQCNN(q={self.n_qubits},l={self.n_layers})"


# ================================================================================
# Part 4: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«3 - å ±é…¬é–¢æ•°
# ================================================================================

class RewardFunction(ABC):
    """å ±é…¬é–¢æ•°ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def calculate_move_reward(self, game_state: GameState, move: Tuple,
                            player: str, piece_info: Dict) -> float:
        """ç§»å‹•ã«å¯¾ã™ã‚‹å ±é…¬ã‚’è¨ˆç®—"""
        pass
    
    @abstractmethod
    def calculate_state_reward(self, game_state: GameState, player: str) -> float:
        """çŠ¶æ…‹ã«å¯¾ã™ã‚‹å ±é…¬ã‚’è¨ˆç®—"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """å ±é…¬é–¢æ•°åã‚’å–å¾—"""
        pass


class StandardRewardFunction(RewardFunction):
    """æ¨™æº–å ±é…¬é–¢æ•°"""
    
    def calculate_move_reward(self, game_state: GameState, move: Tuple,
                            player: str, piece_info: Dict) -> float:
        from_pos, to_pos = move
        reward = 0.0
        
        # é§’ã‚¿ã‚¤ãƒ—ã‚’å–å¾—
        piece_type = piece_info.get(from_pos, 'unknown')
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        if piece_type == 'good':
            # å–„ç‰ï¼šè„±å‡ºã‚’æœ€å„ªå…ˆ
            escape_positions = [(5, 0), (5, 5)] if player == "A" else [(0, 0), (0, 5)]
            
            # è„±å‡ºå£ã¸ã®æ¥è¿‘
            min_dist_before = min(abs(from_pos[0] - ep[0]) + abs(from_pos[1] - ep[1]) 
                                 for ep in escape_positions)
            min_dist_after = min(abs(to_pos[0] - ep[0]) + abs(to_pos[1] - ep[1]) 
                                for ep in escape_positions)
            
            if min_dist_after < min_dist_before:
                reward += 2.0 * (min_dist_before - min_dist_after)
            
            # è„±å‡ºæˆåŠŸ
            if to_pos in escape_positions:
                reward += 100.0
            
            # ãƒªã‚¹ã‚¯è©•ä¾¡ï¼ˆæ•µã«éš£æ¥ï¼‰
            for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
                check_pos = (to_pos[0] + dx, to_pos[1] + dy)
                if (0 <= check_pos[0] < 6 and 0 <= check_pos[1] < 6 and
                    game_state.board[check_pos] == enemy_val):
                    reward -= 1.0
        
        elif piece_type == 'bad':
            # æ‚ªç‰ï¼šæ”»æ’ƒã‚’é‡è¦–
            if game_state.board[to_pos] == enemy_val:
                reward += 5.0
        
        # å…±é€šï¼šå‰é€²ãƒœãƒ¼ãƒŠã‚¹
        if player == "A":
            reward += (to_pos[0] - from_pos[0]) * 0.3
        else:
            reward += (from_pos[0] - to_pos[0]) * 0.3
        
        return reward
    
    def calculate_state_reward(self, game_state: GameState, player: str) -> float:
        reward = 0.0
        
        # é§’æ•°ã®å·®
        if player == "A":
            my_pieces = len(game_state.player_a_pieces)
            enemy_pieces = len(game_state.player_b_pieces)
        else:
            my_pieces = len(game_state.player_b_pieces)
            enemy_pieces = len(game_state.player_a_pieces)
        
        reward += (my_pieces - enemy_pieces) * 2.0
        
        # å‹æ•—
        if game_state.winner == player:
            reward += 1000.0
        elif game_state.winner and game_state.winner != player:
            reward -= 1000.0
        
        return reward
    
    def get_name(self) -> str:
        return "æ¨™æº–å ±é…¬"


class AggressiveRewardFunction(RewardFunction):
    """æ”»æ’ƒçš„å ±é…¬é–¢æ•°"""
    
    def calculate_move_reward(self, game_state: GameState, move: Tuple,
                            player: str, piece_info: Dict) -> float:
        from_pos, to_pos = move
        reward = 0.0
        
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        # æ•µé§’ã‚’å–ã‚‹ã“ã¨ã‚’æœ€é‡è¦–
        if game_state.board[to_pos] == enemy_val:
            reward += 10.0
        
        # ç©æ¥µçš„ãªå‰é€²
        if player == "A":
            reward += (to_pos[0] - from_pos[0]) * 1.0
        else:
            reward += (from_pos[0] - to_pos[0]) * 1.0
        
        return reward
    
    def calculate_state_reward(self, game_state: GameState, player: str) -> float:
        return StandardRewardFunction().calculate_state_reward(game_state, player)
    
    def get_name(self) -> str:
        return "æ”»æ’ƒçš„å ±é…¬"


class DefensiveRewardFunction(RewardFunction):
    """é˜²å¾¡çš„å ±é…¬é–¢æ•°"""
    
    def calculate_move_reward(self, game_state: GameState, move: Tuple,
                            player: str, piece_info: Dict) -> float:
        from_pos, to_pos = move
        reward = 0.0
        
        piece_type = piece_info.get(from_pos, 'unknown')
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        if piece_type == 'good':
            # å–„ç‰ã®å®‰å…¨ã‚’æœ€å„ªå…ˆ
            safe_distance = self._calculate_safety(to_pos, game_state, player)
            reward += safe_distance * 1.0
            
            # æ…é‡ãªè„±å‡º
            escape_positions = [(5, 0), (5, 5)] if player == "A" else [(0, 0), (0, 5)]
            if to_pos in escape_positions:
                if self._is_safe_position(to_pos, game_state, player):
                    reward += 100.0
                else:
                    reward -= 5.0
        
        return reward
    
    def _calculate_safety(self, pos: Tuple, game_state: GameState, player: str) -> float:
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        min_enemy_dist = 10
        for i in range(6):
            for j in range(6):
                if game_state.board[i, j] == enemy_val:
                    dist = abs(pos[0] - i) + abs(pos[1] - j)
                    min_enemy_dist = min(min_enemy_dist, dist)
        
        return min_enemy_dist
    
    def _is_safe_position(self, pos: Tuple, game_state: GameState, player: str) -> bool:
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
            check_pos = (pos[0] + dx, pos[1] + dy)
            if (0 <= check_pos[0] < 6 and 0 <= check_pos[1] < 6 and
                game_state.board[check_pos] == enemy_val):
                return False
        return True
    
    def calculate_state_reward(self, game_state: GameState, player: str) -> float:
        reward = StandardRewardFunction().calculate_state_reward(game_state, player)
        
        # å–„ç‰ã®ç”Ÿå­˜ãƒœãƒ¼ãƒŠã‚¹
        if player == "A":
            my_pieces = game_state.player_a_pieces
        else:
            my_pieces = game_state.player_b_pieces
        
        good_count = sum(1 for piece_type in my_pieces.values() if piece_type == 'good')
        reward += good_count * 5.0
        
        return reward
    
    def get_name(self) -> str:
        return "é˜²å¾¡çš„å ±é…¬"


# ================================================================================
# Part 5: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«4 - Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨
# ================================================================================

class QMapGenerator(ABC):
    """Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def generate(self, board: np.ndarray, estimations: Dict,
                my_pieces: Dict, player: str, 
                reward_function: RewardFunction = None,
                game_state: GameState = None) -> np.ndarray:
        """Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """ç”Ÿæˆå™¨åã‚’å–å¾—"""
        pass


class SimpleQMapGenerator(QMapGenerator):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªQå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def generate(self, board: np.ndarray, estimations: Dict,
                my_pieces: Dict, player: str,
                reward_function: RewardFunction = None,
                game_state: GameState = None) -> np.ndarray:
        
        q_map = np.zeros((6, 6, 4))
        
        for pos, piece_type in my_pieces.items():
            base_value = 1.0 if piece_type == "good" else 0.5
            
            for i, (dx, dy) in enumerate([(0, 1), (0, -1), (1, 0), (-1, 0)]):
                new_pos = (pos[0] + dx, pos[1] + dy)
                
                if not (0 <= new_pos[0] < 6 and 0 <= new_pos[1] < 6):
                    q_map[pos[0], pos[1], i] = -100
                    continue
                
                q_value = base_value
                
                # æ¨å®šçµæœã‚’ä½¿ç”¨
                if new_pos in estimations:
                    est = estimations[new_pos]
                    if piece_type == "bad":
                        q_value += est['good_prob'] * 2.0
                        q_value += est['bad_prob'] * 1.0
                    else:
                        q_value -= est['bad_prob'] * 0.5
                
                # å ±é…¬é–¢æ•°ã‚’é©ç”¨
                if reward_function and game_state:
                    move = (pos, new_pos)
                    reward = reward_function.calculate_move_reward(
                        game_state, move, player, my_pieces
                    )
                    q_value += reward * 0.1
                
                q_map[pos[0], pos[1], i] = q_value
        
        return q_map
    
    def get_name(self) -> str:
        return "ã‚·ãƒ³ãƒ—ãƒ«Qå€¤"


class StrategicQMapGenerator(QMapGenerator):
    """æˆ¦ç•¥çš„Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def generate(self, board: np.ndarray, estimations: Dict,
                my_pieces: Dict, player: str,
                reward_function: RewardFunction = None,
                game_state: GameState = None) -> np.ndarray:
        
        q_map = np.zeros((6, 6, 4))
        
        # è„±å‡ºå£ã®ä½ç½®
        if player == "A":
            escape_positions = [(5, 0), (5, 5)]
        else:
            escape_positions = [(0, 0), (0, 5)]
        
        for pos, piece_type in my_pieces.items():
            for i, (dx, dy) in enumerate([(0, 1), (0, -1), (1, 0), (-1, 0)]):
                new_pos = (pos[0] + dx, pos[1] + dy)
                
                if not (0 <= new_pos[0] < 6 and 0 <= new_pos[1] < 6):
                    q_map[pos[0], pos[1], i] = -100
                    continue
                
                q_value = 0.0
                
                if piece_type == "good":
                    # å–„ç‰ï¼šè„±å‡ºæˆ¦ç•¥
                    min_dist_before = min(abs(pos[0] - ep[0]) + abs(pos[1] - ep[1]) 
                                         for ep in escape_positions)
                    min_dist_after = min(abs(new_pos[0] - ep[0]) + abs(new_pos[1] - ep[1]) 
                                        for ep in escape_positions)
                    
                    if min_dist_after < min_dist_before:
                        q_value += 3.0 + (min_dist_before - min_dist_after) * 1.5
                    
                    if new_pos in escape_positions:
                        q_value += 10.0
                    
                    if new_pos in estimations:
                        est = estimations[new_pos]
                        q_value -= est['bad_prob'] * 2.0
                        q_value += est['good_prob'] * 1.0
                
                else:  # bad
                    # æ‚ªç‰ï¼šæ”»æ’ƒæˆ¦ç•¥
                    if new_pos in estimations:
                        est = estimations[new_pos]
                        q_value += est['good_prob'] * 3.0
                        q_value += est['bad_prob'] * 1.5
                        q_value += est['confidence'] * 0.5
                    
                    # ç›¸æ‰‹ã®è„±å‡ºå£ã‚’å®ˆã‚‹
                    enemy_escape = [(0, 0), (0, 5)] if player == "A" else [(5, 0), (5, 5)]
                    for ep in enemy_escape:
                        if abs(new_pos[0] - ep[0]) + abs(new_pos[1] - ep[1]) <= 2:
                            q_value += 1.0
                
                # å ±é…¬é–¢æ•°ã‚’çµ±åˆ
                if reward_function and game_state:
                    move = (pos, new_pos)
                    reward = reward_function.calculate_move_reward(
                        game_state, move, player, my_pieces
                    )
                    q_value += reward * 0.2
                
                q_map[pos[0], pos[1], i] = q_value
        
        return q_map
    
    def get_name(self) -> str:
        return "æˆ¦ç•¥çš„Qå€¤"


# ================================================================================
# Part 6: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«5 - è¡Œå‹•é¸æŠå™¨
# ================================================================================

class ActionSelector(ABC):
    """è¡Œå‹•é¸æŠå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def select(self, q_map: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        """è¡Œå‹•ã‚’é¸æŠ"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """é¸æŠå™¨åã‚’å–å¾—"""
        pass


class GreedySelector(ActionSelector):
    """è²ªæ¬²é¸æŠå™¨"""
    
    def select(self, q_map: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        if not legal_moves:
            return None
        
        best_move = None
        best_value = -float('inf')
        
        for from_pos, to_pos in legal_moves:
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            dir_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3}
            if (dx, dy) in dir_map:
                dir_idx = dir_map[(dx, dy)]
                value = q_map[from_pos[0], from_pos[1], dir_idx]
                
                if value > best_value:
                    best_value = value
                    best_move = (from_pos, to_pos)
        
        return best_move or random.choice(legal_moves)
    
    def get_name(self) -> str:
        return "è²ªæ¬²é¸æŠ"


class EpsilonGreedySelector(ActionSelector):
    """Îµ-è²ªæ¬²é¸æŠå™¨"""
    
    def __init__(self, epsilon: float = 0.1):
        self.epsilon = epsilon
    
    def select(self, q_map: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        if not legal_moves:
            return None
        
        if random.random() < self.epsilon:
            return random.choice(legal_moves)
        
        return GreedySelector().select(q_map, legal_moves)
    
    def get_name(self) -> str:
        return f"Îµè²ªæ¬²(Îµ={self.epsilon})"


class SoftmaxSelector(ActionSelector):
    """ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é¸æŠå™¨"""
    
    def __init__(self, temperature: float = 1.0):
        self.temperature = temperature
    
    def select(self, q_map: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        if not legal_moves:
            return None
        
        q_values = []
        dir_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3}
        
        for from_pos, to_pos in legal_moves:
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            if (dx, dy) in dir_map:
                dir_idx = dir_map[(dx, dy)]
                value = q_map[from_pos[0], from_pos[1], dir_idx]
            else:
                value = -100
            
            q_values.append(value / self.temperature)
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç¢ºç‡
        q_tensor = torch.tensor(q_values, dtype=torch.float32)
        probs = F.softmax(q_tensor, dim=0).numpy()
        
        # ç¢ºç‡çš„ã«é¸æŠ
        idx = np.random.choice(len(legal_moves), p=probs)
        return legal_moves[idx]
    
    def get_name(self) -> str:
        return f"ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹(T={self.temperature})"


# ================================================================================
# Part 7: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±åˆ
# ================================================================================

@dataclass
class ModuleConfig:
    """5ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ±åˆã™ã‚‹è¨­å®š"""
    placement_strategy: PlacementStrategy
    piece_estimator: PieceEstimator
    reward_function: RewardFunction
    qmap_generator: QMapGenerator
    action_selector: ActionSelector


class ModularAgent:
    """5ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, player_id: str, config: ModuleConfig):
        self.player_id = player_id
        self.config = config
        self.name = self._generate_name()
        self.piece_info = {}  # è‡ªåˆ†ã®é§’ã‚¿ã‚¤ãƒ—ã‚’ä¿æŒ
        self.game_history = []
    
    def _generate_name(self) -> str:
        return f"Agent_{self.player_id}[" \
               f"{self.config.placement_strategy.get_name()[:3]}+" \
               f"{self.config.piece_estimator.get_name()[:6]}+" \
               f"{self.config.reward_function.get_name()[:3]}+" \
               f"{self.config.qmap_generator.get_name()[:3]}+" \
               f"{self.config.action_selector.get_name()[:3]}]"
    
    def get_initial_placement(self) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’å–å¾—"""
        placement = self.config.placement_strategy.get_placement(self.player_id)
        self.piece_info = placement.copy()
        return placement
    
    def get_move(self, game_state: GameState, legal_moves: List[Tuple]) -> Tuple:
        """æ¬¡ã®æ‰‹ã‚’å–å¾—"""
        if not legal_moves:
            return None
        
        try:
            # è‡ªåˆ†ã®é§’æƒ…å ±ã‚’æ›´æ–°
            self._update_piece_info(game_state)
            
            # 1. æ•µé§’ä½ç½®ã‚’ç‰¹å®š
            enemy_positions = self._find_enemy_positions(game_state)
            
            # 2. æ•µé§’ã‚’æ¨å®šï¼ˆ7ãƒãƒ£ãƒ³ãƒãƒ«å…¥åŠ›ï¼‰
            estimations = {}
            if enemy_positions:
                estimations = self.config.piece_estimator.estimate(
                    board=game_state.board,
                    enemy_positions=enemy_positions,
                    player=self.player_id,
                    my_pieces=self.piece_info,
                    turn=game_state.turn
                )
            
            # 3. Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆï¼ˆå ±é…¬é–¢æ•°ã‚‚çµ±åˆï¼‰
            q_map = self.config.qmap_generator.generate(
                board=game_state.board,
                estimations=estimations,
                my_pieces=self.piece_info,
                player=self.player_id,
                reward_function=self.config.reward_function,
                game_state=game_state
            )
            
            # 4. è¡Œå‹•ã‚’é¸æŠ
            selected_move = self.config.action_selector.select(q_map, legal_moves)
            
            # å±¥æ­´ã«è¨˜éŒ²
            self.game_history.append({
                'turn': game_state.turn,
                'move': selected_move,
                'estimations': len(estimations),
                'q_max': np.max(q_map) if q_map is not None else 0
            })
            
            return selected_move
            
        except Exception as e:
            print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ in {self.name}: {e}")
            return random.choice(legal_moves)
    
    def _update_piece_info(self, game_state: GameState):
        """é§’ã‚¿ã‚¤ãƒ—æƒ…å ±ã‚’æ›´æ–°"""
        if self.player_id == "A":
            current_pieces = game_state.player_a_pieces
        else:
            current_pieces = game_state.player_b_pieces
        
        # ç¾åœ¨ã®é§’ä½ç½®ã«åˆã‚ã›ã¦æ›´æ–°
        new_piece_info = {}
        for pos, piece_type in current_pieces.items():
            # å…ƒã®é§’ã‚¿ã‚¤ãƒ—ã‚’ä¿æŒ
            for old_pos, old_type in self.piece_info.items():
                if old_type == piece_type:
                    new_piece_info[pos] = old_type
                    break
        
        self.piece_info = new_piece_info
    
    def _find_enemy_positions(self, game_state: GameState) -> List[Tuple[int, int]]:
        """æ•µé§’ã®ä½ç½®ã‚’ç‰¹å®š"""
        enemy_val = -1 if self.player_id == "A" else 1
        positions = []
        
        for i in range(6):
            for j in range(6):
                if game_state.board[i, j] == enemy_val:
                    positions.append((i, j))
        
        return positions


# ================================================================================
# Part 8: ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã¨å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================

class GameEngine:
    """ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: GameConfig = None):
        self.config = config or GameConfig()
        self.state = GameState()
        self.move_history = []
    
    def start_new_game(self, agent1: ModularAgent, agent2: ModularAgent):
        """æ–°ã—ã„ã‚²ãƒ¼ãƒ ã‚’é–‹å§‹"""
        self.state = GameState()
        self.move_history = []
        
        # åˆæœŸé…ç½®
        placement1 = agent1.get_initial_placement()
        placement2 = agent2.get_initial_placement()
        
        for pos, piece_type in placement1.items():
            self.state.board[pos] = 1
            self.state.player_a_pieces[pos] = piece_type
        
        for pos, piece_type in placement2.items():
            self.state.board[pos] = -1
            self.state.player_b_pieces[pos] = piece_type
        
        return self.state
    
    def get_legal_moves(self, player: str) -> List[Tuple]:
        """åˆæ³•æ‰‹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        legal_moves = []
        pieces = self.state.player_a_pieces if player == "A" else self.state.player_b_pieces
        
        for pos in pieces.keys():
            for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
                new_pos = (pos[0] + dx, pos[1] + dy)
                if self._is_valid_move(pos, new_pos, player):
                    legal_moves.append((pos, new_pos))
        
        return legal_moves
    
    def _is_valid_move(self, from_pos: Tuple, to_pos: Tuple, player: str) -> bool:
        if not (0 <= to_pos[0] < 6 and 0 <= to_pos[1] < 6):
            return False
        
        player_val = 1 if player == "A" else -1
        if self.state.board[to_pos] == player_val:
            return False
        
        return True
    
    def make_move(self, from_pos: Tuple, to_pos: Tuple, player: str):
        """æ‰‹ã‚’å®Ÿè¡Œ"""
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        # æ•µé§’ã‚’å–ã‚‹
        if self.state.board[to_pos] == enemy_val:
            enemy_pieces = self.state.player_b_pieces if player == "A" else self.state.player_a_pieces
            if to_pos in enemy_pieces:
                del enemy_pieces[to_pos]
        
        # ç§»å‹•
        self.state.board[from_pos] = 0
        self.state.board[to_pos] = player_val
        
        pieces = self.state.player_a_pieces if player == "A" else self.state.player_b_pieces
        piece_type = pieces.pop(from_pos)
        pieces[to_pos] = piece_type
        
        # è„±å‡ºåˆ¤å®š
        escape_positions = [(5, 0), (5, 5)] if player == "A" else [(0, 0), (0, 5)]
        if to_pos in escape_positions and piece_type == 'good':
            self.state.winner = player
        
        self.state.turn += 1
        
        # å±¥æ­´ã«è¨˜éŒ²
        self.move_history.append({
            'turn': self.state.turn,
            'player': player,
            'from': from_pos,
            'to': to_pos,
            'piece_type': piece_type
        })
    
    def check_winner(self) -> Optional[str]:
        """å‹è€…ã‚’åˆ¤å®š"""
        if self.state.winner:
            return self.state.winner
        
        # å–„ç‰ãŒå…¨æ»…
        a_good = sum(1 for t in self.state.player_a_pieces.values() if t == 'good')
        b_good = sum(1 for t in self.state.player_b_pieces.values() if t == 'good')
        
        if a_good == 0:
            return "B"
        if b_good == 0:
            return "A"
        
        # ã‚¿ãƒ¼ãƒ³åˆ¶é™
        if self.state.turn >= self.config.max_turns:
            if a_good > b_good:
                return "A"
            elif b_good > a_good:
                return "B"
            else:
                return "Draw"
        
        return None


class BattleSystem:
    """å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.engine = GameEngine()
        self.results = []
    
    def run_match(self, agent1: ModularAgent, agent2: ModularAgent, 
                  verbose: bool = False) -> Dict:
        """1è©¦åˆã‚’å®Ÿè¡Œ"""
        # ã‚²ãƒ¼ãƒ é–‹å§‹
        self.engine.start_new_game(agent1, agent2)
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"å¯¾æˆ¦: {agent1.name} vs {agent2.name}")
            print(f"{'='*60}")
        
        # ã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ—
        current_player = "A"
        current_agent = agent1
        
        while not self.engine.state.is_game_over():
            # åˆæ³•æ‰‹ã‚’å–å¾—
            legal_moves = self.engine.get_legal_moves(current_player)
            
            if not legal_moves:
                break
            
            # æ‰‹ã‚’é¸æŠ
            move = current_agent.get_move(self.engine.state, legal_moves)
            
            if move:
                self.engine.make_move(move[0], move[1], current_player)
                
                if verbose and self.engine.state.turn % 10 == 0:
                    print(f"Turn {self.engine.state.turn}: "
                          f"Player {current_player} moved {move[0]}â†’{move[1]}")
            
            # å‹è€…åˆ¤å®š
            winner = self.engine.check_winner()
            if winner:
                self.engine.state.winner = winner
                break
            
            # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼äº¤ä»£
            if current_player == "A":
                current_player = "B"
                current_agent = agent2
            else:
                current_player = "A"
                current_agent = agent1
        
        # çµæœè¨˜éŒ²
        result = {
            'winner': self.engine.state.winner,
            'turns': self.engine.state.turn,
            'agent1': agent1.name,
            'agent2': agent2.name,
            'timestamp': datetime.now().isoformat()
        }
        
        self.results.append(result)
        
        if verbose:
            print(f"\nå‹è€…: {result['winner']}, ã‚¿ãƒ¼ãƒ³æ•°: {result['turns']}")
        
        return result


# ================================================================================
# Part 9: ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
# ================================================================================

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ """
    print("="*70)
    print("ğŸŒŸ CQCNNå¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ  v2 - 5ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆ")
    print("="*70)
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    modules = {
        'placement': [
            StandardPlacement(),
            AggressivePlacement(),
            DefensivePlacement()
        ],
        'estimator': [
            RandomEstimator(),
            SimpleCNNEstimator(),
            CQCNNEstimator(n_qubits=4, n_layers=2),
            CQCNNEstimator(n_qubits=6, n_layers=3)
        ],
        'reward': [
            StandardRewardFunction(),
            AggressiveRewardFunction(),
            DefensiveRewardFunction()
        ],
        'qmap': [
            SimpleQMapGenerator(),
            StrategicQMapGenerator()
        ],
        'selector': [
            GreedySelector(),
            EpsilonGreedySelector(epsilon=0.1),
            EpsilonGreedySelector(epsilon=0.3),
            SoftmaxSelector(temperature=1.0)
        ]
    }
    
    # ã‚µãƒ³ãƒ—ãƒ«æ§‹æˆ
    print("\nğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«æ§‹æˆ:")
    print("1. ãƒãƒ©ãƒ³ã‚¹å‹")
    print("2. æ”»æ’ƒå‹")
    print("3. é˜²å¾¡å‹")
    print("4. ã‚«ã‚¹ã‚¿ãƒ ")
    
    choice = input("\né¸æŠ (1-4): ")
    
    if choice == "1":
        # ãƒãƒ©ãƒ³ã‚¹å‹
        config1 = ModuleConfig(
            placement_strategy=StandardPlacement(),
            piece_estimator=CQCNNEstimator(n_qubits=6, n_layers=3),
            reward_function=StandardRewardFunction(),
            qmap_generator=StrategicQMapGenerator(),
            action_selector=EpsilonGreedySelector(epsilon=0.1)
        )
        config2 = ModuleConfig(
            placement_strategy=StandardPlacement(),
            piece_estimator=SimpleCNNEstimator(),
            reward_function=StandardRewardFunction(),
            qmap_generator=SimpleQMapGenerator(),
            action_selector=GreedySelector()
        )
        
    elif choice == "2":
        # æ”»æ’ƒå‹
        config1 = ModuleConfig(
            placement_strategy=AggressivePlacement(),
            piece_estimator=CQCNNEstimator(n_qubits=4, n_layers=2),
            reward_function=AggressiveRewardFunction(),
            qmap_generator=SimpleQMapGenerator(),
            action_selector=GreedySelector()
        )
        config2 = ModuleConfig(
            placement_strategy=StandardPlacement(),
            piece_estimator=RandomEstimator(),
            reward_function=StandardRewardFunction(),
            qmap_generator=SimpleQMapGenerator(),
            action_selector=EpsilonGreedySelector(epsilon=0.3)
        )
        
    elif choice == "3":
        # é˜²å¾¡å‹
        config1 = ModuleConfig(
            placement_strategy=DefensivePlacement(),
            piece_estimator=CQCNNEstimator(n_qubits=6, n_layers=3),
            reward_function=DefensiveRewardFunction(),
            qmap_generator=StrategicQMapGenerator(),
            action_selector=SoftmaxSelector(temperature=1.0)
        )
        config2 = ModuleConfig(
            placement_strategy=AggressivePlacement(),
            piece_estimator=SimpleCNNEstimator(),
            reward_function=AggressiveRewardFunction(),
            qmap_generator=SimpleQMapGenerator(),
            action_selector=GreedySelector()
        )
        
    else:
        # ã‚«ã‚¹ã‚¿ãƒ æ§‹æˆ
        print("\nğŸ”§ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠ:")
        
        print("\nåˆæœŸé…ç½®æˆ¦ç•¥:")
        for i, m in enumerate(modules['placement']):
            print(f"  {i}: {m.get_name()}")
        p1 = int(input("Agent1ã®é…ç½®æˆ¦ç•¥: "))
        p2 = int(input("Agent2ã®é…ç½®æˆ¦ç•¥: "))
        
        print("\næ•µé§’æ¨å®šå™¨:")
        for i, m in enumerate(modules['estimator']):
            print(f"  {i}: {m.get_name()}")
        e1 = int(input("Agent1ã®æ¨å®šå™¨: "))
        e2 = int(input("Agent2ã®æ¨å®šå™¨: "))
        
        print("\nå ±é…¬é–¢æ•°:")
        for i, m in enumerate(modules['reward']):
            print(f"  {i}: {m.get_name()}")
        r1 = int(input("Agent1ã®å ±é…¬é–¢æ•°: "))
        r2 = int(input("Agent2ã®å ±é…¬é–¢æ•°: "))
        
        print("\nQå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨:")
        for i, m in enumerate(modules['qmap']):
            print(f"  {i}: {m.get_name()}")
        q1 = int(input("Agent1ã®Qå€¤ç”Ÿæˆå™¨: "))
        q2 = int(input("Agent2ã®Qå€¤ç”Ÿæˆå™¨: "))
        
        print("\nè¡Œå‹•é¸æŠå™¨:")
        for i, m in enumerate(modules['selector']):
            print(f"  {i}: {m.get_name()}")
        s1 = int(input("Agent1ã®é¸æŠå™¨: "))
        s2 = int(input("Agent2ã®é¸æŠå™¨: "))
        
        config1 = ModuleConfig(
            placement_strategy=modules['placement'][p1],
            piece_estimator=modules['estimator'][e1],
            reward_function=modules['reward'][r1],
            qmap_generator=modules['qmap'][q1],
            action_selector=modules['selector'][s1]
        )
        config2 = ModuleConfig(
            placement_strategy=modules['placement'][p2],
            piece_estimator=modules['estimator'][e2],
            reward_function=modules['reward'][r2],
            qmap_generator=modules['qmap'][q2],
            action_selector=modules['selector'][s2]
        )
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
    agent1 = ModularAgent("A", config1)
    agent2 = ModularAgent("B", config2)
    
    print(f"\nğŸ® å¯¾æˆ¦è¨­å®šå®Œäº†:")
    print(f"Agent1: {agent1.name}")
    print(f"Agent2: {agent2.name}")
    
    # å¯¾æˆ¦å®Ÿè¡Œ
    n_games = int(input("\nå¯¾æˆ¦æ•°: "))
    
    battle_system = BattleSystem()
    wins = {'A': 0, 'B': 0, 'Draw': 0}
    
    for i in range(n_games):
        print(f"\nGame {i+1}/{n_games}")
        result = battle_system.run_match(agent1, agent2, verbose=(i == 0))
        
        if result['winner'] == 'A':
            wins['A'] += 1
        elif result['winner'] == 'B':
            wins['B'] += 1
        else:
            wins['Draw'] += 1
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*70)
    print("ğŸ“Š æœ€çµ‚çµæœ:")
    print(f"Agent1å‹åˆ©: {wins['A']} ({wins['A']/n_games*100:.1f}%)")
    print(f"Agent2å‹åˆ©: {wins['B']} ({wins['B']/n_games*100:.1f}%)")
    print(f"å¼•ãåˆ†ã‘: {wins['Draw']} ({wins['Draw']/n_games*100:.1f}%)")
    print("="*70)


if __name__ == "__main__":
    main()
