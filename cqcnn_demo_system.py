#!/usr/bin/env python3
"""
åˆ†é›¢å‹CQCNNå­¦ç¿’ãƒ»ãƒ‡ãƒ¢å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 
é§’æ¨å®šã®å­¦ç¿’ã‹ã‚‰å®Ÿæˆ¦å¯¾æˆ¦ã¾ã§å®Œå…¨å®Ÿè£…
"""

import sys
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, Tuple, List, Optional, Any
import random
import time
import json
from dataclasses import dataclass
import matplotlib.pyplot as plt

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, 'src', 'qugeister_competitive')
sys.path.insert(0, src_path)
sys.path.insert(0, current_dir)

# å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥èª­ã¿è¾¼ã‚“ã§å®Ÿè¡Œ
print("ğŸ“‚ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ä¸­...")

# game_engine.pyã‚’èª­ã¿è¾¼ã¿
with open(os.path.join(src_path, 'game_engine.py'), 'r') as f:
    game_engine_code = f.read()
exec(game_engine_code)

# ai_base.pyã‚’èª­ã¿è¾¼ã¿ï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆéƒ¨åˆ†ã‚’å‰Šé™¤ï¼‰
with open(os.path.join(src_path, 'ai_base.py'), 'r') as f:
    ai_base_code = f.read()
# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¡Œã‚’å‰Šé™¤
ai_base_code = ai_base_code.replace('from .game_engine import GeisterGame, GameState', 
                                    '# from .game_engine import GeisterGame, GameState')
exec(ai_base_code)

# tournament.pyã‚’èª­ã¿è¾¼ã¿ï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆéƒ¨åˆ†ã‚’å‰Šé™¤ï¼‰
with open(os.path.join(src_path, 'tournament.py'), 'r') as f:
    tournament_code = f.read()
tournament_code = tournament_code.replace('from .game_engine import GeisterGame', 
                                        '# from .game_engine import GeisterGame')
tournament_code = tournament_code.replace('from .ai_base import BaseAI', 
                                        '# from .ai_base import BaseAI')
exec(tournament_code)

# separated_cqcnn_qmap.pyã‚’èª­ã¿è¾¼ã¿
from separated_cqcnn_qmap import (
    CQCNNPieceEstimator, 
    PieceEstimationDataExporter,
    QValueMapGenerator,
    IntegratedCQCNNSystem
)

print("âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")


# ================================================================================
# Part 1: CQCNNå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================

class CQCNNTrainer:
    """CQCNNé§’æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, estimator: CQCNNPieceEstimator, learning_rate: float = 0.001):
        self.estimator = estimator
        self.optimizer = optim.Adam(estimator.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()
        self.training_history = {
            'losses': [],
            'accuracies': [],
            'epochs': []
        }
    
    def generate_training_data(self, num_games: int = 100) -> List[Dict]:
        """ã‚²ãƒ¼ãƒ ãƒ—ãƒ¬ã‚¤ã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        print(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­ ({num_games}ã‚²ãƒ¼ãƒ )...")
        training_data = []
        
        for game_idx in range(num_games):
            game = GeisterGame()
            player_a = RandomAI("A")
            player_b = RandomAI("B")
            
            # ã‚²ãƒ¼ãƒ ã‚’é€²è¡Œ
            turn_count = 0
            max_turns = 50
            
            while not game.game_over and turn_count < max_turns:
                current_player = player_a if game.current_player == "A" else player_b
                legal_moves = game.get_legal_moves(game.current_player)
                
                if not legal_moves:
                    break
                
                # ç¾åœ¨ã®çŠ¶æ…‹ã‚’è¨˜éŒ²ï¼ˆç›¸æ‰‹é§’ã®çœŸã®ç¨®é¡ã‚’å«ã‚€ï¼‰
                if turn_count > 5:  # åºç›¤ã¯é™¤å¤–
                    board_state = game.board.copy()
                    
                    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Aã®è¦–ç‚¹
                    if random.random() < 0.5:
                        enemy_pieces = game.player_b_pieces.copy()
                        data_point = {
                            'board': board_state,
                            'player': 'A',
                            'enemy_pieces': enemy_pieces,
                            'turn': turn_count
                        }
                        training_data.append(data_point)
                    
                    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Bã®è¦–ç‚¹
                    else:
                        enemy_pieces = game.player_a_pieces.copy()
                        data_point = {
                            'board': board_state,
                            'player': 'B',
                            'enemy_pieces': enemy_pieces,
                            'turn': turn_count
                        }
                        training_data.append(data_point)
                
                # æ‰‹ã‚’å®Ÿè¡Œ
                move = current_player.get_move(game.get_game_state(game.current_player), legal_moves)
                if move:
                    game.make_move(move[0], move[1])
                
                turn_count += 1
            
            if (game_idx + 1) % 20 == 0:
                print(f"  {game_idx + 1}/{num_games} ã‚²ãƒ¼ãƒ å®Œäº†")
        
        print(f"âœ… {len(training_data)}å€‹ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
        return training_data
    
    def train(self, training_data: List[Dict], epochs: int = 50, batch_size: int = 32):
        """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        print(f"\nğŸ“ CQCNNå­¦ç¿’é–‹å§‹ (ã‚¨ãƒãƒƒã‚¯æ•°: {epochs})")
        print("=" * 60)
        
        for epoch in range(epochs):
            epoch_losses = []
            correct_predictions = 0
            total_predictions = 0
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            random.shuffle(training_data)
            
            # ãƒãƒƒãƒå‡¦ç†
            for i in range(0, len(training_data), batch_size):
                batch = training_data[i:i+batch_size]
                
                batch_loss = 0
                batch_correct = 0
                batch_total = 0
                
                for data in batch:
                    # ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
                    board_tensor = self._prepare_board_tensor(
                        data['board'], 
                        data['player']
                    )
                    
                    # æ•µé§’ã®ä½ç½®ã¨çœŸã®ãƒ©ãƒ™ãƒ«
                    enemy_positions = []
                    true_labels = []
                    
                    enemy_val = -1 if data['player'] == 'A' else 1
                    for pos, piece_type in data['enemy_pieces'].items():
                        if data['board'][pos[1], pos[0]] == enemy_val:
                            enemy_positions.append(pos)
                            # 0: good, 1: bad
                            true_labels.append(0 if piece_type == "good" else 1)
                    
                    if not enemy_positions:
                        continue
                    
                    # æ¨å®šå®Ÿè¡Œ
                    estimations = self.estimator(board_tensor, enemy_positions)
                    
                    # å„é§’ã«å¯¾ã™ã‚‹æå¤±è¨ˆç®—
                    for est, true_label in zip(estimations, true_labels):
                        # æ¨å®šç¢ºç‡ã‚’å–å¾—
                        pred_probs = torch.tensor(
                            [est.good_probability, est.bad_probability],
                            requires_grad=True
                        )
                        
                        # æå¤±è¨ˆç®—
                        target = torch.tensor([true_label], dtype=torch.long)
                        loss = self.criterion(pred_probs.unsqueeze(0), target)
                        batch_loss += loss
                        
                        # ç²¾åº¦è¨ˆç®—
                        pred_label = 0 if est.good_probability > est.bad_probability else 1
                        if pred_label == true_label:
                            batch_correct += 1
                        batch_total += 1
                
                # ãƒãƒƒãƒæ›´æ–°
                if batch_total > 0:
                    avg_loss = batch_loss / batch_total
                    self.optimizer.zero_grad()
                    avg_loss.backward()
                    self.optimizer.step()
                    
                    epoch_losses.append(avg_loss.item())
                    correct_predictions += batch_correct
                    total_predictions += batch_total
            
            # ã‚¨ãƒãƒƒã‚¯çµ±è¨ˆ
            if total_predictions > 0:
                epoch_accuracy = correct_predictions / total_predictions
                avg_epoch_loss = np.mean(epoch_losses) if epoch_losses else 0
                
                self.training_history['epochs'].append(epoch + 1)
                self.training_history['losses'].append(avg_epoch_loss)
                self.training_history['accuracies'].append(epoch_accuracy)
                
                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch+1:3d}/{epochs} | "
                          f"Loss: {avg_epoch_loss:.4f} | "
                          f"Accuracy: {epoch_accuracy:.3f}")
        
        print("\nâœ… å­¦ç¿’å®Œäº†ï¼")
        self._plot_training_history()
    
    def _prepare_board_tensor(self, board: np.ndarray, player: str) -> torch.Tensor:
        """ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
        tensor = torch.zeros(1, 3, 6, 6)
        player_val = 1 if player == 'A' else -1
        enemy_val = -player_val
        
        tensor[0, 0] = torch.from_numpy((board == player_val).astype(np.float32))
        tensor[0, 1] = torch.from_numpy((board == enemy_val).astype(np.float32))
        tensor[0, 2] = torch.from_numpy((board == 0).astype(np.float32))
        
        return tensor
    
    def _plot_training_history(self):
        """å­¦ç¿’å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        if not self.training_history['epochs']:
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # æå¤±æ¨ç§»
        ax1.plot(self.training_history['epochs'], 
                self.training_history['losses'], 'b-')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training Loss')
        ax1.grid(True)
        
        # ç²¾åº¦æ¨ç§»
        ax2.plot(self.training_history['epochs'], 
                self.training_history['accuracies'], 'g-')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.set_title('Piece Type Prediction Accuracy')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('cqcnn_training_history.png')
        print("ğŸ“Š å­¦ç¿’å±¥æ­´ã‚’ä¿å­˜: cqcnn_training_history.png")
    
    def save_model(self, filepath: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        torch.save({
            'model_state': self.estimator.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'training_history': self.training_history
        }, filepath)
        print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {filepath}")
    
    def load_model(self, filepath: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        checkpoint = torch.load(filepath)
        self.estimator.load_state_dict(checkpoint['model_state'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state'])
        self.training_history = checkpoint.get('training_history', {})
        print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿: {filepath}")


# ================================================================================
# Part 2: CQCNN AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
# ================================================================================

class CQCNNAI(BaseAI):
    """åˆ†é›¢å‹CQCNNã‚’ä½¿ç”¨ã™ã‚‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, player_id: str, system: IntegratedCQCNNSystem, name: str = "CQCNN_AI"):
        super().__init__(name, player_id)
        self.system = system
        self.last_q_map = None
        self.exploration_rate = 0.1  # æ¢ç´¢ç‡
    
    def get_move(self, game_state: GameState, legal_moves: List) -> Optional[Tuple]:
        """CQCNNã¨Qå€¤ãƒãƒƒãƒ—ã‚’ä½¿ã£ã¦æ‰‹ã‚’é¸æŠ"""
        if not legal_moves:
            return None
        
        # æ¢ç´¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ é¸æŠï¼‰
        if random.random() < self.exploration_rate:
            return random.choice(legal_moves)
        
        # ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        game_dict = {
            'board': game_state.board.tolist(),
            'current_player': self.player_id,
            'turn': game_state.turn
        }
        
        # è‡ªåˆ†ã®é§’æƒ…å ±
        my_pieces = game_state.player_a_pieces if self.player_id == "A" else game_state.player_b_pieces
        
        # é§’æ¨å®šã¨Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆ
        try:
            estimation_data, q_map = self.system.process_game_state(game_dict, my_pieces)
            self.last_q_map = q_map
            
            # æœ€é©ãªåˆæ³•æ‰‹ã‚’é¸æŠ
            best_move = None
            best_q = -float('inf')
            
            for move in legal_moves:
                from_pos, to_pos = move
                
                # ç§»å‹•æ–¹å‘ã‚’åˆ¤å®š
                dx = to_pos[0] - from_pos[0]
                dy = to_pos[1] - from_pos[1]
                
                # æ–¹å‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                if dy == -1 and dx == 0:    dir_idx = 0  # ä¸Š
                elif dx == 1 and dy == 0:   dir_idx = 1  # å³
                elif dy == 1 and dx == 0:   dir_idx = 2  # ä¸‹
                elif dx == -1 and dy == 0:  dir_idx = 3  # å·¦
                else: continue  # æ–œã‚ç§»å‹•ã¯ç„¡è¦–
                
                # Qå€¤ã‚’å–å¾—
                q_value = q_map[from_pos[1], from_pos[0], dir_idx]
                
                if q_value > best_q:
                    best_q = q_value
                    best_move = move
            
            return best_move if best_move else random.choice(legal_moves)
            
        except Exception as e:
            print(f"âš ï¸ CQCNNæ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            return random.choice(legal_moves)


# ================================================================================
# Part 3: ãƒ‡ãƒ¢å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================

class CQCNNDemoBattle:
    """CQCNN AIã®ãƒ‡ãƒ¢å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.results = {
            'games': [],
            'win_rates': {},
            'statistics': {}
        }
    
    def run_single_game(self, player_a: BaseAI, player_b: BaseAI, 
                       verbose: bool = False) -> Dict:
        """å˜ä¸€ã‚²ãƒ¼ãƒ å®Ÿè¡Œ"""
        game = GeisterGame()
        move_history = []
        
        if verbose:
            print(f"\nğŸ® {player_a.name} vs {player_b.name}")
            print("-" * 40)
        
        max_turns = 100
        for turn in range(max_turns):
            current_player = player_a if game.current_player == "A" else player_b
            legal_moves = game.get_legal_moves(game.current_player)
            
            if not legal_moves:
                break
            
            # æ‰‹ã‚’é¸æŠ
            game_state = game.get_game_state(game.current_player)
            move = current_player.get_move(game_state, legal_moves)
            
            if not move:
                break
            
            # æ‰‹ã‚’å®Ÿè¡Œ
            success = game.make_move(move[0], move[1])
            if not success:
                break
            
            move_history.append({
                'turn': turn,
                'player': game.current_player,
                'move': move
            })
            
            if verbose and turn < 10:  # æœ€åˆã®10æ‰‹ã®ã¿è¡¨ç¤º
                print(f"  Turn {turn+1}: {game.current_player} {move[0]} â†’ {move[1]}")
            
            if game.game_over:
                break
        
        # çµæœè¨˜éŒ²
        result = {
            'player_a': player_a.name,
            'player_b': player_b.name,
            'winner': game.winner,
            'turns': len(move_history),
            'move_history': move_history
        }
        
        if verbose:
            if game.winner in ["A", "B"]:
                winner_name = player_a.name if game.winner == "A" else player_b.name
                print(f"ğŸ† å‹è€…: {winner_name}")
            else:
                print("ğŸ¤ å¼•ãåˆ†ã‘")
            print(f"ğŸ“Š ç·æ‰‹æ•°: {len(move_history)}")
        
        return result
    
    def run_tournament(self, cqcnn_ai: CQCNNAI, opponents: List[BaseAI], 
                      games_per_opponent: int = 10) -> Dict:
        """ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ"""
        print("\nğŸ† CQCNNãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé–‹å§‹")
        print("=" * 60)
        
        total_games = 0
        total_wins = 0
        
        for opponent in opponents:
            wins = 0
            draws = 0
            losses = 0
            
            print(f"\nğŸ“Š vs {opponent.name} ({games_per_opponent}ã‚²ãƒ¼ãƒ )")
            
            for game_num in range(games_per_opponent):
                # å…ˆæ‰‹å¾Œæ‰‹ã‚’äº¤ä»£
                if game_num % 2 == 0:
                    cqcnn_ai.player_id = "A"
                    opponent.player_id = "B"
                    result = self.run_single_game(cqcnn_ai, opponent)
                    
                    if result['winner'] == "A":
                        wins += 1
                    elif result['winner'] == "B":
                        losses += 1
                    else:
                        draws += 1
                else:
                    cqcnn_ai.player_id = "B"
                    opponent.player_id = "A"
                    result = self.run_single_game(opponent, cqcnn_ai)
                    
                    if result['winner'] == "B":
                        wins += 1
                    elif result['winner'] == "A":
                        losses += 1
                    else:
                        draws += 1
                
                self.results['games'].append(result)
                total_games += 1
                
                if result['winner'] == cqcnn_ai.player_id:
                    total_wins += 1
            
            # å¯¾æˆ¦ç›¸æ‰‹åˆ¥ã®çµæœ
            win_rate = wins / games_per_opponent
            self.results['win_rates'][opponent.name] = {
                'wins': wins,
                'losses': losses,
                'draws': draws,
                'win_rate': win_rate
            }
            
            print(f"  çµæœ: {wins}å‹ {losses}æ•— {draws}åˆ† (å‹ç‡: {win_rate:.1%})")
        
        # å…¨ä½“çµ±è¨ˆ
        overall_win_rate = total_wins / total_games if total_games > 0 else 0
        self.results['statistics'] = {
            'total_games': total_games,
            'total_wins': total_wins,
            'overall_win_rate': overall_win_rate
        }
        
        print("\n" + "=" * 60)
        print(f"ğŸ“ˆ ç·åˆæˆç¸¾: {total_wins}/{total_games} (å‹ç‡: {overall_win_rate:.1%})")
        
        return self.results
    
    def analyze_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        print("\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ")
        print("-" * 40)
        
        # å¯¾æˆ¦ç›¸æ‰‹åˆ¥åˆ†æ
        for opponent_name, stats in self.results['win_rates'].items():
            print(f"\nvs {opponent_name}:")
            print(f"  å‹ç‡: {stats['win_rate']:.1%}")
            print(f"  è©³ç´°: {stats['wins']}å‹ {stats['losses']}æ•— {stats['draws']}åˆ†")
        
        # ã‚²ãƒ¼ãƒ é•·åˆ†æ
        game_lengths = [g['turns'] for g in self.results['games']]
        if game_lengths:
            avg_length = np.mean(game_lengths)
            print(f"\nå¹³å‡ã‚²ãƒ¼ãƒ é•·: {avg_length:.1f}æ‰‹")
            print(f"æœ€çŸ­ã‚²ãƒ¼ãƒ : {min(game_lengths)}æ‰‹")
            print(f"æœ€é•·ã‚²ãƒ¼ãƒ : {max(game_lengths)}æ‰‹")


# ================================================================================
# Part 4: ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ================================================================================

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ åˆ†é›¢å‹CQCNNå­¦ç¿’ãƒ»ãƒ‡ãƒ¢å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)
    
    # 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    print("\n[Phase 1] ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
    print("-" * 40)
    
    # CQCNNã‚·ã‚¹ãƒ†ãƒ ä½œæˆ
    system = IntegratedCQCNNSystem(n_qubits=8, n_layers=3)
    print("âœ… CQCNNã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    # å­¦ç¿’å™¨ä½œæˆ
    trainer = CQCNNTrainer(system.estimator)
    print("âœ… å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    # 2. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å­¦ç¿’
    print("\n[Phase 2] å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º")
    print("-" * 40)
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    training_data = trainer.generate_training_data(num_games=50)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    trainer.train(training_data, epochs=30, batch_size=16)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    trainer.save_model("cqcnn_model.pth")
    
    # 3. CQCNN AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
    print("\n[Phase 3] AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ")
    print("-" * 40)
    
    cqcnn_ai = CQCNNAI("A", system, "CQCNN_AI_v1")
    print(f"âœ… {cqcnn_ai.name}ã‚’ä½œæˆ")
    
    # å¯¾æˆ¦ç›¸æ‰‹ä½œæˆ
    opponents = [
        RandomAI("B"),
        SimpleAI("B"),
        AggressiveAI("B")
    ]
    
    print("âœ… å¯¾æˆ¦ç›¸æ‰‹AIæº–å‚™å®Œäº†:")
    for opp in opponents:
        print(f"  - {opp.name}")
    
    # 4. ãƒ‡ãƒ¢å¯¾æˆ¦
    print("\n[Phase 4] ãƒ‡ãƒ¢å¯¾æˆ¦")
    print("-" * 40)
    
    # å˜ä¸€ã‚²ãƒ¼ãƒ ãƒ‡ãƒ¢
    print("\nğŸ’¡ ãƒ‡ãƒ¢ã‚²ãƒ¼ãƒ : CQCNN_AI vs RandomAI")
    demo_battle = CQCNNDemoBattle()
    demo_result = demo_battle.run_single_game(cqcnn_ai, opponents[0], verbose=True)
    
    # 5. ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ
    print("\n[Phase 5] ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆ")
    print("-" * 40)
    
    tournament_results = demo_battle.run_tournament(
        cqcnn_ai, 
        opponents, 
        games_per_opponent=20
    )
    
    # 6. çµæœåˆ†æ
    print("\n[Phase 6] çµæœåˆ†æ")
    print("-" * 40)
    
    demo_battle.analyze_performance()
    
    # 7. çµæœä¿å­˜
    print("\n[Phase 7] çµæœä¿å­˜")
    print("-" * 40)
    
    # çµæœã‚’JSONã§ä¿å­˜
    with open("cqcnn_demo_results.json", "w") as f:
        json.dump({
            'training_history': trainer.training_history,
            'tournament_results': tournament_results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }, f, indent=2)
    
    print("âœ… çµæœã‚’ä¿å­˜: cqcnn_demo_results.json")
    
    # 8. å®Ÿé¨“ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("ğŸ¯ å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    
    print(f"å­¦ç¿’:")
    print(f"  - å­¦ç¿’ã‚²ãƒ¼ãƒ æ•°: 50")
    print(f"  - ã‚¨ãƒãƒƒã‚¯æ•°: 30")
    if trainer.training_history['accuracies']:
        final_acc = trainer.training_history['accuracies'][-1]
        print(f"  - æœ€çµ‚ç²¾åº¦: {final_acc:.3f}")
    
    print(f"\nãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆ:")
    print(f"  - ç·ã‚²ãƒ¼ãƒ æ•°: {tournament_results['statistics']['total_games']}")
    print(f"  - ç·åˆå‹ç‡: {tournament_results['statistics']['overall_win_rate']:.1%}")
    
    print("\nğŸ‰ å®Ÿé¨“å®Œäº†ï¼")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ å®Ÿè¡Œä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
