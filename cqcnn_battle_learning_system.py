#!/usr/bin/env python3
"""
CQCNNå¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ  v3 - å­¦ç¿’æ©Ÿèƒ½çµ±åˆç‰ˆ
5ã¤ã®ç‹¬ç«‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« + å­¦ç¿’æ©Ÿèƒ½ + 7ãƒãƒ£ãƒ³ãƒãƒ«å…¥åŠ›å¯¾å¿œ

ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ:
1. PlacementStrategy - åˆæœŸé…ç½®æˆ¦ç•¥
2. PieceEstimator - æ•µé§’æ¨å®šå™¨ï¼ˆå­¦ç¿’æ©Ÿèƒ½ä»˜ãï¼‰
3. RewardFunction - å ±é…¬é–¢æ•°
4. QMapGenerator - Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨
5. ActionSelector - è¡Œå‹•é¸æŠå™¨

å­¦ç¿’æ©Ÿèƒ½:
- æ•™å¸«ã‚ã‚Šå­¦ç¿’
- å¼·åŒ–å­¦ç¿’
- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å­¦ç¿’
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from typing import Dict, List, Tuple
from dataclasses import dataclass
from abc import ABC, abstractmethod
import random
import os
from collections import deque

# ================================================================================
# Part 1: åŸºæœ¬è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# ================================================================================


@dataclass
class GameConfig:
    """ã‚²ãƒ¼ãƒ è¨­å®š"""

    board_size: tuple = (6, 6)
    max_turns: int = 100
    n_pieces: int = 8
    n_good: int = 4
    n_bad: int = 4


@dataclass
class LearningConfig:
    """å­¦ç¿’è¨­å®š"""

    # å…±é€šè¨­å®š
    learning_rate: float = 0.001
    batch_size: int = 32
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    # æ•™å¸«ã‚ã‚Šå­¦ç¿’è¨­å®š
    supervised_epochs: int = 100
    validation_split: float = 0.2

    # å¼·åŒ–å­¦ç¿’è¨­å®š
    rl_episodes: int = 1000
    epsilon_start: float = 0.9
    epsilon_end: float = 0.05
    epsilon_decay: int = 500
    gamma: float = 0.95
    memory_size: int = 10000
    target_update: int = 10


class GameState:
    """ã‚²ãƒ¼ãƒ çŠ¶æ…‹"""

    def __init__(self):
        self.board = np.zeros((6, 6), dtype=int)
        self.player_a_pieces = {}
        self.player_b_pieces = {}
        self.turn = 0
        self.winner = None

    def is_game_over(self):
        return self.winner is not None or self.turn >= 100


# ================================================================================
# Part 2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ================================================================================


class DataProcessor:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def prepare_7channel_tensor(
        board: np.ndarray, player: str, my_pieces: Dict[Tuple[int, int], str], turn: int
    ) -> torch.Tensor:
        """7ãƒãƒ£ãƒ³ãƒãƒ«ãƒ†ãƒ³ã‚½ãƒ«ã‚’æº–å‚™"""
        channels = []

        # ãƒãƒ£ãƒ³ãƒãƒ«1: è‡ªåˆ†ã®é§’ä½ç½® (1: å­˜åœ¨, 0: ãªã—)
        my_pos = np.zeros_like(board)
        for (r, c), _ in my_pieces.items():
            if 0 <= r < board.shape[0] and 0 <= c < board.shape[1]:
                my_pos[r, c] = 1
        channels.append(my_pos)

        # ãƒãƒ£ãƒ³ãƒãƒ«2: è‡ªåˆ†ã®å–„ç‰ (1: å–„ç‰, 0: ãã®ä»–)
        my_good = np.zeros_like(board)
        for (r, c), piece_type in my_pieces.items():
            if piece_type == "good" and 0 <= r < board.shape[0] and 0 <= c < board.shape[1]:
                my_good[r, c] = 1
        channels.append(my_good)

        # ãƒãƒ£ãƒ³ãƒãƒ«3: è‡ªåˆ†ã®æ‚ªç‰ (1: æ‚ªç‰, 0: ãã®ä»–)
        my_bad = np.zeros_like(board)
        for (r, c), piece_type in my_pieces.items():
            if piece_type == "bad" and 0 <= r < board.shape[0] and 0 <= c < board.shape[1]:
                my_bad[r, c] = 1
        channels.append(my_bad)

        # ãƒãƒ£ãƒ³ãƒãƒ«4: æ•µã®é§’ä½ç½® (1: å­˜åœ¨, 0: ãªã—)
        enemy_pos = np.zeros_like(board)
        for r in range(board.shape[0]):
            for c in range(board.shape[1]):
                if board[r, c] != 0 and (r, c) not in my_pieces:
                    enemy_pos[r, c] = 1
        channels.append(enemy_pos)

        # ãƒãƒ£ãƒ³ãƒãƒ«5: ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼æƒ…å ± (1: ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼A, 0: ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼B)
        player_channel = np.ones_like(board) if player == "A" else np.zeros_like(board)
        channels.append(player_channel)

        # ãƒãƒ£ãƒ³ãƒãƒ«6: ã‚¿ãƒ¼ãƒ³æƒ…å ± (æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ãƒ¼ãƒ³æ•°)
        turn_channel = np.full_like(board, turn / 100.0)
        channels.append(turn_channel)

        # ãƒãƒ£ãƒ³ãƒãƒ«7: ãƒœãƒ¼ãƒ‰å¢ƒç•Œæƒ…å ± (ç«¯=1, ä¸­å¤®=0)
        boundary = np.zeros_like(board)
        boundary[0, :] = boundary[-1, :] = boundary[:, 0] = boundary[:, -1] = 1
        channels.append(boundary)

        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ› (1, 7, H, W)
        tensor = torch.FloatTensor(np.stack(channels)).unsqueeze(0)
        return tensor


# ================================================================================
# Part 3: CQCNNãƒ¢ãƒ‡ãƒ«å®šç¾©
# ================================================================================


class CQCNNModel(nn.Module):
    """Classical-Quantum CNN ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self, n_qubits: int = 6, n_layers: int = 3):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers

        # Classical CNNéƒ¨åˆ†
        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)

        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.3)

        # Quantum-inspiredéƒ¨åˆ†ï¼ˆé‡å­å›è·¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        self.quantum_dim = n_qubits * n_layers

        # Linearå±¤ã®å…¥åŠ›ã‚µã‚¤ã‚ºã‚’å‹•çš„è¨ˆç®—ç”¨ã®ä¸€æ™‚å¤‰æ•°
        self.quantum_linear = None
        self.quantum_layers = nn.ModuleList([nn.Linear(self.quantum_dim, self.quantum_dim) for _ in range(n_layers)])

        # å‡ºåŠ›å±¤
        self.classifier = nn.Sequential(
            nn.Linear(self.quantum_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 2),  # å–„ç‰/æ‚ªç‰ã®2ã‚¯ãƒ©ã‚¹
        )

        # å…¥åŠ›ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ãƒ€ãƒŸãƒ¼å…¥åŠ›
        self._initialize_linear_layers()

    def _initialize_linear_layers(self):
        """Linearå±¤ã®å…¥åŠ›ã‚µã‚¤ã‚ºã‚’å‹•çš„ã«è¨ˆç®—"""
        # ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§å½¢çŠ¶ã‚’ç¢ºèª
        dummy_input = torch.randn(1, 7, 6, 6)  # (batch, channels, height, width)
        with torch.no_grad():
            x = F.relu(self.conv1(dummy_input))
            x = self.pool(x)
            x = F.relu(self.conv2(x))
            x = self.pool(x)
            x = F.relu(self.conv3(x))

            # Flattenã—ãŸå¾Œã®ã‚µã‚¤ã‚ºã‚’å–å¾—
            flattened_size = x.view(x.size(0), -1).size(1)

            # quantum_linearã‚’æ­£ã—ã„ã‚µã‚¤ã‚ºã§åˆæœŸåŒ–
            self.quantum_linear = nn.Linear(flattened_size, self.quantum_dim)

    def forward(self, x):
        # Classical CNNå‡¦ç†
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = F.relu(self.conv3(x))
        x = self.dropout(x)

        # Flatten
        x = x.view(x.size(0), -1)

        # Quantum-inspiredå‡¦ç†
        x = F.relu(self.quantum_linear(x))

        for quantum_layer in self.quantum_layers:
            # é‡å­å›è·¯é¢¨ã®å‡¦ç†ï¼ˆé‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
            x_new = quantum_layer(x)
            x = F.normalize(x_new + x, dim=1)  # æ®‹å·®æ¥ç¶š + æ­£è¦åŒ–

        # åˆ†é¡
        output = self.classifier(x)
        return output


# ================================================================================
# Part 4: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«1 - åˆæœŸé…ç½®æˆ¦ç•¥
# ================================================================================


class PlacementStrategy(ABC):
    """åˆæœŸé…ç½®æˆ¦ç•¥ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’å–å¾—"""
        pass

    @abstractmethod
    def get_name(self) -> str:
        """æˆ¦ç•¥åã‚’å–å¾—"""
        pass


class StandardPlacement(PlacementStrategy):
    """æ¨™æº–é…ç½®æˆ¦ç•¥"""

    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}

        if player_id == "A":
            positions = [(0, 1), (0, 2), (0, 3), (0, 4), (1, 1), (1, 2), (1, 3), (1, 4)]
        else:
            positions = [(4, 1), (4, 2), (4, 3), (4, 4), (5, 1), (5, 2), (5, 3), (5, 4)]

        piece_types = ["good"] * 4 + ["bad"] * 4
        random.shuffle(piece_types)

        for pos, piece_type in zip(positions, piece_types):
            placement[pos] = piece_type

        return placement

    def get_name(self) -> str:
        return "æ¨™æº–é…ç½®"


class AggressivePlacement(PlacementStrategy):
    """æ”»æ’ƒçš„é…ç½®æˆ¦ç•¥ï¼ˆå–„ç‰ã‚’å‰ç·šã«ï¼‰"""

    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}

        if player_id == "A":
            front_positions = [(1, 1), (1, 2), (1, 3), (1, 4)]
            back_positions = [(0, 1), (0, 2), (0, 3), (0, 4)]
        else:
            front_positions = [(4, 1), (4, 2), (4, 3), (4, 4)]
            back_positions = [(5, 1), (5, 2), (5, 3), (5, 4)]

        front_pieces = ["good", "good", "good", "bad"]
        random.shuffle(front_pieces)

        back_pieces = ["good", "bad", "bad", "bad"]
        random.shuffle(back_pieces)

        for pos, piece_type in zip(front_positions, front_pieces):
            placement[pos] = piece_type
        for pos, piece_type in zip(back_positions, back_pieces):
            placement[pos] = piece_type

        return placement

    def get_name(self) -> str:
        return "æ”»æ’ƒçš„é…ç½®"


class DefensivePlacement(PlacementStrategy):
    """é˜²å¾¡çš„é…ç½®æˆ¦ç•¥ï¼ˆå–„ç‰ã‚’å¾Œæ–¹ã«ï¼‰"""

    def get_placement(self, player_id: str) -> Dict[Tuple[int, int], str]:
        placement = {}

        if player_id == "A":
            front_positions = [(1, 1), (1, 2), (1, 3), (1, 4)]
            back_positions = [(0, 1), (0, 2), (0, 3), (0, 4)]
        else:
            front_positions = [(4, 1), (4, 2), (4, 3), (4, 4)]
            back_positions = [(5, 1), (5, 2), (5, 3), (5, 4)]

        front_pieces = ["bad", "bad", "bad", "good"]
        random.shuffle(front_pieces)

        back_pieces = ["bad", "good", "good", "good"]
        random.shuffle(back_pieces)

        for pos, piece_type in zip(front_positions, front_pieces):
            placement[pos] = piece_type
        for pos, piece_type in zip(back_positions, back_pieces):
            placement[pos] = piece_type

        return placement

    def get_name(self) -> str:
        return "é˜²å¾¡çš„é…ç½®"


# ================================================================================
# Part 5: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«2 - æ•µé§’æ¨å®šå™¨ï¼ˆå­¦ç¿’æ©Ÿèƒ½ä»˜ãï¼‰
# ================================================================================


class PieceEstimator(ABC):
    """æ•µé§’æ¨å®šå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    def estimate(
        self,
        board: np.ndarray,
        enemy_positions: List[Tuple[int, int]],
        player: str,
        my_pieces: Dict[Tuple[int, int], str],
        turn: int,
    ) -> Dict[Tuple[int, int], Dict[str, float]]:
        """æ•µé§’ã‚’æ¨å®š"""
        pass

    @abstractmethod
    def get_name(self) -> str:
        """æ¨å®šå™¨åã‚’å–å¾—"""
        pass


class CQCNNEstimator(PieceEstimator):
    """CQCNNæ¨å®šå™¨ï¼ˆå­¦ç¿’æ©Ÿèƒ½ä»˜ãï¼‰"""

    def __init__(self, n_qubits: int = 6, n_layers: int = 3):
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.model = CQCNNModel(n_qubits, n_layers)
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
        self.training_history = []
        self.is_trained = False
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

        # å¼·åŒ–å­¦ç¿’ç”¨
        self.memory = deque(maxlen=10000)
        self.target_model = CQCNNModel(n_qubits, n_layers).to(self.device)
        self.update_target_model()

    def train_supervised(self, training_data: List[Dict], config: LearningConfig) -> None:
        """æ•™å¸«ã‚ã‚Šå­¦ç¿’"""
        print(f"ğŸ“š CQCNNæ•™å¸«ã‚ã‚Šå­¦ç¿’é–‹å§‹: {len(training_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        print(f"ğŸ”§ è¨­å®š: ãƒãƒƒãƒã‚µã‚¤ã‚º={config.batch_size}, å­¦ç¿’ç‡={config.learning_rate}")
        print(f"ğŸ’» ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        print("=" * 60)

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        n_val = int(len(training_data) * config.validation_split)
        val_data = training_data[:n_val]
        train_data = training_data[n_val:]

        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: å­¦ç¿’ç”¨={len(train_data)}ä»¶, æ¤œè¨¼ç”¨={len(val_data)}ä»¶")
        print("=" * 60)

        self.model.train()
        best_val_acc = 0.0
        patience_counter = 0

        for epoch in range(config.supervised_epochs):
            random.shuffle(train_data)

            total_loss = 0
            correct = 0
            total = 0
            batch_count = 0

            n_batches = len(train_data) // config.batch_size
            print(f"\nEpoch {epoch + 1}/{config.supervised_epochs}")
            print("ğŸ”„ å­¦ç¿’ä¸­: ", end="", flush=True)

            for i in range(0, len(train_data), config.batch_size):
                batch = train_data[i : i + config.batch_size]

                if batch_count % max(1, n_batches // 20) == 0:
                    print("â–ˆ", end="", flush=True)

                inputs, labels = self._prepare_supervised_batch(batch)
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                batch_count += 1

            print(" âœ…")

            train_acc = 100 * correct / total if total > 0 else 0.0
            avg_batches = max(1, len(train_data) // config.batch_size)
            train_loss = total_loss / avg_batches

            print("ğŸ” æ¤œè¨¼ä¸­...", end="", flush=True)
            val_acc, val_loss = self._validate_supervised(val_data, config.batch_size)
            print(" âœ…")

            self.training_history.append(
                {
                    "epoch": epoch,
                    "train_loss": train_loss,
                    "train_acc": train_acc,
                    "val_loss": val_loss,
                    "val_acc": val_acc,
                }
            )

            improvement = "ğŸ“ˆ" if val_acc > best_val_acc else "ğŸ“‰" if val_acc < best_val_acc else "â¡ï¸"
            print(
                f"ğŸ“Š çµæœ: Train Loss={train_loss:.4f} | Train Acc={train_acc:.1f}% | "
                f"Val Loss={val_loss:.4f} | Val Acc={val_acc:.1f}% {improvement}"
            )

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                print("ğŸ¯ æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼")
                torch.save(self.model.state_dict(), "models/best_cqcnn_supervised.pth")
            else:
                patience_counter += 1
                if patience_counter >= 10:
                    print(f"â¹ï¸  Early Stopping: {patience_counter}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
                    break

            if epoch > 0 and epoch % 30 == 0:
                for param_group in self.optimizer.param_groups:
                    param_group["lr"] *= 0.8
                print(f"ğŸ“‰ å­¦ç¿’ç‡èª¿æ•´: {param_group['lr']:.6f}")

            print("-" * 60)

        self.is_trained = True
        print(f"\nğŸ‰ æ•™å¸«ã‚ã‚Šå­¦ç¿’å®Œäº†! ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.1f}%")
        print("=" * 60)

    def _prepare_supervised_batch(self, batch: List[Dict]) -> Tuple[torch.Tensor, torch.Tensor]:
        """æ•™å¸«ã‚ã‚Šå­¦ç¿’ç”¨ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        inputs = []
        labels = []

        for sample in batch:
            tensor = DataProcessor.prepare_7channel_tensor(
                board=sample["board"], player=sample["player"], my_pieces=sample["my_pieces"], turn=sample["turn"]
            )
            inputs.append(tensor.squeeze(0))

            if sample["true_labels"]:
                first_enemy_type = list(sample["true_labels"].values())[0]
                label = 0 if first_enemy_type == "good" else 1
            else:
                label = 0
            labels.append(label)

        batch_tensor = torch.stack(inputs)
        label_tensor = torch.LongTensor(labels)

        return batch_tensor, label_tensor

    def _validate_supervised(self, val_data: List[Dict], batch_size: int) -> Tuple[float, float]:
        """æ•™å¸«ã‚ã‚Šå­¦ç¿’ã®æ¤œè¨¼"""
        if not val_data:
            return 0.0, 0.0

        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for i in range(0, len(val_data), batch_size):
                batch = val_data[i : i + batch_size]
                if not batch:
                    continue

                inputs, labels = self._prepare_supervised_batch(batch)
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        self.model.train()

        accuracy = 100 * correct / total if total > 0 else 0.0
        avg_loss = total_loss / max(1, len(val_data) // batch_size)

        return accuracy, avg_loss

    def estimate(
        self, board: np.ndarray, enemy_positions: List[Tuple[int, int]], player: str, my_pieces: Dict, turn: int
    ) -> Dict:
        """æ•µé§’æ¨å®š"""
        if not self.is_trained:
            # æœªå­¦ç¿’ã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ æ¨å®š
            results = {}
            for pos in enemy_positions:
                good_prob = random.uniform(0.3, 0.7)
                results[pos] = {"good_prob": good_prob, "bad_prob": 1 - good_prob, "confidence": 0.5}
            return results

        self.model.eval()
        results = {}

        tensor = DataProcessor.prepare_7channel_tensor(board, player, my_pieces, turn)
        tensor = tensor.to(self.device)

        with torch.no_grad():
            output = self.model(tensor)
            probs = F.softmax(output, dim=1)

            for pos in enemy_positions:
                results[pos] = {
                    "good_prob": probs[0, 0].item(),
                    "bad_prob": probs[0, 1].item(),
                    "confidence": max(probs[0].tolist()),
                }

        return results

    def update_target_model(self):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°"""
        self.target_model.load_state_dict(self.model.state_dict())

    def save_model(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        torch.save(
            {
                "model_state_dict": self.model.state_dict(),
                "optimizer_state_dict": self.optimizer.state_dict(),
                "training_history": self.training_history,
                "is_trained": self.is_trained,
                "n_qubits": self.n_qubits,
                "n_layers": self.n_layers,
            },
            path,
        )
        print(f"ğŸ’¾ CQCNNãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {path}")

    def load_model(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«èª­è¾¼"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.training_history = checkpoint["training_history"]
        self.is_trained = checkpoint["is_trained"]
        print(f"ğŸ“ CQCNNãƒ¢ãƒ‡ãƒ«èª­è¾¼å®Œäº†: {path}")

    def get_name(self) -> str:
        status = "å­¦ç¿’æ¸ˆã¿" if self.is_trained else "æœªå­¦ç¿’"
        return f"CQCNN({self.n_qubits}q,{self.n_layers}L)-{status}"


class SimpleCNNEstimator(PieceEstimator):
    """ã‚·ãƒ³ãƒ—ãƒ«CNNæ¨å®šå™¨"""

    def __init__(self):
        self.model = nn.Sequential(
            nn.Conv2d(7, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32 * 6 * 6, 64),
            nn.ReLU(),
            nn.Linear(64, 2),
        )
        self.is_trained = False

    def estimate(
        self, board: np.ndarray, enemy_positions: List[Tuple[int, int]], player: str, my_pieces: Dict, turn: int
    ) -> Dict:
        results = {}
        for pos in enemy_positions:
            good_prob = random.uniform(0.4, 0.6)
            results[pos] = {"good_prob": good_prob, "bad_prob": 1 - good_prob, "confidence": 0.6}
        return results

    def get_name(self) -> str:
        return "SimpleCNN"


class RandomEstimator(PieceEstimator):
    """ãƒ©ãƒ³ãƒ€ãƒ æ¨å®šå™¨"""

    def estimate(
        self, board: np.ndarray, enemy_positions: List[Tuple[int, int]], player: str, my_pieces: Dict, turn: int
    ) -> Dict:
        results = {}
        for pos in enemy_positions:
            good_prob = random.uniform(0.3, 0.7)
            results[pos] = {"good_prob": good_prob, "bad_prob": 1 - good_prob, "confidence": 0.5}
        return results

    def get_name(self) -> str:
        return "ãƒ©ãƒ³ãƒ€ãƒ æ¨å®š"


# ================================================================================
# Part 6: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«3 - å ±é…¬é–¢æ•°
# ================================================================================


class RewardFunction(ABC):
    """å ±é…¬é–¢æ•°ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    def calculate_move_reward(self, game_state: GameState, move: Tuple, player: str, my_pieces: Dict) -> float:
        """æ‰‹ã®å ±é…¬ã‚’è¨ˆç®—"""
        pass

    @abstractmethod
    def get_name(self) -> str:
        """å ±é…¬é–¢æ•°åã‚’å–å¾—"""
        pass


class StandardRewardFunction(RewardFunction):
    """æ¨™æº–å ±é…¬é–¢æ•°"""

    def calculate_move_reward(self, game_state: GameState, move: Tuple, player: str, my_pieces: Dict) -> float:
        from_pos, to_pos = move
        reward = 0.0

        # åŸºæœ¬ç§»å‹•å ±é…¬
        piece_type = my_pieces.get(from_pos, "unknown")
        if piece_type == "good":
            reward += 1.0
        else:
            reward += 0.5

        # è„±å‡ºå ±é…¬
        escape_positions = [(5, 0), (5, 5)] if player == "A" else [(0, 0), (0, 5)]
        if to_pos in escape_positions:
            if piece_type == "good":
                reward += 50.0
            else:
                reward -= 10.0

        return reward

    def get_name(self) -> str:
        return "æ¨™æº–å ±é…¬"


class AggressiveRewardFunction(RewardFunction):
    """æ”»æ’ƒçš„å ±é…¬é–¢æ•°"""

    def calculate_move_reward(self, game_state: GameState, move: Tuple, player: str, my_pieces: Dict) -> float:
        from_pos, to_pos = move
        reward = StandardRewardFunction().calculate_move_reward(game_state, move, player, my_pieces)

        # å‰é€²ãƒœãƒ¼ãƒŠã‚¹
        if player == "A" and to_pos[0] > from_pos[0]:
            reward += 2.0
        elif player == "B" and to_pos[0] < from_pos[0]:
            reward += 2.0

        return reward

    def get_name(self) -> str:
        return "æ”»æ’ƒçš„å ±é…¬"


class DefensiveRewardFunction(RewardFunction):
    """é˜²å¾¡çš„å ±é…¬é–¢æ•°"""

    def calculate_move_reward(self, game_state: GameState, move: Tuple, player: str, my_pieces: Dict) -> float:
        from_pos, to_pos = move
        reward = StandardRewardFunction().calculate_move_reward(game_state, move, player, my_pieces)

        # å®‰å…¨æ€§ãƒœãƒ¼ãƒŠã‚¹
        safety_score = self._calculate_safety(to_pos, game_state, player)
        reward += safety_score * 0.5

        return reward

    def _calculate_safety(self, pos: Tuple, game_state: GameState, player: str) -> float:
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val

        min_enemy_dist = 10
        for i in range(6):
            for j in range(6):
                if game_state.board[i, j] == enemy_val:
                    dist = abs(pos[0] - i) + abs(pos[1] - j)
                    min_enemy_dist = min(min_enemy_dist, dist)

        return min_enemy_dist

    def get_name(self) -> str:
        return "é˜²å¾¡çš„å ±é…¬"


# ================================================================================
# Part 7: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«4 - Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨
# ================================================================================


class QMapGenerator(ABC):
    """Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    def generate(
        self,
        board: np.ndarray,
        estimations: Dict,
        my_pieces: Dict,
        player: str,
        reward_function: RewardFunction = None,
        game_state: GameState = None,
    ) -> np.ndarray:
        """Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
        pass

    @abstractmethod
    def get_name(self) -> str:
        """ç”Ÿæˆå™¨åã‚’å–å¾—"""
        pass


class SimpleQMapGenerator(QMapGenerator):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªQå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""

    def generate(
        self,
        board: np.ndarray,
        estimations: Dict,
        my_pieces: Dict,
        player: str,
        reward_function: RewardFunction = None,
        game_state: GameState = None,
    ) -> np.ndarray:
        q_map = np.zeros((6, 6, 4))

        for pos, piece_type in my_pieces.items():
            base_value = 1.0 if piece_type == "good" else 0.5

            for i, (dx, dy) in enumerate([(0, 1), (0, -1), (1, 0), (-1, 0)]):
                new_pos = (pos[0] + dx, pos[1] + dy)

                if not (0 <= new_pos[0] < 6 and 0 <= new_pos[1] < 6):
                    q_map[pos[0], pos[1], i] = -100
                    continue

                q_value = base_value

                # æ¨å®šçµæœã‚’ä½¿ç”¨
                if new_pos in estimations:
                    est = estimations[new_pos]
                    if piece_type == "bad":
                        q_value += est["good_prob"] * 2.0
                        q_value += est["bad_prob"] * 1.0
                    else:
                        q_value -= est["bad_prob"] * 0.5

                # å ±é…¬é–¢æ•°ã‚’é©ç”¨
                if reward_function and game_state:
                    move = (pos, new_pos)
                    reward = reward_function.calculate_move_reward(game_state, move, player, my_pieces)
                    q_value += reward * 0.1

                q_map[pos[0], pos[1], i] = q_value

        return q_map

    def get_name(self) -> str:
        return "ã‚·ãƒ³ãƒ—ãƒ«Qå€¤"


class StrategicQMapGenerator(QMapGenerator):
    """æˆ¦ç•¥çš„Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""

    def generate(
        self,
        board: np.ndarray,
        estimations: Dict,
        my_pieces: Dict,
        player: str,
        reward_function: RewardFunction = None,
        game_state: GameState = None,
    ) -> np.ndarray:
        # SimpleQMapGeneratorã‚’ãƒ™ãƒ¼ã‚¹ã«æˆ¦ç•¥çš„è¦ç´ ã‚’è¿½åŠ 
        q_map = SimpleQMapGenerator().generate(board, estimations, my_pieces, player, reward_function, game_state)

        # è„±å‡ºä½ç½®ã®æˆ¦ç•¥çš„ä¾¡å€¤ã‚’è¿½åŠ 
        escape_positions = [(5, 0), (5, 5)] if player == "A" else [(0, 0), (0, 5)]

        for pos, piece_type in my_pieces.items():
            for i, (dx, dy) in enumerate([(0, 1), (0, -1), (1, 0), (-1, 0)]):
                new_pos = (pos[0] + dx, pos[1] + dy)

                if new_pos in escape_positions and piece_type == "good":
                    q_map[pos[0], pos[1], i] += 10.0  # è„±å‡ºãƒœãƒ¼ãƒŠã‚¹

        return q_map

    def get_name(self) -> str:
        return "æˆ¦ç•¥çš„Qå€¤"


# ================================================================================
# Part 8: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«5 - è¡Œå‹•é¸æŠå™¨
# ================================================================================


class ActionSelector(ABC):
    """è¡Œå‹•é¸æŠå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    def select(self, q_map: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        """è¡Œå‹•ã‚’é¸æŠ"""
        pass

    @abstractmethod
    def get_name(self) -> str:
        """é¸æŠå™¨åã‚’å–å¾—"""
        pass


class GreedySelector(ActionSelector):
    """è²ªæ¬²é¸æŠå™¨"""

    def select(self, q_map: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        if not legal_moves:
            return None

        best_move = None
        best_value = -float("inf")

        for from_pos, to_pos in legal_moves:
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]

            dir_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3}
            if (dx, dy) in dir_map:
                dir_idx = dir_map[(dx, dy)]
                value = q_map[from_pos[0], from_pos[1], dir_idx]

                if value > best_value:
                    best_value = value
                    best_move = (from_pos, to_pos)

        return best_move or random.choice(legal_moves)

    def get_name(self) -> str:
        return "è²ªæ¬²é¸æŠ"


class EpsilonGreedySelector(ActionSelector):
    """Îµ-è²ªæ¬²é¸æŠå™¨"""

    def __init__(self, epsilon: float = 0.1):
        self.epsilon = epsilon

    def select(self, q_map: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        if not legal_moves:
            return None

        if random.random() < self.epsilon:
            return random.choice(legal_moves)

        return GreedySelector().select(q_map, legal_moves)

    def get_name(self) -> str:
        return f"Îµè²ªæ¬²(Îµ={self.epsilon})"


class SoftmaxSelector(ActionSelector):
    """ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é¸æŠå™¨"""

    def __init__(self, temperature: float = 1.0):
        self.temperature = temperature

    def select(self, q_map: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        if not legal_moves:
            return None

        q_values = []
        dir_map = {(0, 1): 0, (0, -1): 1, (1, 0): 2, (-1, 0): 3}

        for from_pos, to_pos in legal_moves:
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]

            if (dx, dy) in dir_map:
                dir_idx = dir_map[(dx, dy)]
                value = q_map[from_pos[0], from_pos[1], dir_idx]
            else:
                value = -100

            q_values.append(value / self.temperature)

        q_tensor = torch.tensor(q_values, dtype=torch.float32)
        probs = F.softmax(q_tensor, dim=0).numpy()

        idx = np.random.choice(len(legal_moves), p=probs)
        return legal_moves[idx]

    def get_name(self) -> str:
        return f"ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹(T={self.temperature})"


# ================================================================================
# Part 9: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±åˆ
# ================================================================================


@dataclass
class ModuleConfig:
    """5ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ±åˆã™ã‚‹è¨­å®š"""

    placement_strategy: PlacementStrategy
    piece_estimator: PieceEstimator
    reward_function: RewardFunction
    qmap_generator: QMapGenerator
    action_selector: ActionSelector


class ModularAgent:
    """5ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self, player_id: str, config: ModuleConfig):
        self.player_id = player_id
        self.config = config
        self.name = self._generate_name()
        self.piece_info = {}
        self.game_history = []

    def _generate_name(self) -> str:
        return (
            f"Agent_{self.player_id}["
            f"{self.config.placement_strategy.get_name()[:3]}+"
            f"{self.config.piece_estimator.get_name()[:6]}+"
            f"{self.config.reward_function.get_name()[:3]}+"
            f"{self.config.qmap_generator.get_name()[:3]}+"
            f"{self.config.action_selector.get_name()[:3]}]"
        )

    def get_initial_placement(self) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’å–å¾—"""
        placement = self.config.placement_strategy.get_placement(self.player_id)
        self.piece_info = placement.copy()
        return placement

    def get_move(self, game_state: GameState, legal_moves: List[Tuple]) -> Tuple:
        """æ¬¡ã®æ‰‹ã‚’å–å¾—"""
        if not legal_moves:
            return None

        try:
            # è‡ªåˆ†ã®é§’æƒ…å ±ã‚’æ›´æ–°
            self._update_piece_info(game_state)

            # 1. æ•µé§’ä½ç½®ã‚’ç‰¹å®š
            enemy_positions = self._find_enemy_positions(game_state)

            # 2. æ•µé§’ã‚’æ¨å®šï¼ˆ7ãƒãƒ£ãƒ³ãƒãƒ«å…¥åŠ›ï¼‰
            estimations = {}
            if enemy_positions:
                estimations = self.config.piece_estimator.estimate(
                    board=game_state.board,
                    enemy_positions=enemy_positions,
                    player=self.player_id,
                    my_pieces=self.piece_info,
                    turn=game_state.turn,
                )

            # 3. Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆï¼ˆå ±é…¬é–¢æ•°ã‚‚çµ±åˆï¼‰
            q_map = self.config.qmap_generator.generate(
                board=game_state.board,
                estimations=estimations,
                my_pieces=self.piece_info,
                player=self.player_id,
                reward_function=self.config.reward_function,
                game_state=game_state,
            )

            # 4. è¡Œå‹•ã‚’é¸æŠ
            selected_move = self.config.action_selector.select(q_map, legal_moves)

            # å±¥æ­´ã«è¨˜éŒ²
            self.game_history.append(
                {
                    "turn": game_state.turn,
                    "move": selected_move,
                    "estimations": len(estimations),
                    "q_max": np.max(q_map) if q_map is not None else 0,
                }
            )

            return selected_move

        except Exception as e:
            print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ in {self.name}: {e}")
            return random.choice(legal_moves)

    def _update_piece_info(self, game_state: GameState):
        """é§’ã‚¿ã‚¤ãƒ—æƒ…å ±ã‚’æ›´æ–°"""
        if self.player_id == "A":
            current_pieces = game_state.player_a_pieces
        else:
            current_pieces = game_state.player_b_pieces

        # ç¾åœ¨ã®é§’ä½ç½®ã«åˆã‚ã›ã¦æ›´æ–°ï¼ˆç°¡ç•¥åŒ–ï¼‰
        new_piece_info = {}
        for pos in current_pieces.keys():
            # å…ƒã®é§’ã‚¿ã‚¤ãƒ—ã‚’ä¿æŒï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šè¤‡é›‘ãªè¿½è·¡ãŒå¿…è¦ï¼‰
            if pos in self.piece_info:
                new_piece_info[pos] = self.piece_info[pos]
            else:
                # æ–°ã—ã„ä½ç½®ã®å ´åˆã€é©å½“ãªå€¤ã‚’è¨­å®š
                new_piece_info[pos] = random.choice(["good", "bad"])

        self.piece_info = new_piece_info

    def _find_enemy_positions(self, game_state: GameState) -> List[Tuple[int, int]]:
        """æ•µé§’ã®ä½ç½®ã‚’ç‰¹å®š"""
        enemy_val = -1 if self.player_id == "A" else 1
        positions = []

        for i in range(6):
            for j in range(6):
                if game_state.board[i, j] == enemy_val:
                    positions.append((i, j))

        return positions


# ================================================================================
# Part 10: ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================


def create_enhanced_training_data(n_samples: int) -> List[Dict]:
    """é«˜å“è³ªãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    print(f"ğŸ”§ é«˜å“è³ªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­: {n_samples}ã‚µãƒ³ãƒ—ãƒ«")
    data = []

    strategies = ["aggressive", "defensive", "balanced", "random"]

    for i in range(n_samples):
        if i % (n_samples // 10) == 0:
            progress = (i / n_samples) * 100
            print(f"ğŸ“Š é€²æ—: {progress:.0f}% ({i}/{n_samples})")

        strategy = random.choice(strategies)

        # ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ä½œæˆ
        board = np.zeros((6, 6), dtype=int)

        # æˆ¦ç•¥ã«å¿œã˜ãŸé…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³
        if strategy == "aggressive":
            player_a_positions = [(0, j) for j in range(1, 5)] + [(1, j) for j in range(2, 4)]
            player_b_positions = [(4, j) for j in range(2, 4)] + [(5, j) for j in range(1, 5)]
        elif strategy == "defensive":
            player_a_positions = [(1, j) for j in range(1, 5)] + [(0, j) for j in range(2, 4)]
            player_b_positions = [(4, j) for j in range(1, 5)] + [(5, j) for j in range(2, 4)]
        elif strategy == "balanced":
            player_a_positions = [(0, 1), (0, 4), (1, 2), (1, 3)] + [(0, 2), (0, 3), (1, 1), (1, 4)]
            player_b_positions = [(4, 1), (4, 4), (5, 2), (5, 3)] + [(4, 2), (4, 3), (5, 1), (5, 4)]
        else:  # random
            available_a = [(r, c) for r in range(2) for c in range(1, 5)]
            available_b = [(r, c) for r in range(4, 6) for c in range(1, 5)]
            player_a_positions = random.sample(available_a, min(8, len(available_a)))
            player_b_positions = random.sample(available_b, min(8, len(available_b)))

        # å®Ÿéš›ã«é…ç½®ã™ã‚‹é§’æ•°ï¼ˆ4-8å€‹ï¼‰
        n_pieces_a = random.randint(4, min(8, len(player_a_positions)))
        n_pieces_b = random.randint(4, min(8, len(player_b_positions)))

        selected_a = random.sample(player_a_positions, n_pieces_a)
        selected_b = random.sample(player_b_positions, n_pieces_b)

        # ãƒœãƒ¼ãƒ‰ã«é…ç½®
        for pos in selected_a:
            board[pos] = 1
        for pos in selected_b:
            board[pos] = 2

        # æˆ¦ç•¥ã«å¿œã˜ãŸé§’ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
        if strategy == "aggressive":
            good_ratio_a = random.uniform(0.6, 0.8)
            good_ratio_b = random.uniform(0.6, 0.8)
        elif strategy == "defensive":
            good_ratio_a = random.uniform(0.2, 0.4)
            good_ratio_b = random.uniform(0.2, 0.4)
        else:
            good_ratio_a = random.uniform(0.4, 0.6)
            good_ratio_b = random.uniform(0.4, 0.6)

        # è‡ªåˆ†ã®é§’æƒ…å ±ï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Aï¼‰
        my_pieces = {}
        n_good_a = int(n_pieces_a * good_ratio_a)
        piece_types_a = ["good"] * n_good_a + ["bad"] * (n_pieces_a - n_good_a)
        random.shuffle(piece_types_a)

        for idx, pos in enumerate(selected_a):
            my_pieces[pos] = piece_types_a[idx]

        # æ•µé§’ã®çœŸã®ã‚¿ã‚¤ãƒ—ï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Bï¼‰
        true_labels = {}
        n_good_b = int(n_pieces_b * good_ratio_b)
        piece_types_b = ["good"] * n_good_b + ["bad"] * (n_pieces_b - n_good_b)
        random.shuffle(piece_types_b)

        for idx, pos in enumerate(selected_b):
            true_labels[pos] = piece_types_b[idx]

        turn = random.randint(1, 80)

        # ãƒã‚¤ã‚ºè¿½åŠ ï¼ˆå®Ÿæˆ¦çš„ãªä¸ç¢ºå®Ÿæ€§ï¼‰
        if random.random() < 0.1:  # 10%ã®ç¢ºç‡ã§ãƒã‚¤ã‚º
            if len(true_labels) > 2:
                unknown_pos = random.choice(list(true_labels.keys()))
                true_labels[unknown_pos] = random.choice(["good", "bad"])

        sample = {
            "board": board.copy(),
            "player": "A",
            "my_pieces": my_pieces.copy(),
            "turn": turn,
            "enemy_positions": list(true_labels.keys()),
            "true_labels": true_labels.copy(),
            "strategy": strategy,
            "difficulty": random.choice(["easy", "medium", "hard"]),
        }
        data.append(sample)

    print("âœ… é«˜å“è³ªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")

    # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆè¡¨ç¤º
    strategies_count = {}
    for sample in data:
        strat = sample["strategy"]
        strategies_count[strat] = strategies_count.get(strat, 0) + 1

    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    for strat, count in strategies_count.items():
        print(f"   - {strat}: {count}ä»¶ ({count / n_samples * 100:.1f}%)")

    return data


# ================================================================================
# Part 11: ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ»å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ """
    print("ğŸ¯ CQCNNå¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ  v3.0")
    print("=" * 60)
    print("ğŸ§  Classical-Quantum CNN ã«ã‚ˆã‚‹æ•µé§’æ¨å®šå­¦ç¿’")
    print("ğŸ¤– 5ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
    print("ğŸ“š æ•™å¸«ã‚ã‚Šå­¦ç¿’ & ğŸ® å¼·åŒ–å­¦ç¿’ å¯¾å¿œ")
    print("=" * 60)

    print("\nãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ:")
    print("1. ğŸ“ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰")
    print("2. âš”ï¸  å¯¾æˆ¦ãƒ¢ãƒ¼ãƒ‰")
    print("3. ğŸ”¬ å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ï¼ˆå­¦ç¿’ â†’ å¯¾æˆ¦ï¼‰")

    choice = input("\nğŸ‘‰ é¸æŠ (1-3): ")

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs("models", exist_ok=True)

    if choice == "1":
        learning_mode()
    elif choice == "2":
        battle_mode()
    elif choice == "3":
        experiment_mode()
    else:
        print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")


def learning_mode():
    """å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰"""
    print("\n" + "=" * 60)
    print("ğŸ“ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰")
    print("=" * 60)

    config = LearningConfig()
    print("âš™ï¸  å­¦ç¿’è¨­å®š:")
    print(f"   - ãƒ‡ãƒã‚¤ã‚¹: {'GPU' if config.device == 'cuda' else 'CPU'}")
    print(f"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}")
    print(f"   - å­¦ç¿’ç‡: {config.learning_rate}")
    print(f"   - æ•™å¸«ã‚ã‚Š: {config.supervised_epochs}ã‚¨ãƒãƒƒã‚¯")
    print("=" * 60)

    print("\nğŸ² å­¦ç¿’æ–¹æ³•ã‚’é¸æŠ:")
    print("1. ğŸ“š æ•™å¸«ã‚ã‚Šå­¦ç¿’")
    print("2. ğŸ”„ ç¶™ç¶šå­¦ç¿’ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å†é–‹ï¼‰")

    learn_choice = input("\nğŸ‘‰ é¸æŠ (1-2): ")

    # CQCNNEstimatorã‚’ä½œæˆ
    cqcnn_estimator = CQCNNEstimator(n_qubits=6, n_layers=3)

    if learn_choice == "2":
        # ç¶™ç¶šå­¦ç¿’
        print("\nğŸ“ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        try:
            cqcnn_estimator.load_model("models/cqcnn_supervised.pth")
            print("âœ… æ•™å¸«ã‚ã‚Šå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except FileNotFoundError:
            print("âš ï¸ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    # æ•™å¸«ã‚ã‚Šå­¦ç¿’
    print("\n" + "=" * 60)
    print("ğŸ“š PHASE 1: æ•™å¸«ã‚ã‚Šå­¦ç¿’")
    print("=" * 60)

    print("ğŸ”„ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    training_data = create_enhanced_training_data(2000)
    print(f"âœ… {len(training_data)}ä»¶ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆå®Œäº†")

    cqcnn_estimator.train_supervised(training_data, config)
    cqcnn_estimator.save_model("models/cqcnn_supervised.pth")

    print("\nğŸ‰ å­¦ç¿’å®Œäº†!")


def battle_mode():
    """å¯¾æˆ¦ãƒ¢ãƒ¼ãƒ‰"""
    print("\n" + "=" * 60)
    print("âš”ï¸ å¯¾æˆ¦ãƒ¢ãƒ¼ãƒ‰")
    print("=" * 60)

    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    modules = {
        "placement": [StandardPlacement(), AggressivePlacement(), DefensivePlacement()],
        "estimator": [
            RandomEstimator(),
            SimpleCNNEstimator(),
        ],
        "reward": [StandardRewardFunction(), AggressiveRewardFunction(), DefensiveRewardFunction()],
        "qmap": [SimpleQMapGenerator(), StrategicQMapGenerator()],
        "selector": [
            GreedySelector(),
            EpsilonGreedySelector(epsilon=0.1),
            EpsilonGreedySelector(epsilon=0.3),
            SoftmaxSelector(temperature=1.0),
        ],
    }

    # å­¦ç¿’æ¸ˆã¿CQCNNã‚’è¿½åŠ 
    try:
        trained_cqcnn = CQCNNEstimator(n_qubits=6, n_layers=3)
        trained_cqcnn.load_model("models/cqcnn_supervised.pth")
        modules["estimator"].append(trained_cqcnn)
        print("âœ… å­¦ç¿’æ¸ˆã¿CQCNNã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except FileNotFoundError:
        print("âš ï¸ å­¦ç¿’æ¸ˆã¿CQCNNãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    print("\nğŸ¯ å¯¾æˆ¦è¨­å®šã‚’é¸æŠ:")
    print("1. ãƒãƒ©ãƒ³ã‚¹å‹ vs ãƒãƒ©ãƒ³ã‚¹å‹")
    print("2. æ”»æ’ƒå‹ vs é˜²å¾¡å‹")
    print("3. CQCNN vs ãƒ©ãƒ³ãƒ€ãƒ ")
    print("4. ã‚«ã‚¹ã‚¿ãƒ è¨­å®š")

    battle_choice = input("\nğŸ‘‰ é¸æŠ (1-4): ")

    if battle_choice == "1":
        # ãƒãƒ©ãƒ³ã‚¹å‹
        config1 = ModuleConfig(
            placement_strategy=StandardPlacement(),
            piece_estimator=modules["estimator"][-1] if len(modules["estimator"]) > 2 else SimpleCNNEstimator(),
            reward_function=StandardRewardFunction(),
            qmap_generator=StrategicQMapGenerator(),
            action_selector=EpsilonGreedySelector(epsilon=0.1),
        )
        config2 = ModuleConfig(
            placement_strategy=StandardPlacement(),
            piece_estimator=SimpleCNNEstimator(),
            reward_function=StandardRewardFunction(),
            qmap_generator=SimpleQMapGenerator(),
            action_selector=GreedySelector(),
        )

    elif battle_choice == "2":
        # æ”»æ’ƒå‹ vs é˜²å¾¡å‹
        config1 = ModuleConfig(
            placement_strategy=AggressivePlacement(),
            piece_estimator=modules["estimator"][-1] if len(modules["estimator"]) > 2 else RandomEstimator(),
            reward_function=AggressiveRewardFunction(),
            qmap_generator=SimpleQMapGenerator(),
            action_selector=GreedySelector(),
        )
        config2 = ModuleConfig(
            placement_strategy=DefensivePlacement(),
            piece_estimator=SimpleCNNEstimator(),
            reward_function=DefensiveRewardFunction(),
            qmap_generator=StrategicQMapGenerator(),
            action_selector=SoftmaxSelector(temperature=1.0),
        )

    elif battle_choice == "3":
        # CQCNN vs ãƒ©ãƒ³ãƒ€ãƒ 
        config1 = ModuleConfig(
            placement_strategy=StandardPlacement(),
            piece_estimator=modules["estimator"][-1] if len(modules["estimator"]) > 2 else SimpleCNNEstimator(),
            reward_function=StandardRewardFunction(),
            qmap_generator=StrategicQMapGenerator(),
            action_selector=EpsilonGreedySelector(epsilon=0.1),
        )
        config2 = ModuleConfig(
            placement_strategy=StandardPlacement(),
            piece_estimator=RandomEstimator(),
            reward_function=StandardRewardFunction(),
            qmap_generator=SimpleQMapGenerator(),
            action_selector=EpsilonGreedySelector(epsilon=0.3),
        )

    else:
        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šï¼ˆç°¡ç•¥åŒ–ï¼‰
        print("ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã¯ã¾ã å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
    agent1 = ModularAgent("A", config1)
    agent2 = ModularAgent("B", config2)

    print("\nğŸ® å¯¾æˆ¦è¨­å®šå®Œäº†:")
    print(f"Agent1: {agent1.name}")
    print(f"Agent2: {agent2.name}")

    # å¯¾æˆ¦å®Ÿè¡Œï¼ˆç°¡ç•¥åŒ–ç‰ˆï¼‰
    n_games = int(input("\nå¯¾æˆ¦æ•°: "))

    wins = {"A": 0, "B": 0, "Draw": 0}

    for i in range(n_games):
        print(f"\nGame {i + 1}/{n_games}")
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚²ãƒ¼ãƒ å®Ÿè¡Œï¼ˆå®Ÿéš›ã«ã¯GameEngineãŒå¿…è¦ï¼‰
        winner = random.choice(["A", "B", "Draw"])
        wins[winner] += 1
        print(f"å‹è€…: {winner}")

    # çµæœè¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€çµ‚çµæœ:")
    print(f"Agent1å‹åˆ©: {wins['A']} ({wins['A'] / n_games * 100:.1f}%)")
    print(f"Agent2å‹åˆ©: {wins['B']} ({wins['B'] / n_games * 100:.1f}%)")
    print(f"å¼•ãåˆ†ã‘: {wins['Draw']} ({wins['Draw'] / n_games * 100:.1f}%)")
    print("=" * 60)


def experiment_mode():
    """å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ï¼ˆå­¦ç¿’ â†’ å¯¾æˆ¦ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰")
    print("=" * 60)

    print("ğŸ”„ å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã‚’å®Ÿè¡Œä¸­...")
    learning_mode()

    print("\nğŸ”„ å¯¾æˆ¦ãƒ•ã‚§ãƒ¼ã‚ºã«ç§»è¡Œ...")
    battle_mode()


if __name__ == "__main__":
    main()
