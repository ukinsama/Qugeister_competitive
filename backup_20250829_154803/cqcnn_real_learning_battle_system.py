#!/usr/bin/env python3
"""
å®Ÿéš›ã«å­¦ç¿’ã‚’è¡Œã†ãƒ­ãƒ¼ã‚«ãƒ«å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 
æ—¢å­˜ã®CQCNNå­¦ç¿’æ©Ÿèƒ½ã‚’çµ±åˆã—ã¦ã€æœ¬æ ¼çš„ãªå­¦ç¿’â†’è©•ä¾¡â†’ä¿å­˜â†’ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰
"""

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import time
from datetime import datetime
from typing import Dict, List, Tuple
from dataclasses import dataclass
import hashlib


# ================================================================================
# æ—¢å­˜ã®CQCNNå®Ÿè£…ã‚’çµ±åˆ
# ================================================================================


class LearningConfig:
    """å­¦ç¿’è¨­å®šï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç§»æ¤ï¼‰"""

    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.batch_size = 32
        self.learning_rate = 0.001
        self.supervised_epochs = 50
        self.validation_split = 0.2


def create_enhanced_training_data(n_samples: int = 1000) -> List[Dict]:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç§»æ¤ãƒ»æ”¹è‰¯ï¼‰"""
    print(f"ğŸ“Š {n_samples}ä»¶ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")

    data = []
    piece_types = ["good", "bad"]

    for i in range(n_samples):
        # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒœãƒ¼ãƒ‰çŠ¶æ…‹ç”Ÿæˆ
        board = np.zeros((6, 6), dtype=int)
        my_pieces = {}
        enemy_pieces = {}

        # è‡ªåˆ†ã®é§’é…ç½®
        for _ in range(random.randint(3, 8)):
            while True:
                r, c = random.randint(0, 5), random.randint(0, 5)
                if board[r, c] == 0:
                    piece_type = random.choice(piece_types)
                    board[r, c] = 1  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼A
                    my_pieces[(r, c)] = piece_type
                    break

        # æ•µã®é§’é…ç½®
        for _ in range(random.randint(3, 8)):
            while True:
                r, c = random.randint(0, 5), random.randint(0, 5)
                if board[r, c] == 0:
                    piece_type = random.choice(piece_types)
                    board[r, c] = -1  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼B
                    enemy_pieces[(r, c)] = piece_type
                    break

        # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å¤‰æ›
        for pos, true_type in enemy_pieces.items():
            data.append(
                {
                    "board": board.copy(),
                    "my_pieces": my_pieces.copy(),
                    "enemy_position": pos,
                    "true_type": true_type,
                    "player": "A",
                    "turn": random.randint(1, 50),
                }
            )

        if (i + 1) % 200 == 0:
            print(f"   {i + 1}/{n_samples} å®Œäº†", end="\r")

    print(f"\nâœ… {len(data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆå®Œäº†")
    return data


def create_board_tensor(board: np.ndarray, my_pieces: Dict, player: str, turn: int) -> torch.Tensor:
    """ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç§»æ¤ï¼‰"""
    channels = []

    # ãƒãƒ£ãƒ³ãƒãƒ«1: è‡ªåˆ†ã®é§’ä½ç½®
    my_pos = np.zeros_like(board)
    for (r, c), _ in my_pieces.items():
        if 0 <= r < board.shape[0] and 0 <= c < board.shape[1]:
            my_pos[r, c] = 1
    channels.append(my_pos)

    # ãƒãƒ£ãƒ³ãƒãƒ«2: è‡ªåˆ†ã®å–„ç‰
    my_good = np.zeros_like(board)
    for (r, c), piece_type in my_pieces.items():
        if piece_type == "good" and 0 <= r < board.shape[0] and 0 <= c < board.shape[1]:
            my_good[r, c] = 1
    channels.append(my_good)

    # ãƒãƒ£ãƒ³ãƒãƒ«3: è‡ªåˆ†ã®æ‚ªç‰
    my_bad = np.zeros_like(board)
    for (r, c), piece_type in my_pieces.items():
        if piece_type == "bad" and 0 <= r < board.shape[0] and 0 <= c < board.shape[1]:
            my_bad[r, c] = 1
    channels.append(my_bad)

    # ãƒãƒ£ãƒ³ãƒãƒ«4: æ•µã®é§’ä½ç½®
    enemy_pos = np.zeros_like(board)
    for r in range(board.shape[0]):
        for c in range(board.shape[1]):
            if board[r, c] != 0 and (r, c) not in my_pieces:
                enemy_pos[r, c] = 1
    channels.append(enemy_pos)

    # ãƒãƒ£ãƒ³ãƒãƒ«5: ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼æƒ…å ±
    player_channel = np.ones_like(board) if player == "A" else np.zeros_like(board)
    channels.append(player_channel)

    # ãƒãƒ£ãƒ³ãƒãƒ«6: ã‚¿ãƒ¼ãƒ³æƒ…å ±
    turn_channel = np.full_like(board, turn / 100.0)
    channels.append(turn_channel)

    # ãƒãƒ£ãƒ³ãƒãƒ«7: ãƒœãƒ¼ãƒ‰å¢ƒç•Œæƒ…å ±
    boundary = np.zeros_like(board)
    boundary[0, :] = boundary[-1, :] = boundary[:, 0] = boundary[:, -1] = 1
    channels.append(boundary)

    # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ› (1, 7, H, W)
    tensor = torch.FloatTensor(np.stack(channels)).unsqueeze(0)
    return tensor


class CQCNNModel(nn.Module):
    """å®Ÿéš›ã®CQCNNãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç§»æ¤ãƒ»æ”¹è‰¯ï¼‰"""

    def __init__(self, n_qubits: int = 8, n_layers: int = 3):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers

        # å¤å…¸éƒ¨åˆ†ï¼šCNNã§ç‰¹å¾´æŠ½å‡º
        self.conv_layers = nn.Sequential(
            nn.Conv2d(7, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            nn.AdaptiveAvgPool2d((3, 3)),  # 6x6 -> 3x3
        )

        # é‡å­éƒ¨åˆ†ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰
        self.quantum_feature_dim = 32
        self.quantum_linear = nn.Linear(64 * 3 * 3, self.quantum_feature_dim)

        # å‡ºåŠ›å±¤
        self.classifier = nn.Sequential(
            nn.Linear(self.quantum_feature_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 2),  # good vs bad
        )

    def quantum_layer(self, x):
        """é‡å­å±¤ã®ç°¡ç•¥åŒ–å®Ÿè£…"""
        # å®Ÿéš›ã®é‡å­å›è·¯ã®ä»£ã‚ã‚Šã«ã€è¤‡é›‘ãªéç·šå½¢å¤‰æ›ã‚’ä½¿ç”¨
        batch_size = x.size(0)
        x = x.view(batch_size, -1)

        # "é‡å­çš„"ãªéç·šå½¢å¤‰æ›
        x = self.quantum_linear(x)
        x = torch.tanh(x)  # é‡å­çŠ¶æ…‹ã®ç¯„å›²ã‚’æ¨¡æ“¬

        # é‡å­å¹²æ¸‰åŠ¹æœã‚’æ¨¡æ“¬
        x_real = x[:, : self.quantum_feature_dim // 2]
        x_imag = x[:, self.quantum_feature_dim // 2 :]
        amplitude = torch.sqrt(x_real**2 + x_imag**2 + 1e-8)

        return amplitude

    def forward(self, x):
        # å¤å…¸CNNéƒ¨åˆ†
        conv_out = self.conv_layers(x)

        # é‡å­å±¤
        quantum_out = self.quantum_layer(conv_out)

        # åˆ†é¡
        output = self.classifier(quantum_out)
        return output


class CQCNNEstimator:
    """CQCNNæ¨å®šå™¨ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç§»æ¤ãƒ»æ”¹è‰¯ï¼‰"""

    def __init__(self, n_qubits: int = 8, n_layers: int = 3):
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.model = CQCNNModel(n_qubits, n_layers)
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
        self.training_history = []
        self.is_trained = False
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

    def train_supervised(self, training_data: List[Dict], config: LearningConfig) -> Dict:
        """æ•™å¸«ã‚ã‚Šå­¦ç¿’"""
        print(f"ğŸ”¬ CQCNNæ•™å¸«ã‚ã‚Šå­¦ç¿’é–‹å§‹: {len(training_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        print(f"âš™ï¸ è¨­å®š: ãƒãƒƒãƒã‚µã‚¤ã‚º={config.batch_size}, å­¦ç¿’ç‡={config.learning_rate}")
        print(f"ğŸ’» ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        print("=" * 60)

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        n_val = int(len(training_data) * config.validation_split)
        val_data = training_data[:n_val]
        train_data = training_data[n_val:]

        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: å­¦ç¿’ç”¨={len(train_data)}ä»¶, æ¤œè¨¼ç”¨={len(val_data)}ä»¶")
        print("=" * 60)

        self.model.train()
        best_val_acc = 0.0
        patience_counter = 0
        training_start_time = time.time()

        for epoch in range(config.supervised_epochs):
            random.shuffle(train_data)

            total_loss = 0
            correct = 0
            total = 0
            batch_count = 0

            n_batches = len(train_data) // config.batch_size
            print(f"\nEpoch {epoch + 1}/{config.supervised_epochs}")
            print("ğŸ“ˆ å­¦ç¿’ä¸­: ", end="", flush=True)

            for i in range(0, len(train_data), config.batch_size):
                batch = train_data[i : i + config.batch_size]

                if batch_count % max(1, n_batches // 20) == 0:
                    print("â–ˆ", end="", flush=True)

                inputs, labels = self._prepare_supervised_batch(batch)
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                batch_count += 1

            print(" âœ…")

            train_acc = 100 * correct / total if total > 0 else 0.0
            avg_batches = max(1, len(train_data) // config.batch_size)
            train_loss = total_loss / avg_batches

            print("ğŸ” æ¤œè¨¼ä¸­...", end="", flush=True)
            val_acc, val_loss = self._validate_supervised(val_data, config.batch_size)
            print(" âœ…")

            self.training_history.append(
                {
                    "epoch": epoch,
                    "train_loss": train_loss,
                    "train_acc": train_acc,
                    "val_loss": val_loss,
                    "val_acc": val_acc,
                }
            )

            improvement = "ğŸ“ˆ" if val_acc > best_val_acc else "ğŸ“‰" if val_acc < best_val_acc else "â¡ï¸"
            print(
                f"ğŸ“Š çµæœ: Train Loss={train_loss:.4f} | Train Acc={train_acc:.1f}% | "
                f"Val Loss={val_loss:.4f} | Val Acc={val_acc:.1f}% {improvement}"
            )

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                print("ğŸ¯ æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼")
            else:
                patience_counter += 1
                if patience_counter >= 10:
                    print(f"â¹ï¸ Early Stopping: {patience_counter}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
                    break

            print("-" * 60)

        training_time = time.time() - training_start_time
        self.is_trained = True

        final_result = {
            "best_val_acc": best_val_acc,
            "final_train_acc": train_acc,
            "training_time": training_time,
            "total_epochs": epoch + 1,
            "history": self.training_history,
        }

        print("\nğŸ‰ æ•™å¸«ã‚ã‚Šå­¦ç¿’å®Œäº†!")
        print(f"â±ï¸ å­¦ç¿’æ™‚é–“: {training_time:.1f}ç§’")
        print(f"ğŸ† æœ€é«˜æ¤œè¨¼ç²¾åº¦: {best_val_acc:.1f}%")

        return final_result

    def _prepare_supervised_batch(self, batch: List[Dict]) -> Tuple[torch.Tensor, torch.Tensor]:
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        inputs = []
        labels = []

        for item in batch:
            # ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
            tensor = create_board_tensor(item["board"], item["my_pieces"], item["player"], item["turn"])
            inputs.append(tensor.squeeze(0))

            # ãƒ©ãƒ™ãƒ«å¤‰æ›
            label = 0 if item["true_type"] == "good" else 1
            labels.append(label)

        return torch.stack(inputs), torch.tensor(labels, dtype=torch.long)

    def _validate_supervised(self, val_data: List[Dict], batch_size: int) -> Tuple[float, float]:
        """æ¤œè¨¼"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for i in range(0, len(val_data), batch_size):
                batch = val_data[i : i + batch_size]
                inputs, labels = self._prepare_supervised_batch(batch)
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        self.model.train()
        avg_loss = total_loss / max(1, len(val_data) // batch_size)
        accuracy = 100 * correct / total if total > 0 else 0.0

        return accuracy, avg_loss

    def estimate(
        self, board: np.ndarray, my_pieces: Dict, enemy_positions: List[Tuple], player: str, turn: int = 1
    ) -> Dict:
        """æ•µé§’æ¨å®š"""
        if not self.is_trained:
            # æœªå­¦ç¿’ã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ æ¨å®š
            return {pos: {"good": 0.5, "bad": 0.5} for pos in enemy_positions}

        self.model.eval()
        estimations = {}

        with torch.no_grad():
            for pos in enemy_positions:
                # ç‰¹å®šä½ç½®ã®æ¨å®šã®ãŸã‚ã®ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
                tensor = create_board_tensor(board, my_pieces, player, turn)
                tensor = tensor.to(self.device)

                output = self.model(tensor)
                probs = torch.softmax(output, dim=-1).squeeze()

                estimations[pos] = {"good": float(probs[0]), "bad": float(probs[1])}

        return estimations

    def save_model(self, filepath: str):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        torch.save(
            {
                "model_state_dict": self.model.state_dict(),
                "optimizer_state_dict": self.optimizer.state_dict(),
                "training_history": self.training_history,
                "is_trained": self.is_trained,
                "n_qubits": self.n_qubits,
                "n_layers": self.n_layers,
            },
            filepath,
        )
        print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {filepath}")

    def load_model(self, filepath: str):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.training_history = checkpoint.get("training_history", [])
        self.is_trained = checkpoint.get("is_trained", True)
        print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {filepath}")


# ================================================================================
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šã¨ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³
# ================================================================================


@dataclass
class ModuleConfig:
    """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""

    placement: str
    estimator: str
    reward: str
    qmap: str
    selector: str

    def to_dict(self) -> Dict:
        return {
            "placement": self.placement,
            "estimator": self.estimator,
            "reward": self.reward,
            "qmap": self.qmap,
            "selector": self.selector,
        }


class GameEngine:
    """ç°¡ç•¥åŒ–ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.max_turns = 100

    def play_game(
        self,
        ai1_config: ModuleConfig,
        ai1_estimator: CQCNNEstimator,
        ai2_config: ModuleConfig,
        ai2_estimator: CQCNNEstimator,
    ) -> str:
        """ã‚²ãƒ¼ãƒ å®Ÿè¡Œï¼ˆç°¡ç•¥åŒ–ï¼‰"""
        # å®Ÿéš›ã®ã‚²ãƒ¼ãƒ ãƒ­ã‚¸ãƒƒã‚¯ã®ä»£ã‚ã‚Šã«ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šã¨æ¨å®šå™¨ã®æ€§èƒ½ã«åŸºã¥ãç¢ºç‡çš„åˆ¤å®š

        ai1_strength = self._calculate_strength(ai1_config, ai1_estimator)
        ai2_strength = self._calculate_strength(ai2_config, ai2_estimator)

        # ç¢ºç‡çš„ãªå‹æ•—åˆ¤å®š
        total_strength = ai1_strength + ai2_strength
        if total_strength == 0:
            return random.choice(["A", "B"])

        win_prob_ai1 = ai1_strength / total_strength
        return "A" if random.random() < win_prob_ai1 else "B"

    def _calculate_strength(self, config: ModuleConfig, estimator: CQCNNEstimator) -> float:
        """AIå¼·åº¦è¨ˆç®—"""
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŸºæœ¬å¼·åº¦
        module_strengths = {
            "placement": {"standard": 0.7, "aggressive": 0.8, "defensive": 0.6, "random": 0.4},
            "estimator": {"cqcnn": 0.9, "cnn": 0.7, "pattern": 0.6, "random": 0.3},
            "reward": {"standard": 0.7, "aggressive": 0.8, "defensive": 0.6, "smart": 0.9},
            "qmap": {"simple": 0.6, "strategic": 0.8, "adaptive": 0.9, "greedy": 0.5},
            "selector": {"greedy": 0.6, "epsilon_01": 0.7, "epsilon_03": 0.8, "softmax": 0.7, "adaptive": 0.9},
        }

        base_strength = 0
        for module_type, module_name in config.to_dict().items():
            base_strength += module_strengths.get(module_type, {}).get(module_name, 0.5)

        base_strength /= 5  # å¹³å‡åŒ–

        # æ¨å®šå™¨ãŒå­¦ç¿’æ¸ˆã¿ã®å ´åˆã¯ãƒœãƒ¼ãƒŠã‚¹
        if estimator.is_trained and config.estimator == "cqcnn":
            # å­¦ç¿’ç²¾åº¦ã«åŸºã¥ããƒœãƒ¼ãƒŠã‚¹
            if estimator.training_history:
                last_val_acc = estimator.training_history[-1].get("val_acc", 50)
                learning_bonus = (last_val_acc - 50) / 100  # 50%ã‚’åŸºæº–ã¨ã—ãŸæ­£è¦åŒ–
                base_strength += learning_bonus

        return max(0.1, base_strength)


# ================================================================================
# çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================


class RealLearningBattleSystem:
    """å®Ÿéš›ã«å­¦ç¿’ã‚’è¡Œã†ãƒ­ãƒ¼ã‚«ãƒ«å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.available_modules = self._initialize_modules()
        self.game_engine = GameEngine()
        self.saved_ais = []
        self.models_dir = "saved_ais"
        os.makedirs(self.models_dir, exist_ok=True)

    def _initialize_modules(self) -> Dict:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
        return {
            "placement": {
                "standard": {"name": "ğŸ¯ æ¨™æº–é…ç½®", "description": "ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ¨™æº–çš„ãªé…ç½®æˆ¦ç•¥"},
                "aggressive": {"name": "âš”ï¸ æ”»æ’ƒçš„é…ç½®", "description": "å‰ç·šã«é§’ã‚’é›†ä¸­ã•ã›ã‚‹æ”»æ’ƒçš„æˆ¦ç•¥"},
                "defensive": {"name": "ğŸ›¡ï¸ é˜²å¾¡çš„é…ç½®", "description": "å¾Œæ–¹ã‚’å›ºã‚ã‚‹é˜²å¾¡çš„æˆ¦ç•¥"},
                "random": {"name": "ğŸ² ãƒ©ãƒ³ãƒ€ãƒ é…ç½®", "description": "ãƒ©ãƒ³ãƒ€ãƒ ãªé…ç½®ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰"},
            },
            "estimator": {
                "cqcnn": {"name": "ğŸ”¬ CQCNN", "description": "é‡å­ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"},
                "cnn": {"name": "ğŸ§  ã‚·ãƒ³ãƒ—ãƒ«CNN", "description": "å¾“æ¥ã®ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"},
                "random": {"name": "ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æ¨å®š", "description": "ãƒ©ãƒ³ãƒ€ãƒ ãªæ•µé§’æ¨å®šï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰"},
                "pattern": {"name": "ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³æ¨å®š", "description": "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜"},
            },
            "reward": {
                "standard": {"name": "âš–ï¸ æ¨™æº–å ±é…¬", "description": "ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå ±é…¬é–¢æ•°"},
                "aggressive": {"name": "ğŸ’¥ æ”»æ’ƒçš„å ±é…¬", "description": "æ”»æ’ƒè¡Œå‹•ã‚’é‡è¦–ã™ã‚‹å ±é…¬"},
                "defensive": {"name": "ğŸ”’ é˜²å¾¡çš„å ±é…¬", "description": "é˜²å¾¡è¡Œå‹•ã‚’é‡è¦–ã™ã‚‹å ±é…¬"},
                "smart": {"name": "ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆå ±é…¬", "description": "çŠ¶æ³ã«å¿œã˜ã¦é©å¿œã™ã‚‹å ±é…¬"},
            },
            "qmap": {
                "simple": {"name": "ğŸ“ˆ ã‚·ãƒ³ãƒ—ãƒ«Qå€¤", "description": "åŸºæœ¬çš„ãªQå€¤ãƒãƒƒãƒ”ãƒ³ã‚°"},
                "strategic": {"name": "ğŸ§© æˆ¦ç•¥çš„Qå€¤", "description": "é«˜åº¦ãªæˆ¦ç•¥ã‚’è€ƒæ…®ã—ãŸQå€¤"},
                "adaptive": {"name": "ğŸ”„ é©å¿œçš„Qå€¤", "description": "ç›¸æ‰‹ã«å¿œã˜ã¦é©å¿œã™ã‚‹Qå€¤"},
                "greedy": {"name": "ğŸ’° è²ªæ¬²Qå€¤", "description": "çŸ­æœŸåˆ©ç›Šã‚’é‡è¦–ã™ã‚‹Qå€¤"},
            },
            "selector": {
                "greedy": {"name": "ğŸ¯ è²ªæ¬²é¸æŠ", "description": "å¸¸ã«æœ€è‰¯ã®è¡Œå‹•ã‚’é¸æŠ"},
                "epsilon_01": {"name": "ğŸ² æ¢ç´¢çš„(Îµ=0.1)", "description": "10%ã®ç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•"},
                "epsilon_03": {"name": "ğŸ² æ¢ç´¢çš„(Îµ=0.3)", "description": "30%ã®ç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•"},
                "softmax": {"name": "ğŸŒ¡ï¸ Softmax", "description": "ç¢ºç‡çš„ãªè¡Œå‹•é¸æŠ"},
                "adaptive": {"name": "ğŸ”„ é©å¿œçš„é¸æŠ", "description": "çŠ¶æ³ã«å¿œã˜ã¦é¸æŠæˆ¦ç•¥ã‚’å¤‰æ›´"},
            },
        }

    def select_modules(self) -> ModuleConfig:
        """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é¸æŠUI"""
        print("\nğŸ¯ AIè¨­è¨ˆãƒ•ã‚§ãƒ¼ã‚º")
        print("=" * 60)
        print("5ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æœ€å¼·ã®AIã‚’ä½œã‚Šã¾ã—ã‚‡ã†ï¼")

        selected = {}

        for module_type, modules in self.available_modules.items():
            print(f"\nã€{module_type.upper()}ã€‘ã‚’é¸æŠã—ã¦ãã ã•ã„:")

            for i, (key, info) in enumerate(modules.items(), 1):
                print(f"  {i}. {info['name']}")
                print(f"     {info['description']}")

            while True:
                try:
                    choice = int(input(f"\né¸æŠ (1-{len(modules)}): ")) - 1
                    selected_key = list(modules.keys())[choice]
                    selected[module_type] = selected_key

                    chosen = modules[selected_key]
                    print(f"âœ… {chosen['name']} ã‚’é¸æŠã—ã¾ã—ãŸï¼\n")
                    break
                except (ValueError, IndexError):
                    print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚ã‚‚ã†ä¸€åº¦å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

        return ModuleConfig(**selected)

    def train_ai(self, config: ModuleConfig) -> Tuple[CQCNNEstimator, Dict]:
        """AIå­¦ç¿’ï¼ˆå®Ÿéš›ã®å­¦ç¿’ã‚’å®Ÿè¡Œï¼‰"""
        print("\nğŸ“ AIå­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º")
        print("=" * 60)

        # CQCNNæ¨å®šå™¨ä½œæˆ
        estimator = CQCNNEstimator(n_qubits=8, n_layers=3)

        if config.estimator == "cqcnn":
            print("ğŸ”¬ CQCNNå­¦ç¿’ã‚’é–‹å§‹...")

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            data_size = int(input("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º (100-5000, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1000): ") or "1000")
            training_data = create_enhanced_training_data(data_size)

            # å­¦ç¿’è¨­å®š
            learning_config = LearningConfig()

            # å®Ÿéš›ã®å­¦ç¿’å®Ÿè¡Œ
            training_result = estimator.train_supervised(training_data, learning_config)

            return estimator, training_result
        else:
            print(f"ğŸ“Š {config.estimator}æ¨å®šå™¨ã‚’åˆæœŸåŒ–...")
            # ä»–ã®æ¨å®šå™¨ã¯å­¦ç¿’ãªã—ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
            return estimator, {"message": "éå­¦ç¿’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"}

    def evaluate_ai(self, config: ModuleConfig, estimator: CQCNNEstimator, num_games: int = 30) -> Dict:
        """AIè©•ä¾¡ï¼ˆå®Ÿéš›ã®ã‚²ãƒ¼ãƒ å¯¾æˆ¦ï¼‰"""
        print(f"\nğŸ® AIè©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚º ({num_games}æˆ¦)")
        print("=" * 60)

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³AIè¨­å®š
        baselines = {
            "random": ModuleConfig("random", "random", "standard", "simple", "greedy"),
            "simple": ModuleConfig("standard", "pattern", "standard", "simple", "epsilon_01"),
            "strong": ModuleConfig("aggressive", "cnn", "aggressive", "strategic", "adaptive"),
        }

        results = {}
        total_wins = 0
        total_games = 0

        for baseline_name, baseline_config in baselines.items():
            print(f"\nâš”ï¸ vs {baseline_name.upper()}AI: ", end="", flush=True)

            baseline_estimator = CQCNNEstimator()  # æœªå­¦ç¿’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            wins = 0

            for game in range(num_games // 3):  # å„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨10æˆ¦
                result = self.game_engine.play_game(config, estimator, baseline_config, baseline_estimator)
                if result == "A":
                    wins += 1

                if (game + 1) % 3 == 0:
                    print(".", end="", flush=True)

            win_rate = wins / (num_games // 3)
            results[baseline_name] = {"wins": wins, "total": num_games // 3, "win_rate": win_rate}

            total_wins += wins
            total_games += num_games // 3

            # çµæœè¡¨ç¤º
            status = "ğŸ”¥" if win_rate >= 0.8 else "âœ…" if win_rate >= 0.6 else "âš–ï¸" if win_rate >= 0.5 else "âŒ"
            print(f" {wins}/{num_games // 3} ({win_rate:.1%}) {status}")

        overall_win_rate = total_wins / total_games

        return {
            "vs_random": results["random"],
            "vs_simple": results["simple"],
            "vs_strong": results["strong"],
            "overall_win_rate": overall_win_rate,
            "total_wins": total_wins,
            "total_games": total_games,
            "is_strong": overall_win_rate >= 0.6,
        }

    def save_ai(self, config: ModuleConfig, estimator: CQCNNEstimator, evaluation: Dict, ai_name: str) -> str:
        """AIã‚’ä¿å­˜"""
        ai_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]

        ai_info = {
            "id": ai_id,
            "name": ai_name,
            "config": config.to_dict(),
            "evaluation": evaluation,
            "created_at": datetime.now().isoformat(),
        }

        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        model_path = os.path.join(self.models_dir, f"{ai_id}.pth")
        estimator.save_model(model_path)

        # AIæƒ…å ±ä¿å­˜
        info_path = os.path.join(self.models_dir, f"{ai_id}_info.json")
        with open(info_path, "w", encoding="utf-8") as f:
            json.dump(ai_info, f, indent=2, ensure_ascii=False)

        self.saved_ais.append(ai_info)
        print(f"ğŸ’¾ AI '{ai_name}' ã‚’ä¿å­˜ã—ã¾ã—ãŸ (ID: {ai_id})")
        return ai_id

    def load_saved_ais(self):
        """ä¿å­˜ã•ã‚ŒãŸAIä¸€è¦§ã‚’èª­ã¿è¾¼ã¿"""
        self.saved_ais = []
        for filename in os.listdir(self.models_dir):
            if filename.endswith("_info.json"):
                info_path = os.path.join(self.models_dir, filename)
                with open(info_path, "r", encoding="utf-8") as f:
                    self.saved_ais.append(json.load(f))

    def run_tournament(self, games_per_match: int = 5) -> Dict:
        """ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ"""
        strong_ais = [ai for ai in self.saved_ais if ai["evaluation"].get("is_strong", False)]

        if len(strong_ais) < 2:
            print("âŒ ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆã«ã¯2ã¤ä»¥ä¸Šã®å¼·ã„AIãŒå¿…è¦ã§ã™")
            return {}

        print("\nğŸ† AIãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé–‹å§‹ï¼")
        print(f"å‚åŠ è€…: {len(strong_ais)}ä½“ã®AI")
        print("=" * 60)

        # å‚åŠ AIèª­ã¿è¾¼ã¿
        participants = []
        for ai_info in strong_ais:
            config = ModuleConfig(**ai_info["config"])
            estimator = CQCNNEstimator()
            model_path = os.path.join(self.models_dir, f"{ai_info['id']}.pth")
            if os.path.exists(model_path):
                estimator.load_model(model_path)
            participants.append((ai_info, config, estimator))

        # ç·å½“ãŸã‚Šæˆ¦
        match_results = []
        rankings = {ai[0]["id"]: {"wins": 0, "total": 0, "name": ai[0]["name"]} for ai in participants}

        for i, (ai1_info, ai1_config, ai1_estimator) in enumerate(participants):
            for j, (ai2_info, ai2_config, ai2_estimator) in enumerate(participants):
                if i >= j:
                    continue

                print(f"\nâš”ï¸ {ai1_info['name']} vs {ai2_info['name']}")

                ai1_wins = 0
                for game in range(games_per_match):
                    result = self.game_engine.play_game(ai1_config, ai1_estimator, ai2_config, ai2_estimator)
                    if result == "A":
                        ai1_wins += 1

                ai2_wins = games_per_match - ai1_wins

                match_results.append(
                    {
                        "ai1_id": ai1_info["id"],
                        "ai2_id": ai2_info["id"],
                        "ai1_name": ai1_info["name"],
                        "ai2_name": ai2_info["name"],
                        "ai1_wins": ai1_wins,
                        "ai2_wins": ai2_wins,
                    }
                )

                rankings[ai1_info["id"]]["wins"] += ai1_wins
                rankings[ai1_info["id"]]["total"] += games_per_match
                rankings[ai2_info["id"]]["wins"] += ai2_wins
                rankings[ai2_info["id"]]["total"] += games_per_match

                print(f"   çµæœ: {ai1_wins}-{ai2_wins}")

        # æœ€çµ‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        final_rankings = []
        for ai_id, stats in rankings.items():
            win_rate = stats["wins"] / max(stats["total"], 1)
            final_rankings.append(
                {
                    "ai_id": ai_id,
                    "name": stats["name"],
                    "wins": stats["wins"],
                    "total": stats["total"],
                    "win_rate": win_rate,
                }
            )

        final_rankings.sort(key=lambda x: x["win_rate"], reverse=True)

        return {
            "participants": len(strong_ais),
            "matches": match_results,
            "rankings": final_rankings,
            "date": datetime.now().isoformat(),
        }

    def main_workflow(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""
        print("ğŸ® å®Ÿéš›ã«å­¦ç¿’ã‚’è¡Œã†ãƒ­ãƒ¼ã‚«ãƒ«å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 60)
        print("CQCNNã‚’å®Ÿéš›ã«å­¦ç¿’ã•ã›ã¦ã€æœ€å¼·ã®AIã‚’ä½œã‚ã†ï¼")

        # ä¿å­˜æ¸ˆã¿AIèª­ã¿è¾¼ã¿
        self.load_saved_ais()

        while True:
            print("\nã€ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€‘")
            print("1. æ–°ã—ã„AIã‚’ä½œæˆãƒ»å­¦ç¿’")
            print("2. ä¿å­˜ã•ã‚ŒãŸAIä¸€è¦§")
            print("3. AIãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé–‹å‚¬")
            print("0. çµ‚äº†")

            choice = input("\né¸æŠ (0-3): ")

            if choice == "1":
                # AIä½œæˆãƒ«ãƒ¼ãƒ—
                while True:
                    print("\n" + "=" * 60)
                    print("ğŸ”¬ AIä½œæˆãƒ»è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚º")
                    print("=" * 60)

                    # 1. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é¸æŠ
                    config = self.select_modules()

                    # 2. å­¦ç¿’å®Ÿè¡Œ
                    estimator, training_result = self.train_ai(config)

                    # 3. è©•ä¾¡å®Ÿè¡Œ
                    evaluation = self.evaluate_ai(config, estimator)

                    # 4. çµæœè¡¨ç¤º
                    print("\nğŸ“Š è©•ä¾¡çµæœ")
                    print("=" * 50)
                    print(
                        f"ç·åˆå‹ç‡: {evaluation['overall_win_rate']:.1%} ({evaluation['total_wins']}/{evaluation['total_games']})"
                    )

                    for opponent in ["random", "simple", "strong"]:
                        result = evaluation[f"vs_{opponent}"]
                        rate = result["win_rate"]
                        status = "ğŸ”¥" if rate >= 0.8 else "âœ…" if rate >= 0.6 else "âš–ï¸" if rate >= 0.5 else "âŒ"
                        print(f"  vs {opponent}AI: {result['wins']}/{result['total']} ({rate:.1%}) {status}")

                    # 5. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                    if evaluation["is_strong"]:
                        print("\nğŸŒŸ åˆ¤å®š: ååˆ†å¼·ã„AIï¼ï¼ˆåŸºæº–: 60%ä»¥ä¸Šã®å‹ç‡ï¼‰")
                        action = input("\næ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\n1. ã“ã®AIã‚’ä¿å­˜ã™ã‚‹\n2. ã•ã‚‰ã«æ”¹è‰¯ã‚’è©¦ã™\né¸æŠ (1-2): ")
                        if action == "1":
                            ai_name = input("\nAIã«åå‰ã‚’ã¤ã‘ã¦ãã ã•ã„: ") or "ç„¡åAI"
                            self.save_ai(config, estimator, evaluation, ai_name)
                            break
                    else:
                        print(f"\nâš ï¸ åˆ¤å®š: ã¾ã æ”¹è‰¯ãŒå¿…è¦ï¼ˆç¾åœ¨: {evaluation['overall_win_rate']:.1%}ã€åŸºæº–: 60%ï¼‰")
                        action = input(
                            "\næ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\n"
                            "1. åˆ¥ã®çµ„ã¿åˆã‚ã›ã‚’è©¦ã™\n"
                            "2. ã“ã®AIã§ã‚‚ä¿å­˜ã™ã‚‹\n"
                            "3. ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«æˆ»ã‚‹\n"
                            "é¸æŠ (1-3): "
                        )
                        if action == "2":
                            ai_name = input("\nAIã«åå‰ã‚’ã¤ã‘ã¦ãã ã•ã„: ") or "å¼±ã„AI"
                            self.save_ai(config, estimator, evaluation, ai_name)
                            break
                        elif action == "3":
                            break

            elif choice == "2":
                self.load_saved_ais()
                if not self.saved_ais:
                    print("\nğŸ“­ ã¾ã AIãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                else:
                    print(f"\nğŸ“Š ä¿å­˜ã•ã‚ŒãŸAI: {len(self.saved_ais)}ä½“")
                    for i, ai in enumerate(self.saved_ais, 1):
                        status = "ğŸ’ª" if ai["evaluation"].get("is_strong", False) else "ğŸ˜"
                        rate = ai["evaluation"]["overall_win_rate"]
                        print(f"  {i}. {ai['name']} {status} (å‹ç‡: {rate:.1%})")

            elif choice == "3":
                results = self.run_tournament()
                if results:
                    print("\nğŸ† ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆçµæœ")
                    print("=" * 60)
                    for i, entry in enumerate(results["rankings"]):
                        medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
                        print(f"{medal} {i + 1}ä½: {entry['name']}")
                        print(f"        å‹ç‡: {entry['win_rate']:.1%} ({entry['wins']}/{entry['total']})")

            elif choice == "0":
                print("\nğŸ‘‹ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼")
                break
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        system = RealLearningBattleSystem()
        system.main_workflow()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ å®Ÿè¡Œä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
