#!/usr/bin/env python3
"""
å¼·åŒ–å­¦ç¿’ç‰ˆCQCNNç«¶æŠ€ãƒ©ãƒ³ãƒŠãƒ¼
è‡ªå·±å¯¾æˆ¦ã«ã‚ˆã‚‹å­¦ç¿’æ©Ÿèƒ½ã‚’æŒã¤å®Œå…¨ç‰ˆã‚·ã‚¹ãƒ†ãƒ 
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from typing import Dict, List, Tuple, Optional, NamedTuple
from collections import deque
import random
import matplotlib.pyplot as plt


# ================================================================================
# Part 1: åŸºæœ¬å®šç¾©ã¨çµŒé¨“ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# ================================================================================


class Experience(NamedTuple):
    """å¼·åŒ–å­¦ç¿’ç”¨ã®çµŒé¨“ãƒ‡ãƒ¼ã‚¿"""

    state: np.ndarray
    action: Tuple
    reward: float
    next_state: Optional[np.ndarray]
    done: bool

    # CQCNNç‰¹æœ‰ã®æƒ…å ±
    enemy_positions: List[Tuple[int, int]]
    estimations: Dict[Tuple[int, int], Dict[str, float]]
    q_map: np.ndarray


class RLConfig:
    """å¼·åŒ–å­¦ç¿’è¨­å®š"""

    def __init__(self):
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.learning_rate = 0.001
        self.gamma = 0.95  # å‰²å¼•ç‡
        self.epsilon_start = 1.0
        self.epsilon_end = 0.01
        self.epsilon_decay = 0.995

        # ãƒãƒƒãƒ•ã‚¡è¨­å®š
        self.buffer_size = 10000
        self.batch_size = 32
        self.update_target_every = 100

        # å ±é…¬è¨­å®š
        self.reward_correct_estimation = 1.0
        self.reward_wrong_estimation = -0.5
        self.reward_win = 10.0
        self.reward_lose = -10.0
        self.reward_draw = 0.0
        self.reward_capture = 2.0
        self.reward_move = -0.01


# ================================================================================
# Part 2: é‡å­å›è·¯å±¤ï¼ˆå¼·åŒ–å­¦ç¿’å¯¾å¿œç‰ˆï¼‰
# ================================================================================


class RLQuantumCircuitLayer(nn.Module):
    """å¼·åŒ–å­¦ç¿’ã«æœ€é©åŒ–ã•ã‚ŒãŸé‡å­å›è·¯å±¤"""

    def __init__(self, n_qubits: int, n_layers: int, dropout_rate: float = 0.1):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.dropout = nn.Dropout(dropout_rate)

        # é‡å­å›è·¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.rotation_params = nn.Parameter(torch.randn(n_layers, n_qubits, 3) * 0.1)
        self.entanglement_params = nn.Parameter(torch.randn(n_layers, n_qubits - 1) * 0.1)

        # ãƒãƒƒãƒæ­£è¦åŒ–
        self.batch_norm = nn.BatchNorm1d(n_qubits)

    def forward(self, x: torch.Tensor, training: bool = True) -> torch.Tensor:
        batch_size = x.shape[0]

        # å…¥åŠ›ã‚’é‡å­ãƒ“ãƒƒãƒˆæ•°ã«èª¿æ•´
        if x.shape[1] != self.n_qubits:
            fc = nn.Linear(x.shape[1], self.n_qubits).to(x.device)
            x = fc(x)

        # é‡å­çŠ¶æ…‹ã‚’åˆæœŸåŒ–
        quantum_state = torch.zeros(batch_size, self.n_qubits, 2, dtype=torch.complex64)
        quantum_state[:, :, 0] = 1.0

        # å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæŒ¯å¹…ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
        for i in range(self.n_qubits):
            angle = x[:, i].unsqueeze(1) * np.pi
            quantum_state[:, i, 0] = torch.cos(angle / 2).squeeze()
            quantum_state[:, i, 1] = torch.sin(angle / 2).squeeze()

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–é‡å­å›è·¯
        for layer in range(self.n_layers):
            # å›è»¢ã‚²ãƒ¼ãƒˆ
            for i in range(self.n_qubits):
                rx = self.rotation_params[layer, i, 0]
                ry = self.rotation_params[layer, i, 1]
                rz = self.rotation_params[layer, i, 2]

                # Pauliå›è»¢ã‚’é©ç”¨
                quantum_state[:, i] = self._apply_rotation(quantum_state[:, i], rx, ry, rz)

            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
            for i in range(self.n_qubits - 1):
                strength = torch.sigmoid(self.entanglement_params[layer, i])
                quantum_state = self._apply_entanglement(quantum_state, i, i + 1, strength)

        # æ¸¬å®šï¼ˆæœŸå¾…å€¤ï¼‰
        measurements = (quantum_state.abs() ** 2)[:, :, 1].real

        # ãƒãƒƒãƒæ­£è¦åŒ–ã¨ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        if training:
            measurements = self.batch_norm(measurements)
            measurements = self.dropout(measurements)

        return measurements

    def _apply_rotation(self, state: torch.Tensor, rx: float, ry: float, rz: float) -> torch.Tensor:
        """Pauliå›è»¢ã‚’é©ç”¨"""
        # RZ
        phase_z = torch.exp(1j * rz / 2)
        state[:, 0] *= phase_z
        state[:, 1] *= torch.conj(phase_z)

        # RY
        c_y = torch.cos(ry / 2)
        s_y = torch.sin(ry / 2)
        temp0 = c_y * state[:, 0] - s_y * state[:, 1]
        temp1 = s_y * state[:, 0] + c_y * state[:, 1]
        state[:, 0] = temp0
        state[:, 1] = temp1

        # RX
        c_x = torch.cos(rx / 2)
        s_x = torch.sin(rx / 2) * 1j
        temp0 = c_x * state[:, 0] - s_x * state[:, 1]
        temp1 = -s_x * state[:, 0] + c_x * state[:, 1]
        state[:, 0] = temp0
        state[:, 1] = temp1

        return state

    def _apply_entanglement(self, state: torch.Tensor, q1: int, q2: int, strength: float) -> torch.Tensor:
        """åˆ¶å¾¡NOTã‚²ãƒ¼ãƒˆé¢¨ã®ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã‚’é©ç”¨"""
        control = state[:, q1, 1].abs()
        state[:, q2] = (1 - strength) * state[:, q2] + strength * control.unsqueeze(1) * state[:, q2].roll(1, dims=1)
        return state


# ================================================================================
# Part 3: å¼·åŒ–å­¦ç¿’ç‰ˆCQCNNæ¨å®šå™¨
# ================================================================================


class RLCQCNNEstimator(nn.Module):
    """å¼·åŒ–å­¦ç¿’ç”¨CQCNNæ•µé§’æ¨å®šå™¨"""

    def __init__(self, n_qubits: int = 8, n_layers: int = 3):
        super().__init__()

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self.n_qubits = n_qubits
        self.n_layers = n_layers

        # CNNå±¤ï¼ˆç‰¹å¾´æŠ½å‡ºï¼‰
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)

        # é‡å­å›è·¯å±¤
        self.quantum_layer = RLQuantumCircuitLayer(n_qubits, n_layers)

        # çµ±åˆå±¤
        self.fc1 = nn.Linear(64 * 2 * 2 + n_qubits, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 6)  # 6ç¨®é¡ã®é§’

        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        self.dropout = nn.Dropout(0.2)

    def forward(self, board_state: torch.Tensor, position: Tuple[int, int], training: bool = True) -> torch.Tensor:
        """
        æ•µé§’ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š

        Args:
            board_state: ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ (batch_size, 3, 6, 5)
            position: æ¨å®šå¯¾è±¡ã®ä½ç½®
            training: å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹

        Returns:
            é§’ã‚¿ã‚¤ãƒ—ã®ç¢ºç‡åˆ†å¸ƒ (batch_size, 6)
        """
        # å±€æ‰€çš„ãªç‰¹å¾´ã‚’æŠ½å‡º
        local_features = self._extract_local_features(board_state, position)

        # CNNå‡¦ç†
        x = F.relu(self.conv1(local_features))
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        cnn_features = x.flatten(start_dim=1)

        # é‡å­å›è·¯å‡¦ç†
        quantum_input = cnn_features[:, : self.n_qubits]
        quantum_features = self.quantum_layer(quantum_input, training)

        # ç‰¹å¾´çµ±åˆ
        combined = torch.cat([cnn_features, quantum_features], dim=1)

        # æœ€çµ‚äºˆæ¸¬
        x = F.relu(self.fc1(combined))
        if training:
            x = self.dropout(x)
        x = F.relu(self.fc2(x))
        if training:
            x = self.dropout(x)
        x = self.fc3(x)

        return F.softmax(x, dim=1)

    def _extract_local_features(self, board_state: torch.Tensor, position: Tuple[int, int]) -> torch.Tensor:
        """ä½ç½®å‘¨è¾ºã®å±€æ‰€ç‰¹å¾´ã‚’æŠ½å‡º"""
        x, y = position
        batch_size = board_state.shape[0]

        # 5x5ã®å±€æ‰€é ˜åŸŸã‚’åˆ‡ã‚Šå‡ºã—
        local = torch.zeros(batch_size, 3, 5, 5)

        for dx in range(-2, 3):
            for dy in range(-2, 3):
                nx, ny = x + dx, y + dy
                if 0 <= nx < 5 and 0 <= ny < 6:
                    local[:, :, dx + 2, dy + 2] = board_state[:, :, ny, nx]

        return local


# ================================================================================
# Part 4: DQNï¼ˆDeep Q-Networkï¼‰ã«ã‚ˆã‚‹Qå€¤å­¦ç¿’
# ================================================================================


class DQNCQCNN(nn.Module):
    """DQNãƒ™ãƒ¼ã‚¹ã®Qå€¤æ¨å®šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, n_qubits: int = 8, n_layers: int = 3):
        super().__init__()

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self.n_qubits = n_qubits
        self.n_layers = n_layers

        # ç‰¹å¾´æŠ½å‡ºéƒ¨ï¼ˆCNN + é‡å­å›è·¯ï¼‰
        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)  # 4ch: board + estimations
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.quantum_layer = RLQuantumCircuitLayer(n_qubits, n_layers)

        # Qå€¤è¨ˆç®—éƒ¨
        self.fc1 = nn.Linear(64 * 6 * 5 + n_qubits, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 240)  # 6x5x8 = 240 (å…¨ä½ç½®ãƒ»å…¨æ–¹å‘ã®Qå€¤)

        self.dropout = nn.Dropout(0.2)

    def forward(self, state: torch.Tensor, training: bool = True) -> torch.Tensor:
        """
        çŠ¶æ…‹ã‹ã‚‰Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ

        Args:
            state: çŠ¶æ…‹ãƒ†ãƒ³ã‚½ãƒ« (batch_size, 4, 6, 5)
            training: å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹

        Returns:
            Qå€¤ãƒãƒƒãƒ— (batch_size, 240)
        """
        # CNNç‰¹å¾´æŠ½å‡º
        x = F.relu(self.conv1(state))
        x = F.relu(self.conv2(x))
        cnn_features = x.flatten(start_dim=1)

        # é‡å­å›è·¯å‡¦ç†
        quantum_input = cnn_features[:, : self.n_qubits]
        quantum_features = self.quantum_layer(quantum_input, training)

        # çµåˆ
        combined = torch.cat([cnn_features, quantum_features], dim=1)

        # Qå€¤è¨ˆç®—
        x = F.relu(self.fc1(combined))
        if training:
            x = self.dropout(x)
        x = F.relu(self.fc2(x))
        if training:
            x = self.dropout(x)
        q_values = self.fc3(x)

        return q_values


# ================================================================================
# Part 5: çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡
# ================================================================================


class ExperienceReplayBuffer:
    """å„ªå…ˆåº¦ä»˜ãçµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡"""

    def __init__(self, capacity: int = 10000, alpha: float = 0.6):
        self.capacity = capacity
        self.alpha = alpha  # å„ªå…ˆåº¦ã®é‡ã¿
        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
        self.position = 0

    def push(self, experience: Experience, priority: float = None):
        """çµŒé¨“ã‚’è¿½åŠ """
        if priority is None:
            priority = max(self.priorities, default=1.0)

        self.buffer.append(experience)
        self.priorities.append(priority)

    def sample(self, batch_size: int, beta: float = 0.4) -> Tuple[List[Experience], torch.Tensor]:
        """å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        if len(self.buffer) < batch_size:
            return list(self.buffer), torch.ones(len(self.buffer))

        # å„ªå…ˆåº¦ã‚’ç¢ºç‡ã«å¤‰æ›
        priorities = np.array(self.priorities) ** self.alpha
        probs = priorities / priorities.sum()

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        experiences = [self.buffer[idx] for idx in indices]

        # é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é‡ã¿
        weights = (len(self.buffer) * probs[indices]) ** (-beta)
        weights /= weights.max()

        return experiences, torch.tensor(weights, dtype=torch.float32)

    def update_priorities(self, indices: List[int], priorities: List[float]):
        """å„ªå…ˆåº¦ã‚’æ›´æ–°"""
        for idx, priority in zip(indices, priorities):
            if 0 <= idx < len(self.priorities):
                self.priorities[idx] = priority

    def __len__(self):
        return len(self.buffer)


# ================================================================================
# Part 6: å¼·åŒ–å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
# ================================================================================


class RLCQCNNTrainer:
    """å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹CQCNNå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config: RLConfig = None):
        self.config = config or RLConfig()

        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.estimator = RLCQCNNEstimator()
        self.q_network = DQNCQCNN()
        self.target_q_network = DQNCQCNN()
        self.target_q_network.load_state_dict(self.q_network.state_dict())

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.estimator_optimizer = optim.Adam(self.estimator.parameters(), lr=self.config.learning_rate)
        self.q_optimizer = optim.Adam(self.q_network.parameters(), lr=self.config.learning_rate)

        # ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡
        self.replay_buffer = ExperienceReplayBuffer(self.config.buffer_size)

        # å­¦ç¿’çŠ¶æ…‹
        self.epsilon = self.config.epsilon_start
        self.steps = 0
        self.episodes = 0

        # çµ±è¨ˆæƒ…å ±
        self.training_stats = {
            "episode_rewards": [],
            "episode_lengths": [],
            "loss_history": [],
            "win_rate": deque(maxlen=100),
            "estimation_accuracy": deque(maxlen=100),
        }

    def select_action(self, state: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        """Îµ-greedyè¡Œå‹•é¸æŠ"""
        if random.random() < self.epsilon:
            return random.choice(legal_moves)

        # Qå€¤ã«åŸºã¥ãé¸æŠ
        with torch.no_grad():
            state_tensor = self._prepare_state_tensor(state)
            q_values = self.q_network(state_tensor, training=False)
            q_map = q_values.reshape(6, 5, 8)

            best_move = None
            best_q = -float("inf")

            for move in legal_moves:
                from_pos, to_pos = move[:2]
                dir_idx = self._get_direction_index(from_pos, to_pos)
                if dir_idx is not None:
                    q = q_map[from_pos[1], from_pos[0], dir_idx].item()
                    if q > best_q:
                        best_q = q
                        best_move = move

            return best_move if best_move else random.choice(legal_moves)

    def train_step(self, batch_size: int = None) -> float:
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’"""
        if len(self.replay_buffer) < (batch_size or self.config.batch_size):
            return 0.0

        batch_size = batch_size or self.config.batch_size
        experiences, weights = self.replay_buffer.sample(batch_size)

        # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        states = torch.stack([self._prepare_state_tensor(e.state) for e in experiences])
        actions = torch.tensor([self._action_to_index(e.action) for e in experiences])
        rewards = torch.tensor([e.reward for e in experiences], dtype=torch.float32)
        next_states = torch.stack(
            [
                self._prepare_state_tensor(e.next_state) if e.next_state is not None else torch.zeros_like(states[0])
                for e in experiences
            ]
        )
        dones = torch.tensor([e.done for e in experiences], dtype=torch.float32)

        # ç¾åœ¨ã®Qå€¤
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

        # æ¬¡ã®çŠ¶æ…‹ã®æœ€å¤§Qå€¤ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½¿ç”¨ï¼‰
        with torch.no_grad():
            next_q_values = self.target_q_network(next_states, training=False).max(1)[0]
            target_q_values = rewards + (1 - dones) * self.config.gamma * next_q_values

        # æå¤±è¨ˆç®—ï¼ˆé‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é‡ã¿ã‚’é©ç”¨ï¼‰
        loss = (weights * F.smooth_l1_loss(current_q_values.squeeze(), target_q_values, reduction="none")).mean()

        # æœ€é©åŒ–
        self.q_optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.q_optimizer.step()

        # å„ªå…ˆåº¦æ›´æ–°
        with torch.no_grad():
            td_errors = torch.abs(current_q_values.squeeze() - target_q_values)
            td_errors.cpu().numpy() + 1e-6
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥

        self.steps += 1

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
        if self.steps % self.config.update_target_every == 0:
            self.target_q_network.load_state_dict(self.q_network.state_dict())

        return loss.item()

    def train_estimator(self, experiences: List[Experience]) -> float:
        """æ¨å®šå™¨ã®å­¦ç¿’"""
        if not experiences:
            return 0.0

        total_loss = 0.0

        for exp in experiences:
            if not exp.enemy_positions:
                continue

            # çœŸã®é§’ã‚¿ã‚¤ãƒ—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ä»®å®šï¼‰
            true_types = self._get_true_piece_types(exp)

            for pos in exp.enemy_positions:
                # æ¨å®š
                state_tensor = self._prepare_state_tensor(exp.state)
                estimation = self.estimator(state_tensor.unsqueeze(0), pos)

                # æå¤±è¨ˆç®—
                target = torch.tensor(true_types[pos], dtype=torch.float32)
                loss = F.cross_entropy(estimation, target.unsqueeze(0))

                # æœ€é©åŒ–
                self.estimator_optimizer.zero_grad()
                loss.backward()
                self.estimator_optimizer.step()

                total_loss += loss.item()

        return total_loss / max(len(experiences), 1)

    def episode_end(self, experiences: List[Experience], reward: float, won: bool):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å‡¦ç†"""
        # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        for exp in experiences:
            self.replay_buffer.push(exp)

        # å­¦ç¿’
        if len(self.replay_buffer) >= self.config.batch_size:
            for _ in range(min(10, len(experiences))):
                self.train_step()

        # æ¨å®šå™¨å­¦ç¿’
        self.train_estimator(experiences)

        # Îµæ¸›è¡°
        self.epsilon = max(self.config.epsilon_end, self.epsilon * self.config.epsilon_decay)

        # çµ±è¨ˆè¨˜éŒ²
        self.episodes += 1
        self.training_stats["episode_rewards"].append(reward)
        self.training_stats["episode_lengths"].append(len(experiences))
        self.training_stats["win_rate"].append(1.0 if won else 0.0)

    def _prepare_state_tensor(self, state: np.ndarray) -> torch.Tensor:
        """çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
        if isinstance(state, torch.Tensor):
            return state
        return torch.tensor(state, dtype=torch.float32)

    def _action_to_index(self, action: Tuple) -> int:
        """è¡Œå‹•ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›"""
        from_pos, to_pos = action[:2]
        dir_idx = self._get_direction_index(from_pos, to_pos)
        return from_pos[1] * 40 + from_pos[0] * 8 + (dir_idx or 0)

    def _get_direction_index(self, from_pos: Tuple, to_pos: Tuple) -> Optional[int]:
        """æ–¹å‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—"""
        dx = to_pos[0] - from_pos[0]
        dy = to_pos[1] - from_pos[1]

        directions = [(0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1)]

        try:
            return directions.index((dx, dy))
        except ValueError:
            return None

    def _get_true_piece_types(self, experience: Experience) -> Dict:
        """çœŸã®é§’ã‚¿ã‚¤ãƒ—ã‚’å–å¾—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰"""
        # å®Ÿéš›ã®ã‚²ãƒ¼ãƒ ã§ã¯çœŸã®ã‚¿ã‚¤ãƒ—ãŒåˆ†ã‹ã‚‹ãŒã€
        # ã“ã“ã§ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆ
        types = {}
        piece_types = [0, 1, 2, 3, 4, 5]  # P, K, Q, R, B, N

        for pos in experience.enemy_positions:
            # æ¨å®šå€¤ã«åŸºã¥ã„ã¦æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ã‚¿ã‚¤ãƒ—ã‚’çœŸå€¤ã¨ã™ã‚‹ï¼ˆç°¡ç•¥åŒ–ï¼‰
            if pos in experience.estimations:
                probs = list(experience.estimations[pos].values())
                types[pos] = np.argmax(probs)
            else:
                types[pos] = random.choice(piece_types)

        return types

    def save_checkpoint(self, filepath: str):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        torch.save(
            {
                "estimator_state": self.estimator.state_dict(),
                "q_network_state": self.q_network.state_dict(),
                "target_q_network_state": self.target_q_network.state_dict(),
                "estimator_optimizer": self.estimator_optimizer.state_dict(),
                "q_optimizer": self.q_optimizer.state_dict(),
                "epsilon": self.epsilon,
                "steps": self.steps,
                "episodes": self.episodes,
                "training_stats": self.training_stats,
            },
            filepath,
        )
        print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {filepath}")

    def load_checkpoint(self, filepath: str):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        checkpoint = torch.load(filepath)
        self.estimator.load_state_dict(checkpoint["estimator_state"])
        self.q_network.load_state_dict(checkpoint["q_network_state"])
        self.target_q_network.load_state_dict(checkpoint["target_q_network_state"])
        self.estimator_optimizer.load_state_dict(checkpoint["estimator_optimizer"])
        self.q_optimizer.load_state_dict(checkpoint["q_optimizer"])
        self.epsilon = checkpoint["epsilon"]
        self.steps = checkpoint["steps"]
        self.episodes = checkpoint["episodes"]
        self.training_stats = checkpoint["training_stats"]
        print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {filepath}")


# ================================================================================
# Part 7: è‡ªå·±å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================


class SelfPlayAgent:
    """è‡ªå·±å¯¾æˆ¦ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self, trainer: RLCQCNNTrainer, player_id: str):
        self.trainer = trainer
        self.player_id = player_id
        self.experiences = []

    def get_move(self, state: np.ndarray, legal_moves: List[Tuple]) -> Tuple:
        """æ‰‹ã‚’é¸æŠ"""
        return self.trainer.select_action(state, legal_moves)

    def record_experience(
        self,
        state: np.ndarray,
        action: Tuple,
        reward: float,
        next_state: Optional[np.ndarray],
        done: bool,
        enemy_positions: List,
        estimations: Dict,
        q_map: np.ndarray,
    ):
        """çµŒé¨“ã‚’è¨˜éŒ²"""
        exp = Experience(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            enemy_positions=enemy_positions,
            estimations=estimations,
            q_map=q_map,
        )
        self.experiences.append(exp)


class SelfPlaySystem:
    """è‡ªå·±å¯¾æˆ¦ã«ã‚ˆã‚‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, trainer: RLCQCNNTrainer):
        self.trainer = trainer
        self.board_size = (6, 5)
        self.max_turns = 100

    def play_episode(self) -> Dict:
        """1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è‡ªå·±å¯¾æˆ¦"""
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        agent1 = SelfPlayAgent(self.trainer, "A")
        agent2 = SelfPlayAgent(self.trainer, "B")

        # ã‚²ãƒ¼ãƒ çŠ¶æ…‹åˆæœŸåŒ–
        board = self._initialize_board()
        turn = 0

        # ã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ—
        while turn < self.max_turns:
            turn += 1

            # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼1ã®æ‰‹ç•ª
            legal_moves1 = self._get_legal_moves(board, "A")
            if legal_moves1:
                state1 = self._board_to_state(board, "A")
                move1 = agent1.get_move(state1, legal_moves1)

                # å ±é…¬è¨ˆç®—
                reward1 = self._calculate_reward(board, move1, "A")

                # ãƒœãƒ¼ãƒ‰æ›´æ–°
                board = self._apply_move(board, move1, "A")

                # çµŒé¨“è¨˜éŒ²
                next_state1 = self._board_to_state(board, "A")
                agent1.record_experience(
                    state1,
                    move1,
                    reward1,
                    next_state1,
                    False,
                    self._get_enemy_positions(board, "A"),
                    {},
                    np.zeros((6, 5, 8)),
                )

            # å‹æ•—åˆ¤å®š
            winner = self._check_winner(board)
            if winner:
                break

            # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼2ã®æ‰‹ç•ªï¼ˆåŒæ§˜ã®å‡¦ç†ï¼‰
            legal_moves2 = self._get_legal_moves(board, "B")
            if legal_moves2:
                state2 = self._board_to_state(board, "B")
                move2 = agent2.get_move(state2, legal_moves2)
                reward2 = self._calculate_reward(board, move2, "B")
                board = self._apply_move(board, move2, "B")
                next_state2 = self._board_to_state(board, "B")
                agent2.record_experience(
                    state2,
                    move2,
                    reward2,
                    next_state2,
                    False,
                    self._get_enemy_positions(board, "B"),
                    {},
                    np.zeros((6, 5, 8)),
                )

            winner = self._check_winner(board)
            if winner:
                break

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å‡¦ç†
        if winner == "A":
            final_reward1 = self.trainer.config.reward_win
            final_reward2 = self.trainer.config.reward_lose
        elif winner == "B":
            final_reward1 = self.trainer.config.reward_lose
            final_reward2 = self.trainer.config.reward_win
        else:
            final_reward1 = self.trainer.config.reward_draw
            final_reward2 = self.trainer.config.reward_draw

        # æœ€çµ‚å ±é…¬ã‚’è¨˜éŒ²
        if agent1.experiences:
            agent1.experiences[-1] = agent1.experiences[-1]._replace(
                reward=agent1.experiences[-1].reward + final_reward1, done=True
            )
        if agent2.experiences:
            agent2.experiences[-1] = agent2.experiences[-1]._replace(
                reward=agent2.experiences[-1].reward + final_reward2, done=True
            )

        # å­¦ç¿’
        all_experiences = agent1.experiences + agent2.experiences
        total_reward = sum(exp.reward for exp in all_experiences)

        self.trainer.episode_end(all_experiences, total_reward, winner == "A")

        return {"winner": winner, "turns": turn, "total_reward": total_reward, "experiences": len(all_experiences)}

    def _initialize_board(self) -> np.ndarray:
        """ãƒœãƒ¼ãƒ‰åˆæœŸåŒ–"""
        board = np.zeros(self.board_size)
        # ç°¡ç•¥åŒ–ã®ãŸã‚ã€ãƒ©ãƒ³ãƒ€ãƒ ã«é§’ã‚’é…ç½®
        for i in range(5):
            board[0, i] = 1  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼A
            board[5, i] = -1  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼B
        return board

    def _board_to_state(self, board: np.ndarray, player: str) -> np.ndarray:
        """ãƒœãƒ¼ãƒ‰ã‚’çŠ¶æ…‹ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
        state = np.zeros((4, 6, 5))
        player_val = 1 if player == "A" else -1

        # ãƒãƒ£ãƒ³ãƒãƒ«0: è‡ªåˆ†ã®é§’
        state[0] = (board == player_val).astype(float)
        # ãƒãƒ£ãƒ³ãƒãƒ«1: æ•µã®é§’
        state[1] = (board == -player_val).astype(float)
        # ãƒãƒ£ãƒ³ãƒãƒ«2: ç©ºããƒã‚¹
        state[2] = (board == 0).astype(float)
        # ãƒãƒ£ãƒ³ãƒãƒ«3: ãƒœãƒ¼ãƒ‰å…¨ä½“
        state[3] = board / 2 + 0.5

        return state

    def _get_legal_moves(self, board: np.ndarray, player: str) -> List[Tuple]:
        """åˆæ³•æ‰‹ã‚’å–å¾—"""
        moves = []
        player_val = 1 if player == "A" else -1

        for y in range(6):
            for x in range(5):
                if board[y, x] == player_val:
                    # 8æ–¹å‘ã®ç§»å‹•ã‚’ç¢ºèª
                    for dx, dy in [(0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1)]:
                        nx, ny = x + dx, y + dy
                        if 0 <= nx < 5 and 0 <= ny < 6:
                            if board[ny, nx] != player_val:
                                moves.append(((x, y), (nx, ny)))

        return moves

    def _apply_move(self, board: np.ndarray, move: Tuple, player: str) -> np.ndarray:
        """æ‰‹ã‚’é©ç”¨"""
        new_board = board.copy()
        from_pos, to_pos = move[:2]
        player_val = 1 if player == "A" else -1

        new_board[from_pos[1], from_pos[0]] = 0
        new_board[to_pos[1], to_pos[0]] = player_val

        return new_board

    def _calculate_reward(self, board: np.ndarray, move: Tuple, player: str) -> float:
        """å ±é…¬ã‚’è¨ˆç®—"""
        reward = self.trainer.config.reward_move

        from_pos, to_pos = move[:2]
        player_val = 1 if player == "A" else -1

        # æ•µé§’ã‚’å–ã£ãŸå ´åˆ
        if board[to_pos[1], to_pos[0]] == -player_val:
            reward += self.trainer.config.reward_capture

        # ä¸­å¤®ã¸ã®ç§»å‹•ã‚’è©•ä¾¡
        center_x, center_y = 2, 3
        dist_before = abs(from_pos[0] - center_x) + abs(from_pos[1] - center_y)
        dist_after = abs(to_pos[0] - center_x) + abs(to_pos[1] - center_y)
        reward += (dist_before - dist_after) * 0.1

        return reward

    def _check_winner(self, board: np.ndarray) -> Optional[str]:
        """å‹è€…ã‚’åˆ¤å®š"""
        # ç°¡ç•¥åŒ–: ç‰‡æ–¹ã®é§’ãŒå…¨ã¦ãªããªã£ãŸã‚‰è² ã‘
        has_a = np.any(board == 1)
        has_b = np.any(board == -1)

        if not has_b:
            return "A"
        elif not has_a:
            return "B"
        else:
            return None

    def _get_enemy_positions(self, board: np.ndarray, player: str) -> List[Tuple]:
        """æ•µé§’ã®ä½ç½®ã‚’å–å¾—"""
        enemy_val = -1 if player == "A" else 1
        positions = []

        for y in range(6):
            for x in range(5):
                if board[y, x] == enemy_val:
                    positions.append((x, y))

        return positions


# ================================================================================
# Part 8: å­¦ç¿’å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================


class RLCQCNNRunner:
    """å¼·åŒ–å­¦ç¿’ç‰ˆCQCNNå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.config = RLConfig()
        self.trainer = RLCQCNNTrainer(self.config)
        self.self_play = SelfPlaySystem(self.trainer)

    def train(self, num_episodes: int = 1000, save_interval: int = 100):
        """å­¦ç¿’ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ å¼·åŒ–å­¦ç¿’é–‹å§‹")
        print("=" * 70)
        print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {num_episodes}")
        print(f"ä¿å­˜é–“éš”: {save_interval}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        print(f"åˆæœŸÎµ: {self.config.epsilon_start}")
        print(f"æœ€çµ‚Îµ: {self.config.epsilon_end}")
        print("=" * 70)

        for episode in range(num_episodes):
            # è‡ªå·±å¯¾æˆ¦
            self.self_play.play_episode()

            # é€²æ—è¡¨ç¤º
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(self.trainer.training_stats["episode_rewards"][-10:])
                avg_length = np.mean(self.trainer.training_stats["episode_lengths"][-10:])
                win_rate = np.mean(list(self.trainer.training_stats["win_rate"]))

                print(
                    f"Episode {episode + 1}/{num_episodes} | "
                    f"Reward: {avg_reward:.2f} | "
                    f"Length: {avg_length:.1f} | "
                    f"Win Rate: {win_rate:.2%} | "
                    f"Îµ: {self.trainer.epsilon:.3f}"
                )

            # å®šæœŸä¿å­˜
            if (episode + 1) % save_interval == 0:
                self.trainer.save_checkpoint(f"rl_cqcnn_ep{episode + 1}.pth")

        print("\nâœ… å­¦ç¿’å®Œäº†ï¼")

        # æœ€çµ‚ä¿å­˜
        self.trainer.save_checkpoint("rl_cqcnn_final.pth")

        # å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        self.plot_training_curves()

    def plot_training_curves(self):
        """å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        stats = self.trainer.training_stats

        if not stats["episode_rewards"]:
            print("âš ï¸ ãƒ—ãƒ­ãƒƒãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        fig, axes = plt.subplots(2, 2, figsize=(12, 8))

        # å ±é…¬æ¨ç§»
        axes[0, 0].plot(stats["episode_rewards"])
        axes[0, 0].set_title("Episode Rewards")
        axes[0, 0].set_xlabel("Episode")
        axes[0, 0].set_ylabel("Total Reward")

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·
        axes[0, 1].plot(stats["episode_lengths"])
        axes[0, 1].set_title("Episode Lengths")
        axes[0, 1].set_xlabel("Episode")
        axes[0, 1].set_ylabel("Number of Steps")

        # å‹ç‡ï¼ˆç§»å‹•å¹³å‡ï¼‰
        win_rates = list(stats["win_rate"])
        if win_rates:
            axes[1, 0].plot(win_rates)
            axes[1, 0].set_title("Win Rate (Moving Average)")
            axes[1, 0].set_xlabel("Episode")
            axes[1, 0].set_ylabel("Win Rate")

        # æå¤±æ¨ç§»
        if stats["loss_history"]:
            axes[1, 1].plot(stats["loss_history"])
            axes[1, 1].set_title("Training Loss")
            axes[1, 1].set_xlabel("Training Step")
            axes[1, 1].set_ylabel("Loss")

        plt.tight_layout()
        plt.savefig("rl_training_curves.png")
        print("ğŸ“Š å­¦ç¿’æ›²ç·šã‚’ä¿å­˜: rl_training_curves.png")

    def evaluate(self, num_games: int = 100) -> Dict:
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
        print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ({num_games}ã‚²ãƒ¼ãƒ )")

        wins = 0
        total_rewards = []

        # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆÎµã‚’0ã«ï¼‰
        original_epsilon = self.trainer.epsilon
        self.trainer.epsilon = 0.0

        for game in range(num_games):
            result = self.self_play.play_episode()

            if result["winner"] == "A":
                wins += 1
            total_rewards.append(result["total_reward"])

            if (game + 1) % 20 == 0:
                print(f"  è©•ä¾¡é€²æ—: {game + 1}/{num_games}")

        # Îµã‚’å…ƒã«æˆ»ã™
        self.trainer.epsilon = original_epsilon

        results = {
            "win_rate": wins / num_games,
            "avg_reward": np.mean(total_rewards),
            "std_reward": np.std(total_rewards),
            "max_reward": np.max(total_rewards),
            "min_reward": np.min(total_rewards),
        }

        print("\nè©•ä¾¡çµæœ:")
        print(f"  å‹ç‡: {results['win_rate']:.2%}")
        print(f"  å¹³å‡å ±é…¬: {results['avg_reward']:.2f} Â± {results['std_reward']:.2f}")
        print(f"  æœ€å¤§å ±é…¬: {results['max_reward']:.2f}")
        print(f"  æœ€å°å ±é…¬: {results['min_reward']:.2f}")

        return results


# ================================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ================================================================================


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 70)
    print("ğŸ¤– å¼·åŒ–å­¦ç¿’ç‰ˆCQCNNç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)

    runner = RLCQCNNRunner()

    while True:
        print("\nã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€‘")
        print("1. æ–°è¦å­¦ç¿’é–‹å§‹")
        print("2. å­¦ç¿’å†é–‹ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ï¼‰")
        print("3. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
        print("4. ãƒ‡ãƒ¢å¯¾æˆ¦")
        print("0. çµ‚äº†")

        choice = input("\né¸æŠ (0-4): ").strip()

        if choice == "0":
            break

        elif choice == "1":
            episodes = int(input("å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° (100-10000): ") or "1000")
            runner.train(num_episodes=episodes)

        elif choice == "2":
            checkpoint = input("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å: ").strip()
            try:
                runner.trainer.load_checkpoint(checkpoint)
                episodes = int(input("è¿½åŠ å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: ") or "500")
                runner.train(num_episodes=episodes)
            except FileNotFoundError:
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint}")

        elif choice == "3":
            games = int(input("è©•ä¾¡ã‚²ãƒ¼ãƒ æ•° (10-1000): ") or "100")
            runner.evaluate(num_games=games)

        elif choice == "4":
            print("\nğŸ® ãƒ‡ãƒ¢å¯¾æˆ¦")
            result = runner.self_play.play_episode()
            print(f"å‹è€…: {result['winner'] or 'å¼•ãåˆ†ã‘'}")
            print(f"ã‚¿ãƒ¼ãƒ³æ•°: {result['turns']}")
            print(f"ç·å ±é…¬: {result['total_reward']:.2f}")

    print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
