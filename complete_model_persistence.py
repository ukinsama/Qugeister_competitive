#!/usr/bin/env python3
"""
å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ 
å­¦ç¿’æ¸ˆã¿AIã®å®Œå…¨ãªæ°¸ç¶šåŒ–ã¨å¾©å…ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
import os
import pickle
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
import hashlib

@dataclass
class ModelMetadata:
    """ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    model_name: str
    model_type: str
    creation_date: str
    training_episodes: int
    final_reward: float
    architecture_hash: str
    version: str = "1.0.0"
    description: str = ""

class UniversalModelSaver:
    """æ±ç”¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»å¾©å…ƒã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, save_directory: str = "./saved_models"):
        self.save_directory = save_directory
        os.makedirs(save_directory, exist_ok=True)
        
    def calculate_architecture_hash(self, model: nn.Module) -> str:
        """ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—"""
        model_str = str(model)
        return hashlib.md5(model_str.encode()).hexdigest()[:16]
    
    def save_complete_model(self, 
                          model: nn.Module,
                          model_name: str,
                          optimizer: optim.Optimizer = None,
                          training_history: Dict = None,
                          metadata: Dict = None,
                          additional_data: Dict = None) -> str:
        """å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        
        print(f"ğŸ’¾ å®Œå…¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–‹å§‹: {model_name}")
        print("-" * 50)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = os.path.join(self.save_directory, f"{model_name}_{timestamp}")
        os.makedirs(save_path, exist_ok=True)
        
        # 1. ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ä¿å­˜
        model_file = os.path.join(save_path, "model.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_class': model.__class__.__name__,
            'model_module': model.__class__.__module__,
        }, model_file)
        print(f"  âœ… ãƒ¢ãƒ‡ãƒ«æœ¬ä½“: {model_file}")
        
        # 2. ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ä¿å­˜
        if optimizer:
            optimizer_file = os.path.join(save_path, "optimizer.pth")
            torch.save({
                'optimizer_state_dict': optimizer.state_dict(),
                'optimizer_class': optimizer.__class__.__name__,
                'optimizer_module': optimizer.__class__.__module__,
            }, optimizer_file)
            print(f"  âœ… ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {optimizer_file}")
        
        # 3. ãƒ¢ãƒ‡ãƒ«æ§‹é€ ä¿å­˜
        architecture_file = os.path.join(save_path, "architecture.txt")
        with open(architecture_file, 'w') as f:
            f.write(str(model))
        print(f"  âœ… ãƒ¢ãƒ‡ãƒ«æ§‹é€ : {architecture_file}")
        
        # 4. è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜
        if hasattr(model, 'get_config'):
            config_file = os.path.join(save_path, "config.json")
            with open(config_file, 'w') as f:
                json.dump(model.get_config(), f, indent=2)
            print(f"  âœ… è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {config_file}")
        
        # 5. å­¦ç¿’å±¥æ­´ä¿å­˜
        if training_history:
            history_file = os.path.join(save_path, "training_history.json")
            # numpyé…åˆ—ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
            history_clean = self._clean_for_json(training_history)
            with open(history_file, 'w') as f:
                json.dump(history_clean, f, indent=2)
            print(f"  âœ… å­¦ç¿’å±¥æ­´: {history_file}")
        
        # 6. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ»ä¿å­˜
        model_metadata = ModelMetadata(
            model_name=model_name,
            model_type=model.__class__.__name__,
            creation_date=datetime.now().isoformat(),
            training_episodes=training_history.get('episodes', 0) if training_history else 0,
            final_reward=training_history.get('final_reward', 0.0) if training_history else 0.0,
            architecture_hash=self.calculate_architecture_hash(model),
            description=metadata.get('description', '') if metadata else ''
        )
        
        metadata_file = os.path.join(save_path, "metadata.json")
        with open(metadata_file, 'w') as f:
            json.dump(asdict(model_metadata), f, indent=2)
        print(f"  âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_file}")
        
        # 7. è¿½åŠ ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        if additional_data:
            additional_file = os.path.join(save_path, "additional_data.pkl")
            with open(additional_file, 'wb') as f:
                pickle.dump(additional_data, f)
            print(f"  âœ… è¿½åŠ ãƒ‡ãƒ¼ã‚¿: {additional_file}")
        
        # 8. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        self._update_model_index(save_path, model_metadata)
        
        print(f"ğŸ‰ ä¿å­˜å®Œäº†: {save_path}")
        return save_path
    
    def _clean_for_json(self, obj):
        """JSONä¿å­˜ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if isinstance(obj, dict):
            return {k: self._clean_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._clean_for_json(v) for v in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        else:
            return obj
    
    def _update_model_index(self, save_path: str, metadata: ModelMetadata):
        """ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°"""
        index_file = os.path.join(self.save_directory, "model_index.json")
        
        # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿
        if os.path.exists(index_file):
            with open(index_file, 'r') as f:
                index = json.load(f)
        else:
            index = {"models": []}
        
        # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«è¿½åŠ 
        model_entry = asdict(metadata)
        model_entry["save_path"] = save_path
        index["models"].append(model_entry)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜
        with open(index_file, 'w') as f:
            json.dump(index, f, indent=2)
    
    def list_saved_models(self) -> List[Dict]:
        """ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§"""
        index_file = os.path.join(self.save_directory, "model_index.json")
        
        if not os.path.exists(index_file):
            return []
        
        with open(index_file, 'r') as f:
            index = json.load(f)
        
        return index.get("models", [])
    
    def load_complete_model(self, model_path: str) -> Dict[str, Any]:
        """å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“‚ å®Œå…¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")
        print("-" * 50)
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        
        loaded_data = {}
        
        # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        metadata_file = os.path.join(model_path, "metadata.json")
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r') as f:
                loaded_data['metadata'] = json.load(f)
            print(f"  âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
        
        # 2. ãƒ¢ãƒ‡ãƒ«æœ¬ä½“èª­ã¿è¾¼ã¿
        model_file = os.path.join(model_path, "model.pth")
        if os.path.exists(model_file):
            loaded_data['model_checkpoint'] = torch.load(model_file, map_location='cpu')
            print(f"  âœ… ãƒ¢ãƒ‡ãƒ«æœ¬ä½“èª­ã¿è¾¼ã¿å®Œäº†")
        
        # 3. ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶èª­ã¿è¾¼ã¿
        optimizer_file = os.path.join(model_path, "optimizer.pth")
        if os.path.exists(optimizer_file):
            loaded_data['optimizer_checkpoint'] = torch.load(optimizer_file, map_location='cpu')
            print(f"  âœ… ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶èª­ã¿è¾¼ã¿å®Œäº†")
        
        # 4. è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        config_file = os.path.join(model_path, "config.json")
        if os.path.exists(config_file):
            with open(config_file, 'r') as f:
                loaded_data['config'] = json.load(f)
            print(f"  âœ… è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
        
        # 5. å­¦ç¿’å±¥æ­´èª­ã¿è¾¼ã¿
        history_file = os.path.join(model_path, "training_history.json")
        if os.path.exists(history_file):
            with open(history_file, 'r') as f:
                loaded_data['training_history'] = json.load(f)
            print(f"  âœ… å­¦ç¿’å±¥æ­´èª­ã¿è¾¼ã¿å®Œäº†")
        
        # 6. ãƒ¢ãƒ‡ãƒ«æ§‹é€ èª­ã¿è¾¼ã¿
        architecture_file = os.path.join(model_path, "architecture.txt")
        if os.path.exists(architecture_file):
            with open(architecture_file, 'r') as f:
                loaded_data['architecture'] = f.read()
            print(f"  âœ… ãƒ¢ãƒ‡ãƒ«æ§‹é€ èª­ã¿è¾¼ã¿å®Œäº†")
        
        # 7. è¿½åŠ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        additional_file = os.path.join(model_path, "additional_data.pkl")
        if os.path.exists(additional_file):
            with open(additional_file, 'rb') as f:
                loaded_data['additional_data'] = pickle.load(f)
            print(f"  âœ… è¿½åŠ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
        
        print(f"ğŸ‰ èª­ã¿è¾¼ã¿å®Œäº†: {len(loaded_data)} å€‹ã®è¦ç´ ")
        return loaded_data
    
    def create_model_from_save(self, model_path: str, device: str = 'cpu') -> Tuple[nn.Module, Optional[optim.Optimizer]]:
        """ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’å¾©å…ƒ"""
        print(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«å¾©å…ƒ: {model_path}")
        print("-" * 50)
        
        loaded_data = self.load_complete_model(model_path)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã®å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        if 'model_checkpoint' not in loaded_data:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        model_checkpoint = loaded_data['model_checkpoint']
        
        # è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰
        if 'config' in loaded_data:
            config = loaded_data['config']
            # ã“ã“ã§å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã«å¿œã˜ã¦å‹•çš„ã«å¾©å…ƒ
            print(f"  ğŸ”§ è¨­å®šã‹ã‚‰ãƒ¢ãƒ‡ãƒ«å¾©å…ƒ: {config}")
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶å¾©å…ƒ
        optimizer = None
        if 'optimizer_checkpoint' in loaded_data:
            optimizer_checkpoint = loaded_data['optimizer_checkpoint']
            print(f"  âš™ï¸ ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶å¾©å…ƒ: {optimizer_checkpoint['optimizer_class']}")
        
        return None, None  # å®Ÿè£…ã¯å…·ä½“çš„ãªãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã«å¿œã˜ã¦èª¿æ•´
    
    def export_model_summary(self, model_path: str) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒªãƒ¼ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        loaded_data = self.load_complete_model(model_path)
        
        summary = []
        summary.append("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒªãƒ¼")
        summary.append("=" * 50)
        
        if 'metadata' in loaded_data:
            metadata = loaded_data['metadata']
            summary.append(f"ãƒ¢ãƒ‡ãƒ«å: {metadata['model_name']}")
            summary.append(f"ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥: {metadata['model_type']}")
            summary.append(f"ä½œæˆæ—¥æ™‚: {metadata['creation_date']}")
            summary.append(f"å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {metadata['training_episodes']}")
            summary.append(f"æœ€çµ‚å ±é…¬: {metadata['final_reward']}")
            summary.append(f"èª¬æ˜: {metadata['description']}")
        
        if 'training_history' in loaded_data:
            history = loaded_data['training_history']
            summary.append(f"\nğŸ“ˆ å­¦ç¿’å±¥æ­´:")
            for key, values in history.items():
                if isinstance(values, list) and values:
                    summary.append(f"  {key}: {len(values)} è¨˜éŒ²")
                    if all(isinstance(v, (int, float)) for v in values):
                        summary.append(f"    åˆæœŸ: {values[0]:.4f}, æœ€çµ‚: {values[-1]:.4f}")
        
        if 'architecture' in loaded_data:
            summary.append(f"\nğŸ—ï¸ ãƒ¢ãƒ‡ãƒ«æ§‹é€ :")
            arch_lines = loaded_data['architecture'].split('\n')[:10]
            for line in arch_lines:
                summary.append(f"  {line}")
            if len(loaded_data['architecture'].split('\n')) > 10:
                summary.append("  ...")
        
        return '\n'.join(summary)

class EnhancedCQCNNSaver(UniversalModelSaver):
    """CQCNNå°‚ç”¨ã®æ‹¡å¼µä¿å­˜ã‚·ã‚¹ãƒ†ãƒ """
    
    def save_cqcnn_model(self, 
                        estimator: nn.Module,
                        target: nn.Module,
                        optimizer: optim.Optimizer,
                        epsilon: float,
                        episodes: int,
                        training_history: Dict,
                        model_name: str) -> str:
        """CQCNNå¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å°‚ç”¨ä¿å­˜"""
        
        print(f"ğŸ§  CQCNNå¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_name}")
        print("-" * 50)
        
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿ï¼ˆCQCNNç‰¹æœ‰ï¼‰
        additional_data = {
            'estimator_state_dict': estimator.state_dict(),
            'target_state_dict': target.state_dict(),
            'epsilon': epsilon,
            'episodes': episodes,
            'model_type': 'CQCNN_RL',
            'quantum_params_shape': None,
            'conv_layers': None
        }
        
        # é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±æŠ½å‡º
        if hasattr(estimator, 'quantum_params'):
            additional_data['quantum_params_shape'] = list(estimator.quantum_params.shape)
            print(f"  ğŸŒŒ é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢çŠ¶: {additional_data['quantum_params_shape']}")
        
        # ç•³ã¿è¾¼ã¿å±¤æƒ…å ±æŠ½å‡º
        conv_info = []
        for name, module in estimator.named_modules():
            if isinstance(module, nn.Conv2d):
                conv_info.append({
                    'name': name,
                    'in_channels': module.in_channels,
                    'out_channels': module.out_channels,
                    'kernel_size': module.kernel_size,
                    'padding': module.padding
                })
        additional_data['conv_layers'] = conv_info
        print(f"  ğŸ” ç•³ã¿è¾¼ã¿å±¤: {len(conv_info)} å±¤")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            'description': f'CQCNNå¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ« (Îµ={epsilon:.4f}, {episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)',
            'epsilon': epsilon,
            'episodes': episodes,
            'has_target_network': True,
            'quantum_enabled': hasattr(estimator, 'quantum_params')
        }
        
        # å­¦ç¿’å±¥æ­´ã®æ‹¡å¼µ
        enhanced_history = training_history.copy()
        enhanced_history.update({
            'final_epsilon': epsilon,
            'total_episodes': episodes,
            'model_architecture': 'CQCNN_DQN'
        })
        
        # ä¿å­˜å®Ÿè¡Œ
        save_path = self.save_complete_model(
            model=estimator,
            model_name=model_name,
            optimizer=optimizer,
            training_history=enhanced_history,
            metadata=metadata,
            additional_data=additional_data
        )
        
        print(f"ğŸ‰ CQCNNä¿å­˜å®Œäº†: {save_path}")
        return save_path
    
    def load_cqcnn_model(self, model_path: str) -> Tuple[Dict, str]:
        """CQCNNå°‚ç”¨èª­ã¿è¾¼ã¿"""
        print(f"ğŸ§  CQCNNå°‚ç”¨èª­ã¿è¾¼ã¿: {model_path}")
        
        loaded_data = self.load_complete_model(model_path)
        
        if 'additional_data' not in loaded_data:
            raise ValueError("CQCNNãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        cqcnn_data = loaded_data['additional_data']
        
        # CQCNNç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        required_keys = ['estimator_state_dict', 'target_state_dict', 'epsilon', 'episodes']
        missing_keys = [key for key in required_keys if key not in cqcnn_data]
        
        if missing_keys:
            raise ValueError(f"å¿…è¦ãªCQCNNãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³: {missing_keys}")
        
        print(f"  âœ… CQCNNå¾©å…ƒæº–å‚™å®Œäº†")
        print(f"    æ¢ç´¢ç‡: {cqcnn_data['epsilon']:.6f}")
        print(f"    ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {cqcnn_data['episodes']}")
        
        if 'quantum_params_shape' in cqcnn_data:
            print(f"    é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {cqcnn_data['quantum_params_shape']}")
        
        if 'conv_layers' in cqcnn_data:
            print(f"    ç•³ã¿è¾¼ã¿å±¤: {len(cqcnn_data['conv_layers'])} å±¤")
        
        return loaded_data, model_path

def demo_model_persistence():
    """ãƒ¢ãƒ‡ãƒ«æ°¸ç¶šåŒ–ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸš€ å®Œå…¨ãƒ¢ãƒ‡ãƒ«æ°¸ç¶šåŒ–ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢")
    print("=" * 70)
    
    saver = UniversalModelSaver()
    
    # ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
    models = saver.list_saved_models()
    print(f"ğŸ“‚ ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: {len(models)} å€‹")
    
    if models:
        print("ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
        for i, model in enumerate(models[-5:]):  # æœ€æ–°5å€‹
            print(f"  {i+1}. {model['model_name']} ({model['creation_date']})")
    
    # å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
    rl_files = [f for f in os.listdir('.') if f.startswith('rl_') and f.endswith('.pth')]
    
    if rl_files:
        print(f"\nğŸ¯ æ—¢å­˜ã®å¼·åŒ–å­¦ç¿’ãƒ•ã‚¡ã‚¤ãƒ«: {len(rl_files)} å€‹")
        
        # CQCNNå°‚ç”¨ä¿å­˜ã‚·ã‚¹ãƒ†ãƒ 
        cqcnn_saver = EnhancedCQCNNSaver()
        
        print(f"\nğŸ§  CQCNNå°‚ç”¨ä¿å­˜ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†")
        print("æ¬¡å›ã®å¼·åŒ–å­¦ç¿’å®Ÿè¡Œæ™‚ã«è‡ªå‹•ã§å®Œå…¨ä¿å­˜ã•ã‚Œã¾ã™")
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
        for rl_file in rl_files:
            print(f"\nğŸ“Š åˆ†æ: {rl_file}")
            try:
                checkpoint = torch.load(rl_file, map_location='cpu')
                
                analysis = {
                    'file_size': f"{os.path.getsize(rl_file) / 1024 / 1024:.2f} MB",
                    'keys': list(checkpoint.keys()),
                    'episodes': checkpoint.get('episodes', 'N/A'),
                    'epsilon': checkpoint.get('epsilon', 'N/A')
                }
                
                print(f"  ã‚µã‚¤ã‚º: {analysis['file_size']}")
                print(f"  ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {analysis['episodes']}")
                print(f"  æ¢ç´¢ç‡: {analysis['epsilon']}")
                print(f"  ãƒ‡ãƒ¼ã‚¿è¦ç´ : {len(analysis['keys'])} å€‹")
                
                # å®Œå…¨ä¿å­˜å½¢å¼ã«å¤‰æ›å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                convertible = 'estimator_state' in checkpoint and 'target_state' in checkpoint
                print(f"  å®Œå…¨ä¿å­˜å¤‰æ›: {'âœ… å¯èƒ½' if convertible else 'âŒ è¦èª¿æ•´'}")
                
            except Exception as e:
                print(f"  âŒ åˆ†æå¤±æ•—: {e}")
    else:
        print("\nâš ï¸ å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("å¼·åŒ–å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ .pth ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:")
        print("  python rl_cqcnn_runner.py")
    
    print(f"\nğŸ‰ ãƒ¢ãƒ‡ãƒ«æ°¸ç¶šåŒ–ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†ï¼")
    print("ç‰¹å¾´:")
    print("  âœ… å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ä¿å­˜")
    print("  âœ… å­¦ç¿’å±¥æ­´ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜")
    print("  âœ… ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–")
    print("  âœ… CQCNNå°‚ç”¨æœ€é©åŒ–")
    print("  âœ… å¾©å…ƒæ™‚ã®æ¤œè¨¼æ©Ÿèƒ½")

if __name__ == "__main__":
    demo_model_persistence()