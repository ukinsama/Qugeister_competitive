#!/usr/bin/env python3
"""
å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - DQNå®Ÿè£…
"""

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from collections import deque
from typing import Dict, List, Tuple
from ..core.base_modules import LearningSystem
from ..core.game_state import LearningConfig, GameState
from ..modules.reward import RewardFactory


class DQNReinforcementLearning(LearningSystem):
    """DQNå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.memory = deque(maxlen=10000)
        self.training_history = []
        self.epsilon = 0.9
        self.target_model = None
    
    def train(
        self, 
        model: nn.Module, 
        training_data: List[Dict], 
        config: LearningConfig
    ) -> nn.Module:
        """å¼·åŒ–å­¦ç¿’ã‚’å®Ÿè¡Œ"""
        print(f"ğŸ® DQNå¼·åŒ–å­¦ç¿’é–‹å§‹: {config.rl_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        print(f"ğŸ”§ è¨­å®š: ãƒãƒƒãƒã‚µã‚¤ã‚º={config.batch_size}, å­¦ç¿’ç‡={config.learning_rate}")
        print(f"ğŸ¯ Îµè¨­å®š: {config.epsilon_start} â†’ {config.epsilon_end}")
        
        device = torch.device(config.device)
        model.to(device)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆ
        self.target_model = self._create_target_model(model, device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
        
        # å ±é…¬é–¢æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åŸºæœ¬å ±é…¬ï¼‰
        reward_function = RewardFactory.create_reward("basic")
        
        # Îµæ¸›è¡°è¨­å®š
        self.epsilon = config.epsilon_start
        epsilon_decay_rate = (config.epsilon_start - config.epsilon_end) / config.rl_episodes
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        for episode in range(config.rl_episodes):
            # ç’°å¢ƒãƒªã‚»ãƒƒãƒˆï¼ˆãƒ€ãƒŸãƒ¼ç’°å¢ƒï¼‰
            state = self._create_dummy_state()
            total_reward = 0
            steps = 0
            
            while steps < 50:  # æœ€å¤§50ã‚¹ãƒ†ãƒƒãƒ—
                # Îµ-greedyè¡Œå‹•é¸æŠ
                if random.random() < self.epsilon:
                    action = self._get_random_action(state)
                else:
                    action = self._get_best_action(model, state, device)
                
                # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
                next_state, reward, done = self._dummy_step(state, action, reward_function)
                
                # çµŒé¨“ã‚’è¨˜æ†¶
                self.memory.append((state, action, reward, next_state, done))
                
                total_reward += reward
                state = next_state
                steps += 1
                
                # ãƒãƒƒãƒå­¦ç¿’
                if len(self.memory) > config.batch_size:
                    loss = self._replay_experience(model, optimizer, config, device)
                else:
                    loss = 0.0
                
                if done:
                    break
            
            # Îµæ¸›è¡°
            self.epsilon = max(config.epsilon_end, self.epsilon - epsilon_decay_rate)
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°
            if episode % config.target_update == 0:
                self._update_target_model(model)
            
            # å­¦ç¿’å±¥æ­´è¨˜éŒ²
            self.training_history.append({
                "episode": episode,
                "total_reward": total_reward,
                "epsilon": self.epsilon,
                "steps": steps,
                "loss": loss if 'loss' in locals() else 0.0
            })
            
            # é€²æ—è¡¨ç¤º
            if episode % 100 == 0:
                avg_reward = np.mean([h["total_reward"] for h in self.training_history[-100:]])
                print(f"Episode {episode}/{config.rl_episodes}: "
                      f"Avg Reward={avg_reward:.2f}, Îµ={self.epsilon:.3f}")
        
        print(f"ğŸ‰ å¼·åŒ–å­¦ç¿’å®Œäº†!")
        return model
    
    def evaluate(self, model: nn.Module, test_data: List[Dict]) -> Dict[str, float]:
        """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        device = next(model.parameters()).device
        model.eval()
        
        total_rewards = []
        total_steps = []
        
        # ãƒ†ã‚¹ãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
        for _ in range(10):  # 10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆ
            state = self._create_dummy_state()
            total_reward = 0
            steps = 0
            
            while steps < 50:
                with torch.no_grad():
                    action = self._get_best_action(model, state, device)
                
                reward_function = RewardFactory.create_reward("basic")
                next_state, reward, done = self._dummy_step(state, action, reward_function)
                
                total_reward += reward
                state = next_state
                steps += 1
                
                if done:
                    break
            
            total_rewards.append(total_reward)
            total_steps.append(steps)
        
        return {
            "avg_reward": np.mean(total_rewards),
            "avg_steps": np.mean(total_steps),
            "total_episodes": len(total_rewards)
        }
    
    def _create_target_model(self, model: nn.Module, device: torch.device) -> nn.Module:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        # ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿å¼•æ•°ã‚’å–å¾—ã—ã¦åŒã˜æ§‹æˆã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        if hasattr(model, 'n_qubits') and hasattr(model, 'n_layers'):
            target_model = type(model)(model.n_qubits, model.n_layers)
        else:
            target_model = type(model)()
        
        target_model.load_state_dict(model.state_dict())
        target_model.to(device)
        return target_model
    
    def _update_target_model(self, model: nn.Module):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        if self.target_model is not None:
            self.target_model.load_state_dict(model.state_dict())
    
    def _create_dummy_state(self) -> Dict:
        """ãƒ€ãƒŸãƒ¼çŠ¶æ…‹ä½œæˆ"""
        return {
            "board": np.random.randint(0, 5, (6, 6)),
            "player": random.choice(["A", "B"]),
            "my_pieces": {(i, j): random.choice(["good", "bad"]) 
                         for i in range(2) for j in range(2)},
            "turn": random.randint(1, 50)
        }
    
    def _get_random_action(self, state: Dict) -> Tuple:
        """ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•"""
        positions = list(state["my_pieces"].keys())
        if not positions:
            return ((0, 0), (0, 1))
        
        from_pos = random.choice(positions)
        directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]
        direction = random.choice(directions)
        to_pos = (from_pos[0] + direction[0], from_pos[1] + direction[1])
        
        # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
        to_pos = (max(0, min(5, to_pos[0])), max(0, min(5, to_pos[1])))
        
        return (from_pos, to_pos)
    
    def _get_best_action(self, model: nn.Module, state: Dict, device: torch.device) -> Tuple:
        """æœ€é©è¡Œå‹•é¸æŠ"""
        try:
            from ..core.data_processor import DataProcessor
            tensor = DataProcessor.prepare_7channel_tensor(
                state["board"], state["player"], state["my_pieces"], state["turn"]
            )
            tensor = tensor.to(device)
            
            with torch.no_grad():
                q_values = model(tensor)
                
            # Qå€¤ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®šï¼ˆç°¡ç•¥åŒ–ï¼‰
            if q_values.dim() == 2:
                action_idx = torch.argmax(q_values, dim=1).item()
            else:
                action_idx = 0
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰è¡Œå‹•ã«å¤‰æ›
            directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]
            positions = list(state["my_pieces"].keys())
            
            if positions:
                from_pos = positions[action_idx % len(positions)]
                direction = directions[action_idx % len(directions)]
                to_pos = (from_pos[0] + direction[0], from_pos[1] + direction[1])
                to_pos = (max(0, min(5, to_pos[0])), max(0, min(5, to_pos[1])))
                return (from_pos, to_pos)
        
        except Exception:
            pass
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return self._get_random_action(state)
    
    def _dummy_step(self, state: Dict, action: Tuple, reward_function) -> Tuple[Dict, float, bool]:
        """ãƒ€ãƒŸãƒ¼ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—"""
        # æ–°ã—ã„çŠ¶æ…‹ä½œæˆ
        next_state = state.copy()
        next_state["turn"] += 1
        
        # å ±é…¬è¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        reward = random.uniform(-1.0, 1.0)
        
        # çµ‚äº†åˆ¤å®š
        done = random.random() < 0.1 or next_state["turn"] > 50
        
        return next_state, reward, done
    
    def _replay_experience(
        self, 
        model: nn.Module, 
        optimizer: optim.Optimizer, 
        config: LearningConfig,
        device: torch.device
    ) -> float:
        """çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤"""
        if len(self.memory) < config.batch_size:
            return 0.0
        
        batch = random.sample(self.memory, config.batch_size)
        
        # ãƒãƒƒãƒã‚’æº–å‚™ï¼ˆç°¡ç•¥åŒ–ï¼‰
        batch_size = len(batch)
        dummy_inputs = torch.randn(batch_size, 7, 6, 6).to(device)
        dummy_targets = torch.randn(batch_size, 2).to(device)
        
        optimizer.zero_grad()
        outputs = model(dummy_inputs)
        
        # é©åˆ‡ãªãƒ­ã‚¹è¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        if outputs.shape == dummy_targets.shape:
            loss = nn.MSELoss()(outputs, dummy_targets)
        else:
            # å½¢çŠ¶ãŒåˆã‚ãªã„å ´åˆã®å‡¦ç†
            targets = torch.randint(0, outputs.size(1), (batch_size,)).to(device)
            loss = nn.CrossEntropyLoss()(outputs, targets)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        return loss.item()
    
    def get_training_history(self) -> List[Dict]:
        """å­¦ç¿’å±¥æ­´ã‚’å–å¾—"""
        return self.training_history