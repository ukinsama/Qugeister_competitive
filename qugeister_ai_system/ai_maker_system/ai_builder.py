#!/usr/bin/env python3
"""
AIãƒ“ãƒ«ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ±åˆã—ã¦AIã‚’ç”Ÿæˆ
"""

import torch
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# ã‚³ã‚¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from .core.base_modules import CQCNNModel
from .core.game_state import GameConfig, LearningConfig
from .core.data_processor import DataProcessor

# æ©Ÿèƒ½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from .modules.placement import PlacementFactory
from .modules.estimator import EstimatorFactory
from .modules.reward import RewardFactory
from .modules.qmap import QMapFactory
from .modules.action import ActionFactory

# å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
from .learning.supervised import SupervisedLearning
from .learning.reinforcement import DQNReinforcementLearning


class AIBuilder:
    """AIåˆ¶ä½œã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ã‚¤ãƒ³ãƒ“ãƒ«ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = "generated_ais"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.built_ais = []
        
        print("ğŸš€ AI Maker System åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def create_ai(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¨­å®šã«åŸºã¥ã„ã¦AIã‚’ä½œæˆ
        
        Args:
            config: AIè¨­å®šè¾æ›¸
                - name: AIå
                - placement: é…ç½®æˆ¦ç•¥è¨­å®š
                - estimator: æ¨å®šå™¨è¨­å®š
                - reward: å ±é…¬é–¢æ•°è¨­å®š
                - qmap: Qå€¤ãƒãƒƒãƒ—è¨­å®š
                - action: è¡Œå‹•é¸æŠè¨­å®š
                - learning: å­¦ç¿’è¨­å®š
        
        Returns:
            ä½œæˆã•ã‚ŒãŸAIæƒ…å ±
        """
        print(f"ğŸ”¨ AIä½œæˆé–‹å§‹: {config.get('name', 'UnknownAI')}")
        
        # 1. åŸºæœ¬è¨­å®š
        ai_name = config.get('name', f"AI_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        game_config = GameConfig()
        learning_config = LearningConfig()
        
        # å­¦ç¿’è¨­å®šã‚’æ›´æ–°
        if 'learning' in config:
            learning_cfg = config['learning']
            learning_config.learning_rate = learning_cfg.get('learning_rate', 0.001)
            learning_config.batch_size = learning_cfg.get('batch_size', 32)
            learning_config.supervised_epochs = learning_cfg.get('epochs', 100)
            learning_config.rl_episodes = learning_cfg.get('episodes', 1000)
        
        # 2. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
        modules = self._create_modules(config)
        
        # 3. ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model_config = config.get('model', {})
        n_qubits = model_config.get('n_qubits', 6)
        n_layers = model_config.get('n_layers', 3)
        model = CQCNNModel(n_qubits, n_layers)
        
        print(f"ğŸ§  CQCNNãƒ¢ãƒ‡ãƒ«ä½œæˆ: {n_qubits}é‡å­ãƒ“ãƒƒãƒˆ, {n_layers}å±¤")
        print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters())}")
        
        # 4. å­¦ç¿’å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        trained_model = None
        training_results = None
        
        if config.get('auto_train', False):
            print("ğŸ“ è‡ªå‹•å­¦ç¿’ã‚’é–‹å§‹...")
            trained_model, training_results = self._train_model(
                model, modules, learning_config, config
            )
        
        # 5. AIæƒ…å ±ä½œæˆ
        ai_info = {
            'name': ai_name,
            'creation_time': datetime.now().isoformat(),
            'config': config,
            'modules': {
                'placement': modules['placement'].get_name(),
                'estimator': modules['estimator'].get_name(),
                'reward': modules['reward'].get_name(),
                'qmap': modules['qmap'].get_name(),
                'action': modules['action'].get_name()
            },
            'model': {
                'class_name': model.__class__.__name__,
                'n_qubits': n_qubits,
                'n_layers': n_layers,
                'total_parameters': sum(p.numel() for p in model.parameters())
            },
            'training_results': training_results
        }
        
        # 6. ä¿å­˜
        ai_path = self._save_ai(ai_info, model if trained_model is None else trained_model, modules)
        ai_info['path'] = str(ai_path)
        
        self.built_ais.append(ai_info)
        
        print(f"âœ… AIä½œæˆå®Œäº†: {ai_name}")
        print(f"ğŸ’¾ ä¿å­˜å…ˆ: {ai_path}")
        
        return ai_info
    
    def _create_modules(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¨­å®šã‹ã‚‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆ"""
        modules = {}
        
        # 1. é…ç½®æˆ¦ç•¥
        placement_cfg = config.get('placement', {'type': 'standard'})
        strategy_type = placement_cfg.pop('type', 'standard')
        modules['placement'] = PlacementFactory.create_placement(strategy_type, **placement_cfg)
        
        # 2. æ¨å®šå™¨
        estimator_cfg = config.get('estimator', {'type': 'cqcnn'})
        if 'model' in config:
            estimator_cfg.update({
                'n_qubits': config['model'].get('n_qubits', 6),
                'n_layers': config['model'].get('n_layers', 3)
            })
        estimator_type = estimator_cfg.pop('type', 'cqcnn')
        modules['estimator'] = EstimatorFactory.create_estimator(estimator_type, **estimator_cfg)
        
        # 3. å ±é…¬é–¢æ•°
        reward_cfg = config.get('reward', {'type': 'basic'})
        reward_type = reward_cfg.pop('type', 'basic')
        modules['reward'] = RewardFactory.create_reward(reward_type, **reward_cfg)
        
        # 4. Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨
        qmap_cfg = config.get('qmap', {'type': 'simple'})
        qmap_type = qmap_cfg.pop('type', 'simple')
        modules['qmap'] = QMapFactory.create_qmap_generator(qmap_type, **qmap_cfg)
        
        # 5. è¡Œå‹•é¸æŠå™¨
        action_cfg = config.get('action', {'type': 'epsilon_greedy'})
        action_type = action_cfg.pop('type', 'epsilon_greedy')
        modules['action'] = ActionFactory.create_selector(action_type, **action_cfg)
        
        print(f"ğŸ“¦ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆå®Œäº†:")
        for module_type, module in modules.items():
            print(f"  - {module_type}: {module.get_name()}")
        
        return modules
    
    def _train_model(
        self, 
        model: torch.nn.Module, 
        modules: Dict, 
        learning_config: LearningConfig,
        config: Dict[str, Any]
    ):
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
        learning_type = config.get('learning', {}).get('type', 'supervised')
        
        # ãƒ€ãƒŸãƒ¼å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        training_data = self._generate_dummy_training_data(100)
        
        if learning_type == 'reinforcement':
            print("ğŸ® å¼·åŒ–å­¦ç¿’ã‚’å®Ÿè¡Œ...")
            learner = DQNReinforcementLearning()
        else:
            print("ğŸ“š æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’å®Ÿè¡Œ...")
            learner = SupervisedLearning()
        
        # å­¦ç¿’å®Ÿè¡Œ
        trained_model = learner.train(model, training_data, learning_config)
        
        # è©•ä¾¡
        test_data = self._generate_dummy_training_data(20)
        evaluation_results = learner.evaluate(trained_model, test_data)
        
        training_results = {
            'learning_type': learning_type,
            'training_history': learner.get_training_history()[-10:],  # æœ€å¾Œã®10ã‚¨ãƒãƒƒã‚¯ã®ã¿
            'evaluation': evaluation_results
        }
        
        return trained_model, training_results
    
    def _generate_dummy_training_data(self, num_samples: int) -> List[Dict]:
        """ãƒ€ãƒŸãƒ¼å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        import random
        import numpy as np
        
        data = []
        for _ in range(num_samples):
            sample = {
                'board': np.random.randint(0, 5, (6, 6)),
                'player': random.choice(['A', 'B']),
                'my_pieces': {
                    (random.randint(0, 5), random.randint(0, 5)): random.choice(['good', 'bad'])
                    for _ in range(random.randint(1, 4))
                },
                'turn': random.randint(1, 50),
                'true_labels': {
                    (random.randint(0, 5), random.randint(0, 5)): random.choice(['good', 'bad'])
                    for _ in range(random.randint(1, 3))
                }
            }
            data.append(sample)
        
        return data
    
    def _save_ai(self, ai_info: Dict, model: torch.nn.Module, modules: Dict) -> Path:
        """AIã‚’ä¿å­˜"""
        ai_dir = self.output_dir / ai_info['name']
        ai_dir.mkdir(exist_ok=True)
        
        # 1. AIæƒ…å ±ä¿å­˜
        info_path = ai_dir / "ai_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(ai_info, f, indent=2, ensure_ascii=False, default=str)
        
        # 2. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = ai_dir / "model.pth"
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_class': model.__class__.__name__,
            'model_config': ai_info['model']
        }, model_path)
        
        # 3. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šä¿å­˜
        modules_path = ai_dir / "modules.json"
        module_configs = {
            name: module.get_config() 
            for name, module in modules.items()
        }
        with open(modules_path, 'w', encoding='utf-8') as f:
            json.dump(module_configs, f, indent=2, ensure_ascii=False, default=str)
        
        # 4. å®Ÿè¡Œå¯èƒ½Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
        self._generate_executable_script(ai_dir, ai_info, modules)
        
        return ai_dir
    
    def _generate_executable_script(self, ai_dir: Path, ai_info: Dict, modules: Dict):
        """å®Ÿè¡Œå¯èƒ½ãªPythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        script_content = f'''#!/usr/bin/env python3
"""
è‡ªå‹•ç”ŸæˆAI: {ai_info['name']}
ä½œæˆæ—¥æ™‚: {ai_info['creation_time']}
AI Maker System ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆ
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple

# ===== CQCNNãƒ¢ãƒ‡ãƒ«å®šç¾© =====
class CQCNNModel(nn.Module):
    def __init__(self, n_qubits={ai_info['model']['n_qubits']}, n_layers={ai_info['model']['n_layers']}):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        
        # Classical CNNéƒ¨åˆ†
        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.3)
        
        # Quantum-inspiredéƒ¨åˆ†
        self.quantum_dim = n_qubits * n_layers
        self.quantum_linear = None
        self.quantum_layers = nn.ModuleList([
            nn.Linear(self.quantum_dim, self.quantum_dim) 
            for _ in range(n_layers)
        ])
        
        # å‡ºåŠ›å±¤
        self.classifier = nn.Sequential(
            nn.Linear(self.quantum_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 2),
        )
        
        self._initialize_linear_layers()
    
    def _initialize_linear_layers(self):
        dummy_input = torch.randn(1, 7, 6, 6)
        with torch.no_grad():
            x = torch.relu(self.conv1(dummy_input))
            x = self.pool(x)
            x = torch.relu(self.conv2(x))
            x = self.pool(x)
            x = torch.relu(self.conv3(x))
            
            flattened_size = x.view(x.size(0), -1).size(1)
            self.quantum_linear = nn.Linear(flattened_size, self.quantum_dim)
    
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = torch.relu(self.conv3(x))
        x = self.dropout(x)
        
        x = x.view(x.size(0), -1)
        x = torch.relu(self.quantum_linear(x))
        
        for quantum_layer in self.quantum_layers:
            x_new = quantum_layer(x)
            x = torch.nn.functional.normalize(x_new + x, dim=1)
        
        output = self.classifier(x)
        return output

# ===== AIå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ  =====
class {ai_info['name'].replace(' ', '')}AI:
    def __init__(self):
        self.name = "{ai_info['name']}"
        self.model = CQCNNModel()
        self.load_model()
        
        print(f"ğŸ¤– {{self.name}} åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {{sum(p.numel() for p in self.model.parameters())}}")
    
    def load_model(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            checkpoint = torch.load("model.pth", map_location='cpu')
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            print("âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {{e}}")
            print("ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
    
    def predict(self, board_state: np.ndarray, player: str, my_pieces: Dict, turn: int):
        """æ¨è«–å®Ÿè¡Œ"""
        # 7ãƒãƒ£ãƒ³ãƒãƒ«ãƒ†ãƒ³ã‚½ãƒ«æº–å‚™ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        tensor = torch.randn(1, 7, 6, 6)  # ãƒ€ãƒŸãƒ¼ãƒ†ãƒ³ã‚½ãƒ«
        
        with torch.no_grad():
            output = self.model(tensor)
            probabilities = torch.softmax(output, dim=1)
        
        return probabilities.numpy()[0]
    
    def get_action(self, game_state: Dict) -> Tuple:
        """è¡Œå‹•æ±ºå®š"""
        # æˆ¦ç•¥: {modules['action'].get_name()}
        predictions = self.predict(
            game_state.get('board', np.zeros((6, 6))),
            game_state.get('player', 'A'),
            game_state.get('my_pieces', {{}}),
            game_state.get('turn', 1)
        )
        
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸè¡Œå‹•é¸æŠ
        valid_actions = [(0, 0), (0, 1)]  # ãƒ€ãƒŸãƒ¼
        return valid_actions[0] if valid_actions else None

# ===== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ =====
if __name__ == "__main__":
    ai = {ai_info['name'].replace(' ', '')}AI()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_state = {{
        'board': np.random.randint(0, 5, (6, 6)),
        'player': 'A',
        'my_pieces': {{(0, 0): 'good', (0, 1): 'bad'}},
        'turn': 1
    }}
    
    action = ai.get_action(test_state)
    print(f"ğŸ¯ é¸æŠã•ã‚ŒãŸè¡Œå‹•: {{action}}")
    
    predictions = ai.predict(test_state['board'], test_state['player'], 
                           test_state['my_pieces'], test_state['turn'])
    print(f"ğŸ”® äºˆæ¸¬çµæœ: {{predictions}}")
    
    print(f"âœ… {{ai.name}} ãƒ†ã‚¹ãƒˆå®Œäº†!")
'''
        
        script_path = ai_dir / f"{ai_info['name'].replace(' ', '_')}_ai.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        print(f"ğŸ å®Ÿè¡Œå¯èƒ½ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ: {script_path}")
    
    def list_available_modules(self) -> Dict[str, List[str]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸€è¦§"""
        return {
            'placement': PlacementFactory.get_available_strategies(),
            'estimator': EstimatorFactory.get_available_estimators(),
            'reward': RewardFactory.get_available_rewards(),
            'qmap': QMapFactory.get_available_generators(),
            'action': ActionFactory.get_available_selectors()
        }
    
    def get_built_ais(self) -> List[Dict]:
        """ä½œæˆã•ã‚ŒãŸAIä¸€è¦§"""
        return self.built_ais
    
    def create_ai_from_3step_config(self, step_config: Dict) -> Dict[str, Any]:
        """3stepã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šã‹ã‚‰AIä½œæˆ"""
        # 3stepã®è¨­å®šã‚’ai_builderã®å½¢å¼ã«å¤‰æ›
        ai_config = {
            'name': f"3Step_{step_config.get('reward', 'balanced')}_{datetime.now().strftime('%H%M%S')}",
            'placement': {'type': step_config.get('placement', 'standard')},
            'estimator': {'type': 'cqcnn'},
            'reward': {'type': step_config.get('reward', 'basic')},
            'qmap': {'type': 'strategic', 'strategy': step_config.get('reward', 'balanced')},
            'action': {'type': step_config.get('action', 'epsilon_greedy')},
            'model': {
                'n_qubits': step_config.get('qubits', 6),
                'n_layers': step_config.get('layers', 3)
            },
            'learning': {
                'type': step_config.get('learningMethod', 'supervised'),
                'learning_rate': step_config.get('learningRate', 0.001),
                'batch_size': step_config.get('batchSize', 32),
                'epochs': step_config.get('episodes', 100)
            },
            'auto_train': False  # 3stepã§ã¯æ‰‹å‹•å­¦ç¿’
        }
        
        return self.create_ai(ai_config)


# ===== ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ =====
def demo_ai_creation():
    """AIä½œæˆã®ãƒ‡ãƒ¢"""
    builder = AIBuilder()
    
    print("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«:")
    modules = builder.list_available_modules()
    for module_type, options in modules.items():
        print(f"  {module_type}: {', '.join(options)}")
    
    print("\nğŸ”¨ ã‚µãƒ³ãƒ—ãƒ«AIä½œæˆ:")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¨­å®š
    configs = [
        {
            'name': 'AggressiveQuantumAI',
            'placement': {'type': 'aggressive'},
            'estimator': {'type': 'cqcnn', 'n_qubits': 8, 'n_layers': 4},
            'reward': {'type': 'aggressive'},
            'qmap': {'type': 'strategic', 'strategy': 'aggressive'},
            'action': {'type': 'epsilon_greedy', 'epsilon': 0.1},
            'model': {'n_qubits': 8, 'n_layers': 4},
            'learning': {'type': 'reinforcement', 'episodes': 500},
            'auto_train': True
        },
        {
            'name': 'DefensiveQuantumAI',
            'placement': {'type': 'defensive'},
            'estimator': {'type': 'cqcnn'},
            'reward': {'type': 'defensive'},
            'qmap': {'type': 'strategic', 'strategy': 'defensive'},
            'action': {'type': 'boltzmann', 'temperature': 0.8},
            'auto_train': False
        }
    ]
    
    for config in configs:
        ai_info = builder.create_ai(config)
        print(f"âœ… ä½œæˆå®Œäº†: {ai_info['name']}")
    
    print(f"\nğŸ“Š ä½œæˆã•ã‚ŒãŸAIæ•°: {len(builder.get_built_ais())}")


if __name__ == "__main__":
    demo_ai_creation()