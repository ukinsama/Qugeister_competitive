#!/usr/bin/env python3
"""
Mini2ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼çµ±åˆã‚·ã‚¹ãƒ†ãƒ ä¿®æ­£ç‰ˆ
Part 1, 2, 3ã‚’çµ±åˆã—ãŸå®Œå…¨ç‰ˆ
"""

import numpy as np
import random
import torch
import torch.nn as nn
import pennylane as qml
from pennylane import numpy as pnp
import json
import os
import time
import threading
from datetime import datetime
import argparse
from typing import List, Tuple, Optional, Dict
from collections import defaultdict

# Part 1ã‹ã‚‰ã®ã‚³ãƒ¼ãƒ‰çµ±åˆ
class Mini2GeisterGame:
    """2é§’ç‰ˆMiniã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.board_size = 4
        self.pieces_per_player = 2  # å–„ç‰1 + æ‚ªç‰1
        
        # è„±å‡ºå£ä½ç½®ï¼ˆ6x6ã¨ã®ç›¸å¯¾ä½ç½®ç¶­æŒï¼‰
        self.escape_positions = {
            'A': [(3, 0), (3, 3)],  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Aã®è„±å‡ºå£
            'B': [(0, 0), (0, 3)]   # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Bã®è„±å‡ºå£
        }
        
        # åˆæœŸé…ç½®ä½ç½®
        self.initial_positions = {
            'A': [(3, 1), (3, 2)],  # ä¸‹å´1è¡Œã®ä¸­å¤®2åˆ—
            'B': [(0, 1), (0, 2)]   # ä¸Šå´1è¡Œã®ä¸­å¤®2åˆ—
        }
        
        self.reset_game()
    
    def reset_game(self):
        """ã‚²ãƒ¼ãƒ ãƒªã‚»ãƒƒãƒˆ"""
        self.board = np.zeros((self.board_size, self.board_size), dtype=int)
        self.current_player = 'A'
        self.game_over = False
        self.winner = None
        self.win_reason = None
        self.move_count = 0
        
        # é§’ã®æƒ…å ±
        self.pieces = {
            'A': {'positions': [], 'types': []},  # types: 0=æ‚ªç‰, 1=å–„ç‰
            'B': {'positions': [], 'types': []}
        }
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«å–„æ‚ªã‚’æ±ºã‚ã¦é…ç½®
        self._setup_random_pieces()
    
    def _setup_random_pieces(self):
        """å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®2é§’ã«ãƒ©ãƒ³ãƒ€ãƒ ã§å–„æ‚ªå‰²ã‚Šå½“ã¦"""
        for player in ['A', 'B']:
            positions = self.initial_positions[player].copy()
            types = [0, 1]  # 0=æ‚ªç‰, 1=å–„ç‰
            random.shuffle(types)
            
            for i, pos in enumerate(positions):
                r, c = pos
                piece_id = len(self.pieces[player]['positions']) + 1
                if player == 'A':
                    self.board[r, c] = piece_id  # Aé§’: 1,2
                else:
                    self.board[r, c] = piece_id + 10  # Bé§’: 11,12
                
                self.pieces[player]['positions'].append(pos)
                self.pieces[player]['types'].append(types[i])
    
    def get_state_tensor(self, player_perspective='A'):
        """ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã§å–å¾—"""
        state = np.zeros((4, self.board_size, self.board_size), dtype=np.float32)
        
        my_pieces = self.pieces[player_perspective]['positions']
        opponent = 'B' if player_perspective == 'A' else 'A'
        opponent_pieces = self.pieces[opponent]['positions']
        
        # è‡ªåˆ†ã®é§’
        for pos in my_pieces:
            if pos:  # None ã§ãªã„å ´åˆ
                r, c = pos
                state[0, r, c] = 1.0
        
        # ç›¸æ‰‹ã®é§’
        for pos in opponent_pieces:
            if pos:
                r, c = pos
                state[1, r, c] = 1.0
        
        # è„±å‡ºå£
        for r, c in self.escape_positions[player_perspective]:
            state[2, r, c] = 1.0
        
        # ç©ºããƒã‚¹
        for r in range(self.board_size):
            for c in range(self.board_size):
                if self.board[r, c] == 0:
                    state[3, r, c] = 1.0
        
        return torch.tensor(state, dtype=torch.float32)
    
    def get_possible_actions(self, player=None):
        """å¯èƒ½ãªè¡Œå‹•ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        if player is None:
            player = self.current_player
        
        actions = []
        piece_positions = self.pieces[player]['positions']
        
        for i, pos in enumerate(piece_positions):
            if pos is None:  # å–ã‚‰ã‚ŒãŸé§’ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            r, c = pos
            
            # 4æ–¹å‘ã¸ã®ç§»å‹•ã‚’ãƒã‚§ãƒƒã‚¯
            for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                new_r, new_c = r + dr, c + dc
                
                # ç›¤å†…ç§»å‹•
                if 0 <= new_r < self.board_size and 0 <= new_c < self.board_size:
                    if self._is_valid_move(player, (r, c), (new_r, new_c)):
                        actions.append(((r, c), (new_r, new_c)))
                
                # è„±å‡ºç§»å‹•ï¼ˆå–„ç‰ã®ã¿ï¼‰
                elif self._can_escape(player, i, (r, c), (new_r, new_c)):
                    actions.append(((r, c), (new_r, new_c)))
        
        return actions
    
    def _is_valid_move(self, player, from_pos, to_pos):
        """ç›¤å†…ç§»å‹•ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯"""
        r, c = to_pos
        target_piece = self.board[r, c]
        
        if target_piece == 0:  # ç©ºããƒã‚¹
            return True
        
        # ç›¸æ‰‹ã®é§’ãªã‚‰å–ã‚Œã‚‹
        if player == 'A' and target_piece > 10:
            return True
        elif player == 'B' and 1 <= target_piece <= 10:
            return True
        
        return False
    
    def _can_escape(self, player, piece_index, from_pos, to_pos):
        """è„±å‡ºå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        # å–„ç‰ã®ã¿è„±å‡ºå¯èƒ½
        if self.pieces[player]['types'][piece_index] != 1:
            return False
        
        # è„±å‡ºå£ã‹ã‚‰ã®ç§»å‹•ã®ã¿
        if from_pos not in self.escape_positions[player]:
            return False
        
        return True
    
    def make_move(self, from_pos, to_pos):
        """æ‰‹ã‚’å®Ÿè¡Œ"""
        if self.game_over:
            return False
        
        player = self.current_player
        r1, c1 = from_pos
        r2, c2 = to_pos
        
        # ç›¤å¤–ç§»å‹•ï¼ˆè„±å‡ºï¼‰ã®å ´åˆ
        if not (0 <= r2 < self.board_size and 0 <= c2 < self.board_size):
            return self._execute_escape(player, from_pos)
        
        # é€šå¸¸ç§»å‹•
        return self._execute_normal_move(player, from_pos, to_pos)
    
    def _execute_escape(self, player, from_pos):
        """è„±å‡ºå®Ÿè¡Œ"""
        # é§’ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹
        piece_index = None
        for i, pos in enumerate(self.pieces[player]['positions']):
            if pos == from_pos:
                piece_index = i
                break
        
        if piece_index is None:
            return False
        
        # å–„ç‰è„±å‡ºå‹åˆ©
        if self.pieces[player]['types'][piece_index] == 1:
            self.board[from_pos[0], from_pos[1]] = 0
            self.pieces[player]['positions'][piece_index] = None
            self.winner = player
            self.win_reason = "escape"
            self.game_over = True
            return True
        
        return False
    
    def _execute_normal_move(self, player, from_pos, to_pos):
        """é€šå¸¸ç§»å‹•å®Ÿè¡Œ"""
        r1, c1 = from_pos
        r2, c2 = to_pos
        
        moving_piece = self.board[r1, c1]
        target_piece = self.board[r2, c2]
        
        # é§’ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹
        piece_index = None
        for i, pos in enumerate(self.pieces[player]['positions']):
            if pos == from_pos:
                piece_index = i
                break
        
        if piece_index is None:
            return False
        
        # ç›¸æ‰‹é§’ã‚’å–ã‚‹å ´åˆ
        if target_piece != 0:
            opponent = 'B' if player == 'A' else 'A'
            captured_index = None
            
            for i, pos in enumerate(self.pieces[opponent]['positions']):
                if pos == to_pos:
                    captured_index = i
                    break
            
            if captured_index is not None:
                captured_type = self.pieces[opponent]['types'][captured_index]
                self.pieces[opponent]['positions'][captured_index] = None
                
                # å‹åˆ©æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                if captured_type == 1:  # å–„ç‰ã‚’å–ã£ãŸ
                    self.winner = player
                    self.win_reason = "captured_good"
                    self.game_over = True
                elif captured_type == 0:  # æ‚ªç‰ã‚’å–ã£ãŸ
                    self.winner = opponent
                    self.win_reason = "captured_bad"
                    self.game_over = True
        
        # ç§»å‹•å®Ÿè¡Œ
        self.board[r1, c1] = 0
        self.board[r2, c2] = moving_piece
        self.pieces[player]['positions'][piece_index] = to_pos
        
        self.move_count += 1
        self.current_player = 'B' if player == 'A' else 'A'
        
        return True
    
    def is_terminal(self):
        """ã‚²ãƒ¼ãƒ çµ‚äº†åˆ¤å®š"""
        return self.game_over
    
    def get_reward(self, player):
        """å ±é…¬è¨ˆç®—"""
        if not self.game_over:
            return 0.0
        
        if self.winner == player:
            if self.win_reason == "escape":
                return 100.0  # è„±å‡ºå‹åˆ©
            elif self.win_reason == "captured_good":
                return 80.0   # å–„ç‰å–ã‚Šå‹åˆ©
            else:
                return 90.0   # ãã®ä»–å‹åˆ©
        else:
            if self.win_reason == "captured_bad":
                return -100.0  # æ‚ªç‰å–ã‚‰ã‚Œè² ã‘
            else:
                return -80.0   # ãã®ä»–è² ã‘
    
    def display_board(self):
        """ãƒœãƒ¼ãƒ‰è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        print(f"Move {self.move_count}, Player {self.current_player}'s turn")
        print("  0 1 2 3")
        for r in range(self.board_size):
            row = f"{r} "
            for c in range(self.board_size):
                if (r, c) in self.escape_positions['A']:
                    row += "A "
                elif (r, c) in self.escape_positions['B']:
                    row += "B "
                elif self.board[r, c] == 0:
                    row += ". "
                else:
                    row += f"{self.board[r, c]} "
            print(row)
        print()


class QuantumQNetwork(nn.Module):
    """è»½é‡é‡å­Qå­¦ç¿’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self, n_qubits=4, n_layers=2):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        
        try:
            # é‡å­ãƒ‡ãƒã‚¤ã‚¹
            self.dev = qml.device("lightning.qubit", wires=n_qubits)
        except:
            # lightning.qubitãŒä½¿ãˆãªã„å ´åˆã¯default.qubitã‚’ä½¿ç”¨
            self.dev = qml.device("default.qubit", wires=n_qubits)
        
        # å¤å…¸å‰å‡¦ç†å±¤
        self.state_encoder = nn.Sequential(
            nn.Linear(64, 32),  # 4x4x4 = 64æ¬¡å…ƒ
            nn.ReLU(),
            nn.Linear(32, n_qubits),
            nn.Tanh()  # [-1, 1]ã«æ­£è¦åŒ–
        )
        
        # é‡å­å›è·¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.q_params = nn.Parameter(torch.randn(n_layers * n_qubits) * 0.1)
        
        # å¤å…¸å¾Œå‡¦ç†å±¤
        self.q_value_head = nn.Sequential(
            nn.Linear(n_qubits, 32),
            nn.ReLU(),
            nn.Linear(32, 16),  # 4x4ç›¤é¢ã®Qå€¤
            nn.Tanh()
        )
        
        # é‡å­å›è·¯å®šç¾©
        @qml.qnode(self.dev, interface="torch", diff_method="parameter-shift")
        def quantum_circuit(inputs, weights):
            # çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            for i in range(self.n_qubits):
                qml.RY(inputs[i], wires=i)
            
            # Variational layers
            for layer in range(self.n_layers):
                # Entangling layer
                for i in range(self.n_qubits - 1):
                    qml.CNOT(wires=[i, i + 1])
                if self.n_qubits > 2:
                    qml.CNOT(wires=[self.n_qubits - 1, 0])
                
                # Rotation layer
                for i in range(self.n_qubits):
                    param_idx = layer * self.n_qubits + i
                    qml.RY(weights[param_idx], wires=i)
            
            # æ¸¬å®š
            return [qml.expval(qml.PauliZ(wires=i)) for i in range(self.n_qubits)]
        
        self.quantum_circuit = quantum_circuit
    
    def forward(self, state):
        """å‰å‘ãä¼æ’­"""
        batch_size = state.shape[0]
        
        # çŠ¶æ…‹ã‚’å¹³å¦åŒ–
        state_flat = state.view(batch_size, -1)
        
        # å¤å…¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        encoded = self.state_encoder(state_flat)
        
        # é‡å­å›è·¯å®Ÿè¡Œï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
        quantum_outputs = []
        for i in range(batch_size):
            q_out = self.quantum_circuit(encoded[i], self.q_params)
            quantum_outputs.append(torch.stack(q_out))
        
        quantum_features = torch.stack(quantum_outputs)
        
        # Qå€¤è¨ˆç®—
        q_values = self.q_value_head(quantum_features)
        
        return q_values.view(batch_size, 4, 4)


class QuantumQAgent:
    """é‡å­Qå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, player_id, learning_rate=0.001, epsilon=0.9, 
                 epsilon_decay=0.995, epsilon_min=0.1, memory_size=2000):
        self.player_id = player_id
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.learning_rate = learning_rate
        
        # Q-Network
        self.q_network = QuantumQNetwork()
        self.target_network = QuantumQNetwork()
        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # Experience Replay
        self.memory = []
        self.memory_size = memory_size
        self.batch_size = 32
        
        # å­¦ç¿’çµ±è¨ˆ
        self.training_stats = {
            'episodes': 0,
            'total_reward': 0,
            'losses': [],
            'epsilon_values': []
        }
    
    def get_action(self, game_state, possible_actions, training=True):
        """è¡Œå‹•é¸æŠ"""
        if training and random.random() < self.epsilon:
            return random.choice(possible_actions) if possible_actions else None
        
        if not possible_actions:
            return None
        
        # Qå€¤è¨ˆç®—
        state_tensor = game_state.get_state_tensor(self.player_id).unsqueeze(0)
        
        with torch.no_grad():
            q_values = self.q_network(state_tensor).squeeze(0)
        
        # å¯èƒ½ãªè¡Œå‹•ã®Qå€¤ã®ã¿è€ƒæ…®
        best_action = None
        best_q_value = float('-inf')
        
        for action in possible_actions:
            from_pos, to_pos = action
            
            # ç›¤å¤–ç§»å‹•ï¼ˆè„±å‡ºï¼‰ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            if not (0 <= to_pos[0] < 4 and 0 <= to_pos[1] < 4):
                # è„±å‡ºè¡Œå‹•ã¯é«˜ã„ä¾¡å€¤ã‚’ä¸ãˆã‚‹
                q_val = 10.0
            else:
                q_val = q_values[to_pos[0], to_pos[1]].item()
            
            if q_val > best_q_value:
                best_q_value = q_val
                best_action = action
        
        return best_action
    
    def remember(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’è¨˜æ†¶"""
        self.memory.append((state, action, reward, next_state, done))
        if len(self.memory) > self.memory_size:
            self.memory.pop(0)
    
    def replay(self):
        """çµŒé¨“å†ç”Ÿå­¦ç¿’"""
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states = torch.stack([exp[0] for exp in batch])
        rewards = torch.tensor([exp[2] for exp in batch], dtype=torch.float32)
        dones = torch.tensor([exp[4] for exp in batch], dtype=torch.bool)
        
        current_q_values = self.q_network(states)
        
        with torch.no_grad():
            next_states = torch.stack([exp[3] for exp in batch])
            next_q_values = self.target_network(next_states)
            target_q_values = rewards + 0.95 * torch.max(next_q_values.view(self.batch_size, -1), dim=1)[0] * ~dones
        
        # æå¤±è¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        loss = nn.MSELoss()(current_q_values.view(self.batch_size, -1).max(dim=1)[0], target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        self.training_stats['losses'].append(loss.item())
        
        # Îµæ¸›è¡°
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        self.training_stats['epsilon_values'].append(self.epsilon)
    
    def update_target_network(self):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'training_stats': self.training_stats,
            'epsilon': self.epsilon
        }, filepath)
    
    def load_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        checkpoint = torch.load(filepath)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.training_stats = checkpoint['training_stats']
        self.epsilon = checkpoint['epsilon']


# åŸºæœ¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
class RandomAgent:
    """ãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, player_id):
        self.player_id = player_id
    
    def get_action(self, game_state, possible_actions, training=False):
        return random.choice(possible_actions) if possible_actions else None


class SmartRandomAgent:
    """è³¢ã„ãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæ˜ã‚‰ã‹ã«æ‚ªã„æ‰‹ã‚’é¿ã‘ã‚‹ï¼‰"""
    
    def __init__(self, player_id):
        self.player_id = player_id
    
    def get_action(self, game_state, possible_actions, training=False):
        if not possible_actions:
            return None
        
        # è„±å‡ºå¯èƒ½ãªæ‰‹ãŒã‚ã‚Œã°å„ªå…ˆ
        escape_actions = []
        for action in possible_actions:
            from_pos, to_pos = action
            if not (0 <= to_pos[0] < 4 and 0 <= to_pos[1] < 4):
                escape_actions.append(action)
        
        if escape_actions:
            return random.choice(escape_actions)
        
        return random.choice(possible_actions)


# ç°¡æ˜“å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ 
class SimpleTournament:
    """ç°¡æ˜“å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.agents = {}
        self.results = defaultdict(lambda: {'wins': 0, 'losses': 0, 'draws': 0})
    
    def register_agent(self, name, agent):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™»éŒ²"""
        self.agents[name] = agent
        print(f"âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ '{name}' ç™»éŒ²")
    
    def play_match(self, agent_a_name, agent_b_name, verbose=False):
        """å˜ä¸€è©¦åˆ"""
        game = Mini2GeisterGame()
        agent_a = self.agents[agent_a_name]
        agent_b = self.agents[agent_b_name]
        
        max_moves = 50
        move_count = 0
        
        while not game.is_terminal() and move_count < max_moves:
            current_agent_name = agent_a_name if game.current_player == 'A' else agent_b_name
            current_agent = agent_a if game.current_player == 'A' else agent_b
            
            possible_actions = game.get_possible_actions()
            if not possible_actions:
                break
            
            action = current_agent.get_action(game, possible_actions, training=False)
            if action is None:
                break
            
            if not game.make_move(action[0], action[1]):
                break
                
            move_count += 1
            
            if verbose and move_count % 10 == 0:
                game.display_board()
        
        # çµæœè¨˜éŒ²
        if game.is_terminal():
            winner_name = agent_a_name if game.winner == 'A' else agent_b_name
            loser_name = agent_b_name if game.winner == 'A' else agent_a_name
            
            self.results[winner_name]['wins'] += 1
            self.results[loser_name]['losses'] += 1
            
            return winner_name, game.win_reason, move_count
        else:
            self.results[agent_a_name]['draws'] += 1
            self.results[agent_b_name]['draws'] += 1
            return "Draw", "timeout", move_count
    
    def run_tournament(self, episodes_per_pair=50):
        """ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ"""
        agent_names = list(self.agents.keys())
        print(f"ğŸ† ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé–‹å§‹: {len(agent_names)}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
        
        for i, agent_a in enumerate(agent_names):
            for j, agent_b in enumerate(agent_names):
                if i >= j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹
                    continue
                
                print(f"âš”ï¸ {agent_a} vs {agent_b}")
                wins_a = 0
                
                for episode in range(episodes_per_pair):
                    winner, reason, moves = self.play_match(agent_a, agent_b)
                    if winner == agent_a:
                        wins_a += 1
                
                winrate_a = wins_a / episodes_per_pair
                print(f"   {agent_a}: {winrate_a:.1%} ({wins_a}/{episodes_per_pair})")
        
        self.print_rankings()
    
    def print_rankings(self):
        """ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤º"""
        print("\nğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
        print("="*50)
        
        for agent_name, stats in self.results.items():
            total = stats['wins'] + stats['losses'] + stats['draws']
            winrate = stats['wins'] / total if total > 0 else 0
            print(f"{agent_name:15s} | å‹ç‡: {winrate:6.1%} | "
                  f"{stats['wins']:3d}å‹ {stats['losses']:3d}æ•— {stats['draws']:3d}åˆ†")


def load_config():
    """è¨­å®šèª­ã¿è¾¼ã¿"""
    config_path = os.path.join(os.path.dirname(__file__), "config.json")
    default_config = {
        "game_settings": {"board_size": 4, "pieces_per_player": 2, "max_moves": 50},
        "quantum_settings": {"n_qubits": 4, "n_layers": 2, "learning_rate": 0.001},
        "training_settings": {"episodes": 500, "batch_size": 32},
        "evaluation_settings": {"tournament_episodes": 50}
    }
    
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return default_config


def train_quantum_agent(episodes=500):
    """é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´"""
    print(f"ğŸš€ é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´é–‹å§‹: {episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
    
    quantum_agent = QuantumQAgent('A', epsilon=0.9)
    opponent = SmartRandomAgent('B')
    
    wins = 0
    
    for episode in range(episodes):
        game = Mini2GeisterGame()
        states = []
        actions = []
        
        while not game.is_terminal():
            if game.current_player == 'A':  # é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
                state = game.get_state_tensor('A')
                possible_actions = game.get_possible_actions()
                
                if not possible_actions:
                    break
                
                action = quantum_agent.get_action(game, possible_actions, training=True)
                states.append(state)
                actions.append(action)
            else:  # ç›¸æ‰‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
                possible_actions = game.get_possible_actions()
                if possible_actions:
                    action = opponent.get_action(game, possible_actions)
                else:
                    break
            
            if action:
                game.make_move(action[0], action[1])
        
        # å­¦ç¿’æ›´æ–°
        final_reward = game.get_reward('A')
        for i, (state, action) in enumerate(zip(states, actions)):
            reward = final_reward if i == len(states) - 1 else 0
            next_state = states[i + 1] if i + 1 < len(states) else state
            done = (i == len(states) - 1)
            quantum_agent.remember(state, action, reward, next_state, done)
        
        if len(quantum_agent.memory) > quantum_agent.batch_size:
            quantum_agent.replay()
        
        if game.winner == 'A':
            wins += 1
        
        if episode % 100 == 0:
            winrate = wins / (episode + 1)
            print(f"Episode {episode:4d}/{episodes} | å‹ç‡: {winrate:.1%} | Îµ: {quantum_agent.epsilon:.3f}")
        
        if episode % 100 == 0:
            quantum_agent.update_target_network()
    
    final_winrate = wins / episodes
    print(f"âœ… è¨“ç·´å®Œäº†! æœ€çµ‚å‹ç‡: {final_winrate:.1%}")
    
    return quantum_agent


def quick_start():
    """ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ"""
    print("ğŸš€ Mini2ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
    tournament = SimpleTournament()
    tournament.register_agent("Quantum", QuantumQAgent('A', epsilon=0.1))
    tournament.register_agent("Random", RandomAgent('A'))
    tournament.register_agent("SmartRandom", SmartRandomAgent('A'))
    
    # ã‚¯ã‚¤ãƒƒã‚¯è©•ä¾¡ï¼ˆé‡å­ vs ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    print("\nğŸ¯ ã‚¯ã‚¤ãƒƒã‚¯è©•ä¾¡: é‡å­ vs ãƒ©ãƒ³ãƒ€ãƒ  (20è©¦åˆ)")
    wins = 0
    for i in range(20):
        winner, reason, moves = tournament.play_match("Quantum", "Random")
        if winner == "Quantum":
            wins += 1
    
    winrate = wins / 20
    print(f"ğŸ“Š é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹ç‡: {winrate:.1%} ({wins}/20)")
    
    if winrate > 0.6:
        print("ğŸ‰ é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‰¯å¥½ãªæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼")
    else:
        print("âš ï¸ é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨“ç·´ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“")


def interactive_mode():
    """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
    config = load_config()
    
    print("ğŸ® Mini2ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
    print("="*40)
    
    tournament = SimpleTournament()
    
    while True:
        print("\né¸æŠã—ã¦ãã ã•ã„:")
        print("1. ã‚¯ã‚¤ãƒƒã‚¯è©•ä¾¡")
        print("2. é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´")
        print("3. ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ")
        print("4. 1å¯¾1è¦³æˆ¦")
        print("0. çµ‚äº†")
        
        choice = input(">>> ").strip()
        
        if choice == "1":
            quick_start()
        elif choice == "2":
            episodes = int(input("è¨“ç·´ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ500): ") or "500")
            trained_agent = train_quantum_agent(episodes)
            tournament.register_agent("TrainedQuantum", trained_agent)
            print("âœ… è¨“ç·´æ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
        elif choice == "3":
            # åŸºæœ¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç™»éŒ²
            if "Random" not in tournament.agents:
                tournament.register_agent("Random", RandomAgent('A'))
                tournament.register_agent("SmartRandom", SmartRandomAgent('A'))
                tournament.register_agent("Quantum", QuantumQAgent('A', epsilon=0.1))
            
            episodes = int(input("å¯¾æˆ¦æ•°/ãƒšã‚¢ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30): ") or "30")
            tournament.run_tournament(episodes)
        elif choice == "4":
            if len(tournament.agents) < 2:
                tournament.register_agent("Random", RandomAgent('A'))
                tournament.register_agent("SmartRandom", SmartRandomAgent('A'))
            
            agents = list(tournament.agents.keys())
            print(f"åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: {', '.join(agents)}")
            
            agent_a = input("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆA: ").strip()
            agent_b = input("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆB: ").strip()
            
            if agent_a in agents and agent_b in agents:
                print(f"\nâš”ï¸ {agent_a} vs {agent_b}")
                winner, reason, moves = tournament.play_match(agent_a, agent_b, verbose=True)
                print(f"ğŸ† å‹è€…: {winner} ({reason}) - {moves}æ‰‹")
            else:
                print("âŒ ç„¡åŠ¹ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå")
        elif choice == "0":
            print("ğŸ‘‹ çµ‚äº†")
            break
        else:
            print("âŒ ç„¡åŠ¹ãªé¸æŠ")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "quick":
            quick_start()
        elif sys.argv[1] == "train":
            episodes = int(sys.argv[2]) if len(sys.argv) > 2 else 500
            train_quantum_agent(episodes)
        elif sys.argv[1] == "interactive":
            interactive_mode()
        else:
            print("ä½¿ç”¨æ³•: python mini2_integrated_system.py [quick|train|interactive]")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
        interactive_mode()


if __name__ == "__main__":
    print("ğŸ® Mini2ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*40)
    main()