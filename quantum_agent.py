"""
é‡å­å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…
ç¾åœ¨ã®ai_base.pyã®BaseAIã‚¯ãƒ©ã‚¹ã¨å®Œå…¨äº’æ›
"""

import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
from typing import List, Tuple, Optional
import os

try:
    import pennylane as qml
    PENNYLANE_AVAILABLE = True
except ImportError:
    PENNYLANE_AVAILABLE = False
    print("âš ï¸  PennyLane not available. Install with: pip install pennylane")

from qugeister_competitive.ai_base import BaseAI
from qugeister_competitive.game_engine import GameState

class QuantumAgent(BaseAI):
    """é‡å­å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, player_id: str, n_qubits=6, n_layers=3, learning_rate=0.01):
        super().__init__("QuantumAI", player_id)
        
        # é‡å­å›è·¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.learning_rate = learning_rate
        
        # é‡å­ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if PENNYLANE_AVAILABLE:
            self.dev = qml.device('default.qubit', wires=n_qubits)
            self._setup_quantum_circuit()
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤å…¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            print("ğŸ”„ PennyLaneæœªå¯¾å¿œï¼šå¤å…¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ä»£æ›¿")
            self._setup_classical_fallback()
        
        # å¼·åŒ–å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.epsilon = 0.9  # æ¢ç´¢ç‡ï¼ˆé«˜ã‚ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆï¼‰
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.95  # å‰²å¼•ç‡
        self.batch_size = 32
        
        # çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤
        self.memory = deque(maxlen=2000)
        self.training_step = 0
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²
        self.training_history = {
            'episodes': [],
            'rewards': [],
            'losses': [],
            'epsilon_values': []
        }
    
    def _setup_quantum_circuit(self):
        """é‡å­å›è·¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        # å¤‰åˆ†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.params = torch.randn(self.n_layers, self.n_qubits, 3, requires_grad=True)
        
        # å¤å…¸å‰å‡¦ç†
        self.input_encoder = nn.Linear(36, self.n_qubits)  # 6x6=36
        self.output_decoder = nn.Linear(self.n_qubits, 4)  # 4æ–¹å‘ã®ä¾¡å€¤
        
        # é‡å­å›è·¯å®šç¾©
        @qml.qnode(self.dev, interface='torch', diff_method='backprop')
        def quantum_circuit(inputs, params):
            # çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            for i in range(self.n_qubits):
                qml.RY(inputs[i], wires=i)
            
            # å¤‰åˆ†ãƒ¬ã‚¤ãƒ¤ãƒ¼
            for layer in range(self.n_layers):
                # å›è»¢ã‚²ãƒ¼ãƒˆ
                for i in range(self.n_qubits):
                    qml.RX(params[layer, i, 0], wires=i)
                    qml.RY(params[layer, i, 1], wires=i)
                    qml.RZ(params[layer, i, 2], wires=i)
                
                # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
                for i in range(self.n_qubits - 1):
                    qml.CNOT(wires=[i, i + 1])
                if self.n_qubits > 2:
                    qml.CNOT(wires=[self.n_qubits - 1, 0])  # å¾ªç’°çµåˆ
            
            # æ¸¬å®š
            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]
        
        self.quantum_circuit = quantum_circuit
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.Adam([self.params] + 
                                   list(self.input_encoder.parameters()) + 
                                   list(self.output_decoder.parameters()), 
                                   lr=self.learning_rate)
    
    def _setup_classical_fallback(self):
        """å¤å…¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        self.network = nn.Sequential(
            nn.Linear(36, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 4)
        )
        self.optimizer = optim.Adam(self.network.parameters(), lr=self.learning_rate)
        self.quantum_circuit = None
    
    def encode_game_state(self, game_state: GameState) -> torch.Tensor:
        """ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        # ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’æ•°å€¤åŒ–
        board = game_state.board.copy()
        
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç‚¹ã§æ­£è¦åŒ–
        if self.player_id == "B":
            board = -board  # Bè¦–ç‚¹ã§ã¯ç¬¦å·åè»¢
            board = np.flipud(board)  # ä¸Šä¸‹åè»¢
        
        # ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¦æ­£è¦åŒ–
        state_vector = board.flatten().astype(np.float32)
        state_vector = state_vector / 2.0  # [-1, 1]ã«æ­£è¦åŒ–
        
        return torch.tensor(state_vector, dtype=torch.float32)
    
    def get_q_values(self, state_tensor: torch.Tensor) -> torch.Tensor:
        """Qå€¤ã‚’è¨ˆç®—"""
        if self.quantum_circuit is not None:
            # é‡å­å›è·¯ä½¿ç”¨
            encoded_state = torch.tanh(self.input_encoder(state_tensor))
            quantum_output = self.quantum_circuit(encoded_state, self.params)
            q_values = self.output_decoder(torch.stack(quantum_output))
        else:
            # å¤å…¸ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½¿ç”¨
            q_values = self.network(state_tensor)
        
        return q_values
    
    def get_move(self, game_state: GameState, legal_moves: List[Tuple[Tuple[int, int], Tuple[int, int]]]) -> Optional[Tuple[Tuple[int, int], Tuple[int, int]]]:
        """æ‰‹ã‚’é¸æŠï¼ˆBaseAIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æº–æ‹ ï¼‰"""
        if not legal_moves:
            return None
        
        # Îµ-greedyæ¢ç´¢
        if random.random() < self.epsilon:
            return random.choice(legal_moves)
        
        # Qå€¤ã«åŸºã¥ãè¡Œå‹•é¸æŠ
        state_tensor = self.encode_game_state(game_state)
        q_values = self.get_q_values(state_tensor)
        
        # åˆæ³•æ‰‹ã«å¯¾å¿œã™ã‚‹è¡Œå‹•ã‚’è©•ä¾¡
        best_move = self._select_best_legal_move(q_values, legal_moves, game_state)
        return best_move
    
    def _select_best_legal_move(self, q_values: torch.Tensor, legal_moves: List, game_state: GameState) -> Tuple:
        """Qå€¤ã¨åˆæ³•æ‰‹ã‹ã‚‰æœ€é©æ‰‹ã‚’é¸æŠ"""
        move_scores = []
        
        for move in legal_moves:
            from_pos, to_pos = move
            
            # ç§»å‹•æ–¹å‘ã‚’åˆ¤å®š
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            # æ–¹å‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (ä¸Šå³ä¸‹å·¦)
            if dy == -1:    direction = 0  # ä¸Š
            elif dx == 1:   direction = 1  # å³
            elif dy == 1:   direction = 2  # ä¸‹
            elif dx == -1:  direction = 3  # å·¦
            else:           direction = 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Bè¦–ç‚¹ã§ã¯æ–¹å‘èª¿æ•´
            if self.player_id == "B":
                direction = (direction + 2) % 4  # ä¸Šä¸‹åè»¢
            
            score = q_values[direction].item()
            
            # æˆ¦è¡“çš„ãƒœãƒ¼ãƒŠã‚¹
            score += self._calculate_tactical_bonus(move, game_state)
            
            move_scores.append((move, score))
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®æ‰‹ã‚’é¸æŠ
        best_move = max(move_scores, key=lambda x: x[1])[0]
        return best_move
    
    def _calculate_tactical_bonus(self, move: Tuple, game_state: GameState) -> float:
        """æˆ¦è¡“çš„ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—"""
        from_pos, to_pos = move
        bonus = 0.0
        
        # å‰é€²ãƒœãƒ¼ãƒŠã‚¹
        if self.player_id == "A" and to_pos[1] > from_pos[1]:
            bonus += 0.1
        elif self.player_id == "B" and to_pos[1] < from_pos[1]:
            bonus += 0.1
        
        # ä¸­å¤®å¯„ã‚Šãƒœãƒ¼ãƒŠã‚¹
        center_distance = abs(to_pos[0] - 2.5)
        bonus += (2.5 - center_distance) * 0.02
        
        # ç›¸æ‰‹é§’å–ã‚Šãƒœãƒ¼ãƒŠã‚¹
        opponent_pieces = (game_state.player_b_pieces if self.player_id == "A" 
                          else game_state.player_a_pieces)
        if to_pos in opponent_pieces:
            bonus += 0.5
        
        return bonus
    
    def remember(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’è¨˜æ†¶"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay_training(self):
        """çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤å­¦ç¿’"""
        if len(self.memory) < self.batch_size:
            return 0.0
        
        # ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = random.sample(self.memory, self.batch_size)
        states = torch.stack([exp[0] for exp in batch])
        actions = torch.tensor([exp[1] for exp in batch])
        rewards = torch.tensor([exp[2] for exp in batch], dtype=torch.float32)
        next_states = torch.stack([exp[3] for exp in batch])
        dones = torch.tensor([exp[4] for exp in batch], dtype=torch.bool)
        
        # ç¾åœ¨ã®Qå€¤
        current_q_values = []
        for i, state in enumerate(states):
            q_vals = self.get_q_values(state)
            current_q_values.append(q_vals[actions[i]])
        current_q_values = torch.stack(current_q_values)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤
        target_q_values = []
        for i, (reward, next_state, done) in enumerate(zip(rewards, next_states, dones)):
            if done:
                target = reward
            else:
                next_q_vals = self.get_q_values(next_state)
                target = reward + self.gamma * torch.max(next_q_vals)
            target_q_values.append(target)
        target_q_values = torch.stack(target_q_values)
        
        # æå¤±è¨ˆç®—
        loss = nn.MSELoss()(current_q_values, target_q_values.detach())
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # Îµå€¤æ¸›è¡°
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        self.training_step += 1
        return loss.item()
    
    def save_model(self, filepath: str):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        save_dict = {
            'model_type': 'quantum' if self.quantum_circuit else 'classical',
            'player_id': self.player_id,
            'n_qubits': self.n_qubits,
            'n_layers': self.n_layers,
            'training_step': self.training_step,
            'epsilon': self.epsilon,
            'training_history': self.training_history
        }
        
        if self.quantum_circuit:
            save_dict.update({
                'quantum_params': self.params,
                'input_encoder_state': self.input_encoder.state_dict(),
                'output_decoder_state': self.output_decoder.state_dict()
            })
        else:
            save_dict['network_state'] = self.network.state_dict()
        
        torch.save(save_dict, filepath)
        print(f"ğŸ’¾ é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜: {filepath}")
    
    def load_model(self, filepath: str):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        if not os.path.exists(filepath):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
            return False
        
        checkpoint = torch.load(filepath)
        
        self.training_step = checkpoint.get('training_step', 0)
        self.epsilon = checkpoint.get('epsilon', 0.01)
        self.training_history = checkpoint.get('training_history', {})
        
        if checkpoint['model_type'] == 'quantum' and self.quantum_circuit:
            self.params = checkpoint['quantum_params']
            self.input_encoder.load_state_dict(checkpoint['input_encoder_state'])
            self.output_decoder.load_state_dict(checkpoint['output_decoder_state'])
        elif checkpoint['model_type'] == 'classical' and not self.quantum_circuit:
            self.network.load_state_dict(checkpoint['network_state'])
        
        print(f"âœ… é‡å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {filepath}")
        return True
    
    def get_model_info(self) -> dict:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        return {
            'name': self.name,
            'type': 'quantum' if self.quantum_circuit else 'classical',
            'n_qubits': self.n_qubits,
            'n_layers': self.n_layers,
            'training_step': self.training_step,
            'epsilon': self.epsilon,
            'games_played': self.games_played,
            'wins': self.wins,
            'win_rate': self.win_rate
        }