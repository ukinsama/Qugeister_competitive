#!/usr/bin/env python3
"""
CQCNNç«¶æŠ€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦å¯¾æˆ¦ã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ 

ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ:
1. åˆæœŸé…ç½®æˆ¦ç•¥
2. æ•µé§’æ¨å®šå™¨ï¼ˆCQCNNï¼‰
3. Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨
4. è¡Œå‹•é¸æŠã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from abc import ABC, abstractmethod
import json
import random
import time
from dataclasses import dataclass

# ================================================================================
# Module 1: åˆæœŸé…ç½®æˆ¦ç•¥
# ================================================================================

class InitialPlacementStrategy(ABC):
    """åˆæœŸé…ç½®æˆ¦ç•¥ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def get_placement(self, player: str) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’è¿”ã™"""
        pass
    
    @abstractmethod
    def get_strategy_name(self) -> str:
        """æˆ¦ç•¥åã‚’è¿”ã™"""
        pass


class StandardPlacement(InitialPlacementStrategy):
    """æ¨™æº–é…ç½®: å‰åˆ—å–„ç‰ã€å¾Œåˆ—æ‚ªç‰"""
    
    def get_placement(self, player: str) -> Dict[Tuple[int, int], str]:
        if player == "A":
            return {
                (1, 0): "good", (2, 0): "good", (3, 0): "good", (4, 0): "good",
                (1, 1): "bad", (2, 1): "bad", (3, 1): "bad", (4, 1): "bad"
            }
        else:
            return {
                (1, 5): "good", (2, 5): "good", (3, 5): "good", (4, 5): "good",
                (1, 4): "bad", (2, 4): "bad", (3, 4): "bad", (4, 4): "bad"
            }
    
    def get_strategy_name(self) -> str:
        return "æ¨™æº–é…ç½®"


class DefensivePlacement(InitialPlacementStrategy):
    """å®ˆå‚™çš„é…ç½®: å–„ç‰ã‚’å¾Œã‚ã«éš ã™"""
    
    def get_placement(self, player: str) -> Dict[Tuple[int, int], str]:
        if player == "A":
            return {
                (1, 0): "bad", (2, 0): "bad", (3, 0): "bad", (4, 0): "bad",
                (1, 1): "good", (2, 1): "good", (3, 1): "good", (4, 1): "good"
            }
        else:
            return {
                (1, 5): "bad", (2, 5): "bad", (3, 5): "bad", (4, 5): "bad",
                (1, 4): "good", (2, 4): "good", (3, 4): "good", (4, 4): "good"
            }
    
    def get_strategy_name(self) -> str:
        return "å®ˆå‚™çš„é…ç½®"


class RandomPlacement(InitialPlacementStrategy):
    """ãƒ©ãƒ³ãƒ€ãƒ é…ç½®: å–„ç‰ã¨æ‚ªç‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é…ç½®"""
    
    def get_placement(self, player: str) -> Dict[Tuple[int, int], str]:
        if player == "A":
            positions = [(1, 0), (2, 0), (3, 0), (4, 0),
                        (1, 1), (2, 1), (3, 1), (4, 1)]
        else:
            positions = [(1, 5), (2, 5), (3, 5), (4, 5),
                        (1, 4), (2, 4), (3, 4), (4, 4)]
        
        random.shuffle(positions)
        placement = {}
        for i, pos in enumerate(positions):
            placement[pos] = "good" if i < 4 else "bad"
        
        return placement
    
    def get_strategy_name(self) -> str:
        return "ãƒ©ãƒ³ãƒ€ãƒ é…ç½®"


class MixedPlacement(InitialPlacementStrategy):
    """æ··åˆé…ç½®: å–„ç‰ã¨æ‚ªç‰ã‚’äº¤äº’ã«é…ç½®"""
    
    def get_placement(self, player: str) -> Dict[Tuple[int, int], str]:
        if player == "A":
            return {
                (1, 0): "good", (2, 0): "bad", (3, 0): "good", (4, 0): "bad",
                (1, 1): "bad", (2, 1): "good", (3, 1): "bad", (4, 1): "good"
            }
        else:
            return {
                (1, 5): "good", (2, 5): "bad", (3, 5): "good", (4, 5): "bad",
                (1, 4): "bad", (2, 4): "good", (3, 4): "bad", (4, 4): "good"
            }
    
    def get_strategy_name(self) -> str:
        return "æ··åˆé…ç½®"


# ================================================================================
# Module 2: æ•µé§’æ¨å®šå™¨ï¼ˆCQCNNï¼‰
# ================================================================================

class PieceEstimator(ABC):
    """æ•µé§’æ¨å®šå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def estimate(self, board_state: np.ndarray, enemy_positions: List[Tuple[int, int]], 
                player: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        """
        æ•µé§’ã®ç¨®é¡ã‚’æ¨å®š
        Returns: {position: {'good_prob': float, 'bad_prob': float, 'confidence': float}}
        """
        pass
    
    @abstractmethod
    def train(self, training_data: List[Dict]) -> None:
        """å­¦ç¿’ã‚’å®Ÿè¡Œ"""
        pass
    
    @abstractmethod
    def get_estimator_name(self) -> str:
        """æ¨å®šå™¨åã‚’è¿”ã™"""
        pass


class SimpleCQCNNEstimator(PieceEstimator):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªCQCNNæ¨å®šå™¨"""
    
    def __init__(self, n_qubits: int = 4, n_layers: int = 2):
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.model = self._build_model()
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)
    
    def _build_model(self):
        """ç°¡æ˜“CQCNNãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
        return nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32 * 6 * 6, 64),
            nn.ReLU(),
            nn.Linear(64, 2)  # good/badç¢ºç‡
        )
    
    def estimate(self, board_state: np.ndarray, enemy_positions: List[Tuple[int, int]], 
                player: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        """æ•µé§’æ¨å®š"""
        results = {}
        
        # ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        board_tensor = self._prepare_board_tensor(board_state, player)
        
        with torch.no_grad():
            output = self.model(board_tensor)
            probs = F.softmax(output, dim=1)
            
            # å„æ•µé§’ä½ç½®ã«å¯¾ã—ã¦åŒã˜æ¨å®šã‚’è¿”ã™ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            for pos in enemy_positions:
                results[pos] = {
                    'good_prob': probs[0, 0].item(),
                    'bad_prob': probs[0, 1].item(),
                    'confidence': max(probs[0].tolist())
                }
        
        return results
    
    def _prepare_board_tensor(self, board: np.ndarray, player: str) -> torch.Tensor:
        """ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
        tensor = torch.zeros(1, 3, 6, 6)
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        tensor[0, 0] = torch.from_numpy((board == player_val).astype(np.float32))
        tensor[0, 1] = torch.from_numpy((board == enemy_val).astype(np.float32))
        tensor[0, 2] = torch.from_numpy((board == 0).astype(np.float32))
        
        return tensor
    
    def train(self, training_data: List[Dict]) -> None:
        """ç°¡æ˜“å­¦ç¿’"""
        # å®Ÿè£…ã¯çœç•¥ï¼ˆå®Ÿéš›ã«ã¯training_dataã‹ã‚‰å­¦ç¿’ï¼‰
        pass
    
    def get_estimator_name(self) -> str:
        return f"SimpleCQCNN({self.n_qubits}qubits)"


class AdvancedCQCNNEstimator(PieceEstimator):
    """é«˜åº¦ãªCQCNNæ¨å®šå™¨ï¼ˆé‡å­å›è·¯ä»˜ãï¼‰"""
    
    def __init__(self, n_qubits: int = 6, n_layers: int = 3):
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.quantum_params = nn.Parameter(torch.randn(n_layers, n_qubits, 3) * 0.1)
        self.cnn = self._build_cnn()
        self.optimizer = torch.optim.Adam(
            list(self.cnn.parameters()) + [self.quantum_params], 
            lr=0.001
        )
    
    def _build_cnn(self):
        """CNNéƒ¨åˆ†ã®æ§‹ç¯‰"""
        return nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(16),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(32 * 3 * 3, self.n_qubits),
            nn.Tanh()
        )
    
    def _quantum_circuit(self, x: torch.Tensor) -> torch.Tensor:
        """ç°¡æ˜“é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        batch_size = x.shape[0]
        state = x.clone()
        
        for layer in range(self.n_layers):
            # å›è»¢ã‚²ãƒ¼ãƒˆ
            for q in range(self.n_qubits):
                rotation = self.quantum_params[layer, q]
                state[:, q] = torch.cos(rotation[0]) * state[:, q] + \
                             torch.sin(rotation[1]) * torch.roll(state, 1, dims=1)[:, q]
        
        return state
    
    def estimate(self, board_state: np.ndarray, enemy_positions: List[Tuple[int, int]], 
                player: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        """é«˜åº¦ãªæ•µé§’æ¨å®š"""
        results = {}
        board_tensor = self._prepare_board_tensor(board_state, player)
        
        with torch.no_grad():
            # CNNç‰¹å¾´æŠ½å‡º
            features = self.cnn(board_tensor)
            
            # é‡å­å›è·¯å‡¦ç†
            quantum_output = self._quantum_circuit(features)
            
            # å„ä½ç½®ã«å¯¾ã—ã¦ç•°ãªã‚‹æ¨å®š
            for i, pos in enumerate(enemy_positions):
                # ä½ç½®ä¾å­˜ã®æ¨å®š
                position_factor = (pos[0] / 5.0 + pos[1] / 5.0) / 2.0
                
                # é‡å­å‡ºåŠ›ã‹ã‚‰ç¢ºç‡ã‚’è¨ˆç®—
                q_val = quantum_output[0, i % self.n_qubits].item()
                good_prob = (1 + q_val * position_factor) / 2
                good_prob = max(0.0, min(1.0, good_prob))
                
                results[pos] = {
                    'good_prob': good_prob,
                    'bad_prob': 1 - good_prob,
                    'confidence': 0.5 + abs(q_val) * 0.3
                }
        
        return results
    
    def _prepare_board_tensor(self, board: np.ndarray, player: str) -> torch.Tensor:
        """ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
        tensor = torch.zeros(1, 3, 6, 6)
        player_val = 1 if player == "A" else -1
        enemy_val = -player_val
        
        tensor[0, 0] = torch.from_numpy((board == player_val).astype(np.float32))
        tensor[0, 1] = torch.from_numpy((board == enemy_val).astype(np.float32))
        tensor[0, 2] = torch.from_numpy((board == 0).astype(np.float32))
        
        return tensor
    
    def train(self, training_data: List[Dict]) -> None:
        """é«˜åº¦ãªå­¦ç¿’ï¼ˆå¼·åŒ–å­¦ç¿’ï¼‰"""
        # å®Ÿè£…ã¯çœç•¥
        pass
    
    def get_estimator_name(self) -> str:
        return f"AdvancedCQCNN({self.n_qubits}qubits,{self.n_layers}layers)"


class RandomEstimator(PieceEstimator):
    """ãƒ©ãƒ³ãƒ€ãƒ æ¨å®šå™¨ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰"""
    
    def estimate(self, board_state: np.ndarray, enemy_positions: List[Tuple[int, int]], 
                player: str) -> Dict[Tuple[int, int], Dict[str, float]]:
        """ãƒ©ãƒ³ãƒ€ãƒ ãªæ¨å®š"""
        results = {}
        for pos in enemy_positions:
            good_prob = random.random()
            results[pos] = {
                'good_prob': good_prob,
                'bad_prob': 1 - good_prob,
                'confidence': random.uniform(0.3, 0.7)
            }
        return results
    
    def train(self, training_data: List[Dict]) -> None:
        """å­¦ç¿’ãªã—"""
        pass
    
    def get_estimator_name(self) -> str:
        return "ãƒ©ãƒ³ãƒ€ãƒ æ¨å®š"


# ================================================================================
# Module 3: Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨
# ================================================================================

class QMapGenerator(ABC):
    """Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def generate(self, board_state: np.ndarray, 
                estimations: Dict[Tuple[int, int], Dict[str, float]],
                my_pieces: Dict[Tuple[int, int], str],
                player: str) -> np.ndarray:
        """
        Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
        Returns: (6, 6, 4) ã®é…åˆ— - å„ä½ç½®ã®4æ–¹å‘ã¸ã®ç§»å‹•ä¾¡å€¤
        """
        pass
    
    @abstractmethod
    def get_generator_name(self) -> str:
        """ç”Ÿæˆå™¨åã‚’è¿”ã™"""
        pass


class SimpleQMapGenerator(QMapGenerator):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªQå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def generate(self, board_state: np.ndarray, 
                estimations: Dict[Tuple[int, int], Dict[str, float]],
                my_pieces: Dict[Tuple[int, int], str],
                player: str) -> np.ndarray:
        """åŸºæœ¬çš„ãªQå€¤ãƒãƒƒãƒ—ç”Ÿæˆ"""
        q_map = np.zeros((6, 6, 4))
        
        for piece_pos, piece_type in my_pieces.items():
            x, y = piece_pos
            
            # 4æ–¹å‘ã®è©•ä¾¡ï¼ˆä¸Šå³ä¸‹å·¦ï¼‰
            directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]
            
            for dir_idx, (dx, dy) in enumerate(directions):
                new_x, new_y = x + dx, y + dy
                
                # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                if not (0 <= new_x < 6 and 0 <= new_y < 6):
                    q_map[y, x, dir_idx] = -100
                    continue
                
                # åŸºæœ¬ã‚¹ã‚³ã‚¢
                score = 0.0
                
                # å‰é€²ãƒœãƒ¼ãƒŠã‚¹
                if player == "A" and dy > 0:
                    score += 2.0
                elif player == "B" and dy < 0:
                    score += 2.0
                
                # é§’å–ã‚Šè©•ä¾¡
                if (new_x, new_y) in estimations:
                    est = estimations[(new_x, new_y)]
                    # å–„ç‰ã‚’å–ã‚‹ä¾¡å€¤ - æ‚ªç‰ã‚’å–ã‚‹ãƒªã‚¹ã‚¯
                    score += est['good_prob'] * 5.0 - est['bad_prob'] * 3.0
                
                q_map[y, x, dir_idx] = score
        
        return q_map
    
    def get_generator_name(self) -> str:
        return "ã‚·ãƒ³ãƒ—ãƒ«Qå€¤ç”Ÿæˆ"


class StrategicQMapGenerator(QMapGenerator):
    """æˆ¦ç•¥çš„Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.weights = {
            'forward': 3.0,
            'capture_good': 8.0,
            'capture_bad': -4.0,
            'escape': 10.0,
            'center': 1.5,
            'protection': 2.0
        }
    
    def generate(self, board_state: np.ndarray, 
                estimations: Dict[Tuple[int, int], Dict[str, float]],
                my_pieces: Dict[Tuple[int, int], str],
                player: str) -> np.ndarray:
        """æˆ¦ç•¥çš„ãªQå€¤ãƒãƒƒãƒ—ç”Ÿæˆ"""
        q_map = np.zeros((6, 6, 4))
        
        # è„±å‡ºå£ã®å®šç¾©
        escape_positions = [(0, 5), (5, 5)] if player == "A" else [(0, 0), (5, 0)]
        
        for piece_pos, piece_type in my_pieces.items():
            x, y = piece_pos
            directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]
            
            for dir_idx, (dx, dy) in enumerate(directions):
                new_x, new_y = x + dx, y + dy
                
                if not (0 <= new_x < 6 and 0 <= new_y < 6):
                    q_map[y, x, dir_idx] = -100
                    continue
                
                score = 0.0
                
                # å‰é€²è©•ä¾¡
                if player == "A" and dy > 0:
                    score += self.weights['forward']
                elif player == "B" and dy < 0:
                    score += self.weights['forward']
                
                # è„±å‡ºè©•ä¾¡ï¼ˆå–„ç‰ã®ã¿ï¼‰
                if piece_type == "good" and (new_x, new_y) in escape_positions:
                    score += self.weights['escape']
                
                # ä¸­å¤®åˆ¶å¾¡
                center_dist = abs(new_x - 2.5) + abs(new_y - 2.5)
                score += self.weights['center'] * (5 - center_dist) / 5
                
                # é§’å–ã‚Šè©•ä¾¡ï¼ˆæ¨å®šã‚’è€ƒæ…®ï¼‰
                if (new_x, new_y) in estimations:
                    est = estimations[(new_x, new_y)]
                    score += est['good_prob'] * self.weights['capture_good']
                    score += est['bad_prob'] * self.weights['capture_bad']
                    score *= est['confidence']  # ç¢ºä¿¡åº¦ã§é‡ã¿ä»˜ã‘
                
                # å‘³æ–¹ã¨ã®é€£æº
                for ally_pos in my_pieces:
                    if ally_pos != piece_pos:
                        dist = abs(new_x - ally_pos[0]) + abs(new_y - ally_pos[1])
                        if dist == 1:
                            score += self.weights['protection']
                
                q_map[y, x, dir_idx] = score
        
        return q_map
    
    def get_generator_name(self) -> str:
        return "æˆ¦ç•¥çš„Qå€¤ç”Ÿæˆ"


class NeuralQMapGenerator(QMapGenerator):
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆå™¨"""
    
    def __init__(self):
        # å…¥åŠ›æ¬¡å…ƒã‚’æ­£ç¢ºã«è¨ˆç®—
        # ãƒœãƒ¼ãƒ‰: 6*6 = 36
        # æ¨å®šç‰¹å¾´: 10
        # åˆè¨ˆ: 46
        input_dim = 6 * 6 + 10
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, 128),  # 46 -> 128
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 6 * 6 * 4),  # 64 -> 144 (6*6*4)
            nn.Tanh()
        )
    
    def generate(self, board_state: np.ndarray, 
                estimations: Dict[Tuple[int, int], Dict[str, float]],
                my_pieces: Dict[Tuple[int, int], str],
                player: str) -> np.ndarray:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qå€¤ãƒãƒƒãƒ—ç”Ÿæˆ"""
        # å…¥åŠ›ç‰¹å¾´é‡ã‚’ä½œæˆ
        board_flat = board_state.flatten()
        
        # æ¨å®šæƒ…å ±ã‚’é›†ç´„
        est_features = np.zeros(10)
        if estimations:
            good_probs = [e['good_prob'] for e in estimations.values()]
            confidences = [e['confidence'] for e in estimations.values()]
            est_features[0] = np.mean(good_probs)
            est_features[1] = np.std(good_probs)
            est_features[2] = np.mean(confidences)
            est_features[3] = len(my_pieces)
            est_features[4] = len(estimations)
        
        # å…¥åŠ›çµåˆ
        input_tensor = torch.tensor(
            np.concatenate([board_flat, est_features]), 
            dtype=torch.float32
        ).unsqueeze(0)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¨è«–
        with torch.no_grad():
            output = self.network(input_tensor)
            q_map = output.view(6, 6, 4).numpy()
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        q_map = q_map * 10.0
        
        return q_map
    
    def get_generator_name(self) -> str:
        return "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«Qå€¤ç”Ÿæˆ"


# ================================================================================
# Module 4: è¡Œå‹•é¸æŠã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
# ================================================================================

class ActionSelector(ABC):
    """è¡Œå‹•é¸æŠã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def select_action(self, q_map: np.ndarray, 
                     legal_moves: List[Tuple[Tuple[int, int], Tuple[int, int]]]) -> Tuple:
        """Qå€¤ãƒãƒƒãƒ—ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ"""
        pass
    
    @abstractmethod
    def get_selector_name(self) -> str:
        """é¸æŠå™¨åã‚’è¿”ã™"""
        pass


class GreedySelector(ActionSelector):
    """è²ªæ¬²é¸æŠ: æœ€å¤§Qå€¤ã®è¡Œå‹•ã‚’é¸æŠ"""
    
    def select_action(self, q_map: np.ndarray, 
                     legal_moves: List[Tuple[Tuple[int, int], Tuple[int, int]]]) -> Tuple:
        """æœ€å¤§Qå€¤ã®è¡Œå‹•ã‚’é¸æŠ"""
        best_move = None
        best_q = -float('inf')
        
        for move in legal_moves:
            from_pos, to_pos = move
            
            # æ–¹å‘ã‚’è¨ˆç®—
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            if dy == -1 and dx == 0:    dir_idx = 0
            elif dx == 1 and dy == 0:   dir_idx = 1
            elif dy == 1 and dx == 0:   dir_idx = 2
            elif dx == -1 and dy == 0:  dir_idx = 3
            else: continue
            
            q_value = q_map[from_pos[1], from_pos[0], dir_idx]
            
            if q_value > best_q:
                best_q = q_value
                best_move = move
        
        return best_move if best_move else random.choice(legal_moves)
    
    def get_selector_name(self) -> str:
        return "è²ªæ¬²é¸æŠ"


class EpsilonGreedySelector(ActionSelector):
    """Îµ-è²ªæ¬²é¸æŠ: ç¢ºç‡çš„ã«æ¢ç´¢"""
    
    def __init__(self, epsilon: float = 0.1):
        self.epsilon = epsilon
    
    def select_action(self, q_map: np.ndarray, 
                     legal_moves: List[Tuple[Tuple[int, int], Tuple[int, int]]]) -> Tuple:
        """Îµç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ ã€ãã‚Œä»¥å¤–ã¯æœ€å¤§Qå€¤"""
        if random.random() < self.epsilon:
            return random.choice(legal_moves)
        
        # Greedyã¨åŒã˜å‡¦ç†
        best_move = None
        best_q = -float('inf')
        
        for move in legal_moves:
            from_pos, to_pos = move
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            if dy == -1 and dx == 0:    dir_idx = 0
            elif dx == 1 and dy == 0:   dir_idx = 1
            elif dy == 1 and dx == 0:   dir_idx = 2
            elif dx == -1 and dy == 0:  dir_idx = 3
            else: continue
            
            q_value = q_map[from_pos[1], from_pos[0], dir_idx]
            
            if q_value > best_q:
                best_q = q_value
                best_move = move
        
        return best_move if best_move else random.choice(legal_moves)
    
    def get_selector_name(self) -> str:
        return f"Îµ-è²ªæ¬²(Îµ={self.epsilon})"


class SoftmaxSelector(ActionSelector):
    """ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é¸æŠ: Qå€¤ã«åŸºã¥ãç¢ºç‡çš„é¸æŠ"""
    
    def __init__(self, temperature: float = 1.0):
        self.temperature = temperature
    
    def select_action(self, q_map: np.ndarray, 
                     legal_moves: List[Tuple[Tuple[int, int], Tuple[int, int]]]) -> Tuple:
        """Qå€¤ã‚’ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã§ç¢ºç‡åŒ–ã—ã¦é¸æŠ"""
        q_values = []
        
        for move in legal_moves:
            from_pos, to_pos = move
            dx = to_pos[0] - from_pos[0]
            dy = to_pos[1] - from_pos[1]
            
            if dy == -1 and dx == 0:    dir_idx = 0
            elif dx == 1 and dy == 0:   dir_idx = 1
            elif dy == 1 and dx == 0:   dir_idx = 2
            elif dx == -1 and dy == 0:  dir_idx = 3
            else:
                q_values.append(-100)
                continue
            
            q_values.append(q_map[from_pos[1], from_pos[0], dir_idx])
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç¢ºç‡ã‚’è¨ˆç®—
        q_tensor = torch.tensor(q_values, dtype=torch.float32) / self.temperature
        probs = F.softmax(q_tensor, dim=0).numpy()
        
        # ç¢ºç‡çš„ã«é¸æŠ
        choice_idx = np.random.choice(len(legal_moves), p=probs)
        return legal_moves[choice_idx]
    
    def get_selector_name(self) -> str:
        return f"ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹(T={self.temperature})"


# ================================================================================
# çµ±åˆ: ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼CQCNN AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
# ================================================================================

@dataclass
class ModuleConfig:
    """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""
    placement_strategy: InitialPlacementStrategy
    piece_estimator: PieceEstimator
    qmap_generator: QMapGenerator
    action_selector: ActionSelector


class ModularCQCNNAgent:
    """ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼å‹CQCNN AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, player_id: str, config: ModuleConfig):
        self.player_id = player_id
        self.config = config
        self.name = self._generate_name()
        
        # çµ±è¨ˆæƒ…å ±
        self.games_played = 0
        self.wins = 0
        self.training_data = []
    
    def _generate_name(self) -> str:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã‚’ç”Ÿæˆ"""
        return f"Agent[{self.config.placement_strategy.get_strategy_name()[:3]}+" \
               f"{self.config.piece_estimator.get_estimator_name()[:6]}+" \
               f"{self.config.qmap_generator.get_generator_name()[:3]}+" \
               f"{self.config.action_selector.get_selector_name()[:3]}]"
    
    def get_initial_placement(self) -> Dict[Tuple[int, int], str]:
        """åˆæœŸé…ç½®ã‚’å–å¾—"""
        return self.config.placement_strategy.get_placement(self.player_id)
    
    def get_move(self, game_state: Any, legal_moves: List) -> Optional[Tuple]:
        """æ‰‹ã‚’é¸æŠ"""
        if not legal_moves:
            return None
        
        # 1. æ•µé§’ä½ç½®ã‚’ç‰¹å®š
        enemy_positions = self._find_enemy_positions(game_state)
        
        # 2. æ•µé§’ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š
        estimations = self.config.piece_estimator.estimate(
            game_state.board,
            enemy_positions,
            self.player_id
        )
        
        # 3. Qå€¤ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
        my_pieces = game_state.player_a_pieces if self.player_id == "A" else game_state.player_b_pieces
        q_map = self.config.qmap_generator.generate(
            game_state.board,
            estimations,
            my_pieces,
            self.player_id
        )
        
        # 4. è¡Œå‹•ã‚’é¸æŠ
        action = self.config.action_selector.select_action(q_map, legal_moves)
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
        self.training_data.append({
            'board': game_state.board.copy(),
            'estimations': estimations,
            'q_map': q_map.copy(),
            'action': action
        })
        
        return action
    
    def _find_enemy_positions(self, game_state: Any) -> List[Tuple[int, int]]:
        """æ•µé§’ã®ä½ç½®ã‚’ç‰¹å®š"""
        enemy_pieces = game_state.player_b_pieces if self.player_id == "A" else game_state.player_a_pieces
        return list(enemy_pieces.keys())
    
    def record_game_result(self, won: bool):
        """ã‚²ãƒ¼ãƒ çµæœã‚’è¨˜éŒ²"""
        self.games_played += 1
        if won:
            self.wins += 1
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«çµæœã‚’åæ˜ 
        for data in self.training_data[-20:]:  # æœ€å¾Œã®20æ‰‹
            data['result'] = 1.0 if won else -1.0
    
    def train(self):
        """å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å­¦ç¿’"""
        if self.training_data:
            # æ¨å®šå™¨ã®å­¦ç¿’
            self.config.piece_estimator.train(self.training_data)
            
            # ä»–ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚‚å­¦ç¿’å¯èƒ½ãªã‚‰å®Ÿè¡Œ
            # ï¼ˆå®Ÿè£…ã¯çœç•¥ï¼‰
    
    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            'name': self.name,
            'games_played': self.games_played,
            'wins': self.wins,
            'win_rate': self.wins / max(self.games_played, 1),
            'modules': {
                'placement': self.config.placement_strategy.get_strategy_name(),
                'estimator': self.config.piece_estimator.get_estimator_name(),
                'qmap': self.config.qmap_generator.get_generator_name(),
                'selector': self.config.action_selector.get_selector_name()
            }
        }


# ================================================================================
# ç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ 
# ================================================================================

class CQCNNCompetition:
    """CQCNNç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.available_modules = {
            'placement': [
                StandardPlacement(),
                DefensivePlacement(),
                RandomPlacement(),
                MixedPlacement()
            ],
            'estimator': [
                SimpleCQCNNEstimator(n_qubits=4),
                AdvancedCQCNNEstimator(n_qubits=6),
                RandomEstimator()
            ],
            'qmap': [
                SimpleQMapGenerator(),
                StrategicQMapGenerator(),
                NeuralQMapGenerator()
            ],
            'selector': [
                GreedySelector(),
                EpsilonGreedySelector(epsilon=0.1),
                SoftmaxSelector(temperature=1.0)
            ]
        }
    
    def create_agent(self, player_id: str, 
                    placement_idx: int = 0,
                    estimator_idx: int = 0,
                    qmap_idx: int = 0,
                    selector_idx: int = 0) -> ModularCQCNNAgent:
        """æŒ‡å®šã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ"""
        config = ModuleConfig(
            placement_strategy=self.available_modules['placement'][placement_idx],
            piece_estimator=self.available_modules['estimator'][estimator_idx],
            qmap_generator=self.available_modules['qmap'][qmap_idx],
            action_selector=self.available_modules['selector'][selector_idx]
        )
        
        return ModularCQCNNAgent(player_id, config)
    
    def show_available_modules(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¡¨ç¤º"""
        print("=" * 70)
        print("ğŸ® åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«")
        print("=" * 70)
        
        for category, modules in self.available_modules.items():
            print(f"\nã€{category.upper()}ã€‘")
            for i, module in enumerate(modules):
                if category == 'placement':
                    name = module.get_strategy_name()
                elif category == 'estimator':
                    name = module.get_estimator_name()
                elif category == 'qmap':
                    name = module.get_generator_name()
                elif category == 'selector':
                    name = module.get_selector_name()
                print(f"  {i}: {name}")
    
    def run_match(self, agent1: ModularCQCNNAgent, agent2: ModularCQCNNAgent, 
                 verbose: bool = True) -> str:
        """å¯¾æˆ¦ã‚’å®Ÿè¡Œï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if verbose:
            print(f"\nğŸ® å¯¾æˆ¦: {agent1.name} vs {agent2.name}")
        
        # ã“ã“ã§ã¯çµæœã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¿”ã™ï¼ˆå®Ÿéš›ã®ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã¨çµ±åˆãŒå¿…è¦ï¼‰
        winner = random.choice([agent1.player_id, agent2.player_id, "Draw"])
        
        if winner == agent1.player_id:
            agent1.record_game_result(True)
            agent2.record_game_result(False)
            if verbose:
                print(f"ğŸ† å‹è€…: {agent1.name}")
        elif winner == agent2.player_id:
            agent1.record_game_result(False)
            agent2.record_game_result(True)
            if verbose:
                print(f"ğŸ† å‹è€…: {agent2.name}")
        else:
            agent1.record_game_result(False)
            agent2.record_game_result(False)
            if verbose:
                print("ğŸ¤ å¼•ãåˆ†ã‘")
        
        return winner


# ================================================================================
# ãƒ‡ãƒ¢å®Ÿè¡Œ
# ================================================================================

def main():
    """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("ğŸš€ CQCNNç«¶æŠ€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")
    print("=" * 70)
    
    # ç«¶æŠ€ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    competition = CQCNNCompetition()
    
    # åˆ©ç”¨å¯èƒ½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¡¨ç¤º
    competition.show_available_modules()
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆä¾‹
    print("\n" + "=" * 70)
    print("ğŸ“ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆãƒ‡ãƒ¢")
    print("=" * 70)
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ1: æ¨™æº–é…ç½® + ã‚·ãƒ³ãƒ—ãƒ«CQCNN + ã‚·ãƒ³ãƒ—ãƒ«Qå€¤ + è²ªæ¬²é¸æŠ
    agent1 = competition.create_agent("A", 0, 0, 0, 0)
    print(f"\nAgent 1: {agent1.name}")
    print(f"  é…ç½®: {agent1.config.placement_strategy.get_strategy_name()}")
    print(f"  æ¨å®š: {agent1.config.piece_estimator.get_estimator_name()}")
    print(f"  Qå€¤: {agent1.config.qmap_generator.get_generator_name()}")
    print(f"  é¸æŠ: {agent1.config.action_selector.get_selector_name()}")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ2: å®ˆå‚™é…ç½® + é«˜åº¦CQCNN + æˆ¦ç•¥Qå€¤ + Îµ-è²ªæ¬²
    agent2 = competition.create_agent("B", 1, 1, 1, 1)
    print(f"\nAgent 2: {agent2.name}")
    print(f"  é…ç½®: {agent2.config.placement_strategy.get_strategy_name()}")
    print(f"  æ¨å®š: {agent2.config.piece_estimator.get_estimator_name()}")
    print(f"  Qå€¤: {agent2.config.qmap_generator.get_generator_name()}")
    print(f"  é¸æŠ: {agent2.config.action_selector.get_selector_name()}")
    
    # ãƒ‡ãƒ¢å¯¾æˆ¦
    print("\n" + "=" * 70)
    print("ğŸ† ãƒ‡ãƒ¢å¯¾æˆ¦")
    print("=" * 70)
    
    for i in range(3):
        competition.run_match(agent1, agent2)
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n" + "=" * 70)
    print("ğŸ“Š çµ±è¨ˆ")
    print("=" * 70)
    
    for agent in [agent1, agent2]:
        stats = agent.get_statistics()
        print(f"\n{stats['name']}:")
        print(f"  å‹ç‡: {stats['win_rate']:.1%} ({stats['wins']}/{stats['games_played']})")
    
    print("\nâœ… ãƒ‡ãƒ¢å®Œäº†ï¼")
    print("\nğŸ’¡ ä½¿ã„æ–¹:")
    print("  1. create_agent()ã§ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç•ªå·ã‚’æŒ‡å®šã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ")
    print("  2. ç•°ãªã‚‹çµ„ã¿åˆã‚ã›ã§æ€§èƒ½ã‚’æ¯”è¼ƒ")
    print("  3. å­¦ç¿’ã«ã‚ˆã‚Šå„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ”¹å–„")


if __name__ == "__main__":
    main()
